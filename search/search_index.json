{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data-driven systems \u00b6 Lecture notes, seminar materials and homework exercises for course BMEVIAUAC01 Data-driven systems. Pull requests welcome As a student of this course you can earn extra points by contributing to the materials! Open a pull request to fix an error or contribute to the materials in any way! Check the link to the repository in the upper right corner. License The materials provided here are created for the students of course BMEVIAUAC01. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. These materials are to be viewed within the context of the course. For any other usage scenarios the material is provided as-is.","title":"Data-driven systems"},{"location":"#data-driven-systems","text":"Lecture notes, seminar materials and homework exercises for course BMEVIAUAC01 Data-driven systems. Pull requests welcome As a student of this course you can earn extra points by contributing to the materials! Open a pull request to fix an error or contribute to the materials in any way! Check the link to the repository in the upper right corner. License The materials provided here are created for the students of course BMEVIAUAC01. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. These materials are to be viewed within the context of the course. For any other usage scenarios the material is provided as-is.","title":"Data-driven systems"},{"location":"db/","text":"Sample database scheme \u00b6 The examples and seminars during the semester will use a sample database. The database is a simplified retail management system with products, customers, and orders. This description details the relational schema of the database; the MongoDB variant is the appropriate mirror of this scheme. The context of the database \u00b6 The system is designed to help the retail process of products. The products are grouped into hierarchical categories . Customers can browse products, place orders , and track the status of the orders. Customers can have multiple sites (e.g., retail stores of one company with multiple addresses). The order can be completed to any of these sites. Each customer has exactly one \"main site,\" which is where the invoices are addressed. An order can have multiple items ; each item has its status. An invoice is printed if the order is ready. An invoice cannot be changed once it is created. Different products have different VAT (value-added tax) rates. These VAT rates are subject to change over time, but these changes must not affect existing invoices. Data scheme \u00b6 The model of the whole database is depicted below. Tables and columns \u00b6 Table Column Description VAT ID Auto-generated primary key. Percentage The percentage of the value-added tax. PaymentMethod ID Auto-generated primary key. Method Short name of the payment method, e.g., cash or wire transfer. Deadline The deadline of the payment method, that is, the deadline for completing the transaction after the invoice is received. Status ID Auto-generated primary key. Name Short name of the status (e.g., new, processed). Category ID Auto-generated primary key. Name Name of the category, e.g., toys, LEGO, etc. ParentCategoryID Foreign key indicating the parent category; null if this is a top-level category. Product ID Auto-generated primary key. Name Product name. Price Product price without tax. Stock Amount of this product in stock. VATID Foreign key to the VAT table. CategoryID Foreign key to the Category table. Description XML description of the product. Customer ID Auto-generated primary key. Name Customer name. BankAccount Back account number of the customer. Login Login name for the webshop. Password Password for the webshop. Email Email address of the customer. MainCustomerSiteID The main site of the customer; a foreign key to the CustomerSite table. CustomerSite ID Auto-generated primary key. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. Tel Telephone number. Fax Fax number. CustomerID Foreign key to the Customer table. Order ID Auto-generated primary key. Date Date when the order was placed. Deadline Deadline until the order must be completed. CustomerSiteID Foreign key to the CustomerSite table; the order is billed and shipped to this site. StatusID Foreign key to the Status table; the actual status of the order. PaymentMethodID Foreign key to the PaymentMethod table; the chosen method of payment. OrderItem ID Auto-generated primary key. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g. for bulk order). OrderID Foreign key to the Order table; identifier the order this item belongs to. ProductID Foreign key to the Product table; identifies the product that is ordered. StatusID Foreign key to the Status table; the actual status of the item. InvoiceIssuer ID Auto-generated primary key. Name Name of the company selling the products. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. TaxIdentifier Tax identifier of the company. BankAccount Bank account number of the company. Invoice ID Auto-generated primary key. CustomerName The name of the customer; printed on the invoice. CustomerZipCode The zip code of the address. CustomerCity The city part of the address. CustomerStreet The street and house number part of the address. PrintedCopies Number of copies printed of this invoice. Cancelled Has the invoice been cancelled? PaymentMethod Payment method of the invoice (text name). CreationDate Date when the invoice was created. DeliveryDate Delivery date of the invoice. PaymentDeadline Deadline of the payment. InvoiceIssuerID Foreign key to the InvoiceIssuer table; the issuer of the invoice. OrderID Foreign key to the Order table; the order out of which this invoice was created. InvoiceItem ID Auto-generated primary key. Name Name of the product; printed on the invoice. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g., for bulk order). VATPercentage The effective percentage of the tax applied. InvoiceID Foreign key to the Invoice table; the invoice this item is part of. OrderItemID Foreign key to the OrderItem table; the item out of which this invoice item was created. Peculiarities \u00b6 Invoicing \u00b6 Invoices cannot be altered once issued; they can only be canceled. Therefore all data that appears on the order are copied into the Invoice and InvoiceItem tables once the invoice is created. There is only a single original copy of the invoice; therefore, the number of printed copies is recorded. Invoice issuer \u00b6 The issuer of the invoices changes very infrequently. However, in case it changes, existing invoices must remain unaltered. The issuer of the invoice is recorded in a separate table, and only one is in effect at all times. Each invoice references the right InvoiceIssuerId at the time. VAT \u00b6 The VAT percentage of products can change at any time. However, existing invoices must not be altered. Therefore the actual VAT percentage is stored with the invoice when created and not referenced from the VAT table. Product description \u00b6 Some products contain an additional XML description, such as the following example. <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <package_parameters> <number_of_packages> 1 </number_of_packages> <package_size> <unit> cm </unit> <width> 150 </width> <height> 20 </height> <depth> 20 </depth> </package_size> </package_parameters> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product>","title":"Sample database scheme"},{"location":"db/#sample-database-scheme","text":"The examples and seminars during the semester will use a sample database. The database is a simplified retail management system with products, customers, and orders. This description details the relational schema of the database; the MongoDB variant is the appropriate mirror of this scheme.","title":"Sample database scheme"},{"location":"db/#the-context-of-the-database","text":"The system is designed to help the retail process of products. The products are grouped into hierarchical categories . Customers can browse products, place orders , and track the status of the orders. Customers can have multiple sites (e.g., retail stores of one company with multiple addresses). The order can be completed to any of these sites. Each customer has exactly one \"main site,\" which is where the invoices are addressed. An order can have multiple items ; each item has its status. An invoice is printed if the order is ready. An invoice cannot be changed once it is created. Different products have different VAT (value-added tax) rates. These VAT rates are subject to change over time, but these changes must not affect existing invoices.","title":"The context of the database"},{"location":"db/#data-scheme","text":"The model of the whole database is depicted below.","title":"Data scheme"},{"location":"db/#tables-and-columns","text":"Table Column Description VAT ID Auto-generated primary key. Percentage The percentage of the value-added tax. PaymentMethod ID Auto-generated primary key. Method Short name of the payment method, e.g., cash or wire transfer. Deadline The deadline of the payment method, that is, the deadline for completing the transaction after the invoice is received. Status ID Auto-generated primary key. Name Short name of the status (e.g., new, processed). Category ID Auto-generated primary key. Name Name of the category, e.g., toys, LEGO, etc. ParentCategoryID Foreign key indicating the parent category; null if this is a top-level category. Product ID Auto-generated primary key. Name Product name. Price Product price without tax. Stock Amount of this product in stock. VATID Foreign key to the VAT table. CategoryID Foreign key to the Category table. Description XML description of the product. Customer ID Auto-generated primary key. Name Customer name. BankAccount Back account number of the customer. Login Login name for the webshop. Password Password for the webshop. Email Email address of the customer. MainCustomerSiteID The main site of the customer; a foreign key to the CustomerSite table. CustomerSite ID Auto-generated primary key. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. Tel Telephone number. Fax Fax number. CustomerID Foreign key to the Customer table. Order ID Auto-generated primary key. Date Date when the order was placed. Deadline Deadline until the order must be completed. CustomerSiteID Foreign key to the CustomerSite table; the order is billed and shipped to this site. StatusID Foreign key to the Status table; the actual status of the order. PaymentMethodID Foreign key to the PaymentMethod table; the chosen method of payment. OrderItem ID Auto-generated primary key. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g. for bulk order). OrderID Foreign key to the Order table; identifier the order this item belongs to. ProductID Foreign key to the Product table; identifies the product that is ordered. StatusID Foreign key to the Status table; the actual status of the item. InvoiceIssuer ID Auto-generated primary key. Name Name of the company selling the products. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. TaxIdentifier Tax identifier of the company. BankAccount Bank account number of the company. Invoice ID Auto-generated primary key. CustomerName The name of the customer; printed on the invoice. CustomerZipCode The zip code of the address. CustomerCity The city part of the address. CustomerStreet The street and house number part of the address. PrintedCopies Number of copies printed of this invoice. Cancelled Has the invoice been cancelled? PaymentMethod Payment method of the invoice (text name). CreationDate Date when the invoice was created. DeliveryDate Delivery date of the invoice. PaymentDeadline Deadline of the payment. InvoiceIssuerID Foreign key to the InvoiceIssuer table; the issuer of the invoice. OrderID Foreign key to the Order table; the order out of which this invoice was created. InvoiceItem ID Auto-generated primary key. Name Name of the product; printed on the invoice. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g., for bulk order). VATPercentage The effective percentage of the tax applied. InvoiceID Foreign key to the Invoice table; the invoice this item is part of. OrderItemID Foreign key to the OrderItem table; the item out of which this invoice item was created.","title":"Tables and columns"},{"location":"db/#peculiarities","text":"","title":"Peculiarities"},{"location":"db/#invoicing","text":"Invoices cannot be altered once issued; they can only be canceled. Therefore all data that appears on the order are copied into the Invoice and InvoiceItem tables once the invoice is created. There is only a single original copy of the invoice; therefore, the number of printed copies is recorded.","title":"Invoicing"},{"location":"db/#invoice-issuer","text":"The issuer of the invoices changes very infrequently. However, in case it changes, existing invoices must remain unaltered. The issuer of the invoice is recorded in a separate table, and only one is in effect at all times. Each invoice references the right InvoiceIssuerId at the time.","title":"Invoice issuer"},{"location":"db/#vat","text":"The VAT percentage of products can change at any time. However, existing invoices must not be altered. Therefore the actual VAT percentage is stored with the invoice when created and not referenced from the VAT table.","title":"VAT"},{"location":"db/#product-description","text":"Some products contain an additional XML description, such as the following example. <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <package_parameters> <number_of_packages> 1 </number_of_packages> <package_size> <unit> cm </unit> <width> 150 </width> <height> 20 </height> <depth> 20 </depth> </package_size> </package_parameters> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product>","title":"Product description"},{"location":"db/mongodb/","text":"Using MongoDB \u00b6 MongoDB is a free, open-source database server. We are using the community edition and Robo 3T as the client. Download links: https://www.mongodb.com/download-center/community https://robomongo.org/download Installation instructions: https://docs.mongodb.com/manual/administration/install-community/ Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/9bb0c06f-7b5f-454b-b5ff-c784bd96d84b Starting MongoDB server \u00b6 Depending on the installation model, the server might automatically start. If we opted out of this option, we could start the server with the following command within the installation directory. (Note, that the server application is the mongo\u200b d executable.) mongod.exe --dbpath = \"<workdir>\" The database will be stored in directory workdir . When started with the command above, the server is alive until the console is closed. We can shut down the server using Ctrl + C . Mongo shell \u00b6 The Mongo shell is a simple console client. The official documentation uses this shell in the examples; however, we will not use this app. Robo 3T \u00b6 Robo 3T is a simple free client application for accessing MongoDB databases. There are other client applications (e.g., Studio 3T), but the free version is sufficient. When the app starts, it displays out previous connections, or we can create a new one. By default, the address is localhost , and the port is 27017 . After the connection is established, the databases and collections are displayed in a tree-view on the left. To begin with, we will not have any database or collections. (We can create them manually: right-click on the server and Create Database . We will not use this, though.) A collection can be opened by double-clicking. This will open a new tab, where a search command is executed. This command can be edited, and we can issue custom queries too. The content of the collection (the documents) is listed below. Each document is a separate row. A document can be viewed, edited and deleted by right clicking a document. Edit is performed by editing the raw JSON document. A new document can be inserted by right-clicking and writing the JSON content. It is advised to copy an existing document and change it to ensure that key names are correct.","title":"Using MongoDB"},{"location":"db/mongodb/#using-mongodb","text":"MongoDB is a free, open-source database server. We are using the community edition and Robo 3T as the client. Download links: https://www.mongodb.com/download-center/community https://robomongo.org/download Installation instructions: https://docs.mongodb.com/manual/administration/install-community/ Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/9bb0c06f-7b5f-454b-b5ff-c784bd96d84b","title":"Using MongoDB"},{"location":"db/mongodb/#starting-mongodb-server","text":"Depending on the installation model, the server might automatically start. If we opted out of this option, we could start the server with the following command within the installation directory. (Note, that the server application is the mongo\u200b d executable.) mongod.exe --dbpath = \"<workdir>\" The database will be stored in directory workdir . When started with the command above, the server is alive until the console is closed. We can shut down the server using Ctrl + C .","title":"Starting MongoDB server"},{"location":"db/mongodb/#mongo-shell","text":"The Mongo shell is a simple console client. The official documentation uses this shell in the examples; however, we will not use this app.","title":"Mongo shell"},{"location":"db/mongodb/#robo-3t","text":"Robo 3T is a simple free client application for accessing MongoDB databases. There are other client applications (e.g., Studio 3T), but the free version is sufficient. When the app starts, it displays out previous connections, or we can create a new one. By default, the address is localhost , and the port is 27017 . After the connection is established, the databases and collections are displayed in a tree-view on the left. To begin with, we will not have any database or collections. (We can create them manually: right-click on the server and Create Database . We will not use this, though.) A collection can be opened by double-clicking. This will open a new tab, where a search command is executed. This command can be edited, and we can issue custom queries too. The content of the collection (the documents) is listed below. Each document is a separate row. A document can be viewed, edited and deleted by right clicking a document. Edit is performed by editing the raw JSON document. A new document can be inserted by right-clicking and writing the JSON content. It is advised to copy an existing document and change it to ensure that key names are correct.","title":"Robo 3T"},{"location":"db/mssql/","text":"Using Microsoft SQL Server \u00b6 Microsoft SQL Server is accessed using SQL Server Management Studio. We are using the so-called LocalDB version that hosts the server locally for development purposes, but you can also use the Express edition (any version). Download links: LocalDB is installed with Visual Studio https://www.microsoft.com/en-us/sql-server/sql-server-editions-express https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/98a6697d-daec-4a5f-82b6-8e96f06302e8 Using SQL Server Management Studio \u00b6 In university computer laboratories, you can start the software from the start menu. Connection details are configured when the application starts. When using LocalDB, the Server name is (localdb)\\mssqllocaldb ; for Express Edition the name is .\\sqlexpress (if installed with default settings). Either case use Windows Authentication . When the connection is established, the databases are listed on the left in the Object Explorer window under Databases . A database can be expanded, and the tables (along with other schema elements) are listed here. SQL code can be executed in a new Query window opened using on the toolbar. The commands in the Query window are executed on the currently selected database. This database is selected from a dropdown in the toolbar (see in the image below with yellow). We can open multiple Query windows. The SQL command is executed using the button on the toolbar. Only the selection is executed if any text is selected; otherwise, the entire window content is sent to the server. The result and errors are printed under the script. Creating a new database \u00b6 If we have no database, we must create one first. In Object Explorer right-click Databases to open a dialog. We need to specify a name and leave all other options as-is. After creating a new database, we shall not forget to select the toolbar's current database for any Query window we have open! Concurrent transactions \u00b6 To simulate concurrent transactions, we need two Query windows; open two by pressing the New Query button twice. You can align these windows next to each other by right-clicking the Query tab title and selecting New Vertical Tab Group . Listing and editing table content \u00b6 To view any database table's content, open the database in Object Explorer , locate the table under Tables , then right-click and chose Select Top 1000 Rows or Edit Top 200 Rows . Intellisense reload \u00b6 Intellisense often does not work in SQL Management Studio query windows. Press Control+Shift+R-t to trigger a reload of Intellisense cache. We also need to use this after creating a new object (e.g., a new stored procedure). Creating stored procedures and triggers \u00b6 We can create new stored procedures or triggers by writing the T-SQL code to create them in a Query window. Once an item with the same name exists, we cannot create it but have to modify the existing one using the proper command. Existing stored procedures are listed in Object Explorer under our database in the Programability/Stored Procedures folder. (Newly created items do not appear in the folder, but we have to refresh the folder content by right-clicking and choosing Refresh .) Triggers are found in Object Explorer under the table on which they are defined in the Triggers folder (system-level triggers are in the Programability folder under the database itself). The code of any existing stored procedure or trigger can be opened by locating them (see above), then right-clicking and choosing the Modify command. This will open a new Query window with an alter command and the current code of the program.","title":"Using Microsoft SQL Server"},{"location":"db/mssql/#using-microsoft-sql-server","text":"Microsoft SQL Server is accessed using SQL Server Management Studio. We are using the so-called LocalDB version that hosts the server locally for development purposes, but you can also use the Express edition (any version). Download links: LocalDB is installed with Visual Studio https://www.microsoft.com/en-us/sql-server/sql-server-editions-express https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/98a6697d-daec-4a5f-82b6-8e96f06302e8","title":"Using Microsoft SQL Server"},{"location":"db/mssql/#using-sql-server-management-studio","text":"In university computer laboratories, you can start the software from the start menu. Connection details are configured when the application starts. When using LocalDB, the Server name is (localdb)\\mssqllocaldb ; for Express Edition the name is .\\sqlexpress (if installed with default settings). Either case use Windows Authentication . When the connection is established, the databases are listed on the left in the Object Explorer window under Databases . A database can be expanded, and the tables (along with other schema elements) are listed here. SQL code can be executed in a new Query window opened using on the toolbar. The commands in the Query window are executed on the currently selected database. This database is selected from a dropdown in the toolbar (see in the image below with yellow). We can open multiple Query windows. The SQL command is executed using the button on the toolbar. Only the selection is executed if any text is selected; otherwise, the entire window content is sent to the server. The result and errors are printed under the script.","title":"Using SQL Server Management Studio"},{"location":"db/mssql/#creating-a-new-database","text":"If we have no database, we must create one first. In Object Explorer right-click Databases to open a dialog. We need to specify a name and leave all other options as-is. After creating a new database, we shall not forget to select the toolbar's current database for any Query window we have open!","title":"Creating a new database"},{"location":"db/mssql/#concurrent-transactions","text":"To simulate concurrent transactions, we need two Query windows; open two by pressing the New Query button twice. You can align these windows next to each other by right-clicking the Query tab title and selecting New Vertical Tab Group .","title":"Concurrent transactions"},{"location":"db/mssql/#listing-and-editing-table-content","text":"To view any database table's content, open the database in Object Explorer , locate the table under Tables , then right-click and chose Select Top 1000 Rows or Edit Top 200 Rows .","title":"Listing and editing table content"},{"location":"db/mssql/#intellisense-reload","text":"Intellisense often does not work in SQL Management Studio query windows. Press Control+Shift+R-t to trigger a reload of Intellisense cache. We also need to use this after creating a new object (e.g., a new stored procedure).","title":"Intellisense reload"},{"location":"db/mssql/#creating-stored-procedures-and-triggers","text":"We can create new stored procedures or triggers by writing the T-SQL code to create them in a Query window. Once an item with the same name exists, we cannot create it but have to modify the existing one using the proper command. Existing stored procedures are listed in Object Explorer under our database in the Programability/Stored Procedures folder. (Newly created items do not appear in the folder, but we have to refresh the folder content by right-clicking and choosing Refresh .) Triggers are found in Object Explorer under the table on which they are defined in the Triggers folder (system-level triggers are in the Programability folder under the database itself). The code of any existing stored procedure or trigger can be opened by locating them (see above), then right-clicking and choosing the Modify command. This will open a new Query window with an alter command and the current code of the program.","title":"Creating stored procedures and triggers"},{"location":"homework/","text":"Optional homework \u00b6 These exercises are optional . You can earn extra points that are added to your exam score. In the exercises and the evaluation results, you will see a text \u201ciMsc\u201d; please ignore that (it is for the Hungarian students). All exercises are available for extra points on this course. Here you find the exercise descriptions; the submission of the solutions is expected via GitHub Classroom. Working code You are expected to write code that actually works! Your code will be executed, and it is required to fulfill the specified task. The exercises \u00b6 MSSQL server-side programming ADO.NET data access Entity Framework MongoDB REST API and Web API Submission \u00b6 Each homework must be submitted in a personal git repository. Please refer to the detailed guideline here . You must carefully study these guidelines! IMPORTANT The submissions of the homework must follow these guidelines. Submissions not adhering to the expected format are not considered. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized. Screenshots \u00b6 Some of the exercises require you to create a screenshot. This screenshot is proof of the completion of the exercise. The expected content of these screenshots is detailed in the exercise description. The screenshot may include the entire desktop or just the required portion of the screen. The screenshots must be submitted as part of the solution code, uploaded to the git repository. The repositories are private; only you and the instructions can access them. If there is any content on the screenshot that is not relevant to the exercise and you would like to remove, you can obscure these parts. Required tools \u00b6 Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. GitHub account and a git client. For homework using the MSSQL platform: Microsoft SQL Server. The free Express version is sufficient, or you may also use localdb installed with Visual Studio. A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql For homework using a MongoDB database: MongoDB Community Server Robo 3T Sample database initialization script: mongo.js For the REST API homework: Postman For writing C# code (most homework, except the first one): Microsoft Visual Studio 2022 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET SDK, and dotnet CLI . .NET 6.0 SDK .NET 6.0 Mind the version! You need .NET SDK version 6.0 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS. Submission evaluation \u00b6 The evaluation of the exercises is semi-automatic . Your code will be executed; therefore, it is vital to follow the exercise descriptions precisely (e.g., use the provided code skeleton, change only the allowed parts of the code, etc.). You will receive a preliminary result about your submission in GitHub; see in the guideline here ). If there are some issues you need to diagnose, the entire log of the execution is available for you on the GitHub Actions web page. A short introduction is provided here . Verification In some of the exercises, where the technology permits, you will find unit tests. These tests help you verify your work, but these are no substitution for your validation . When you upload your work, more exhaustive testing will evaluate your submission.","title":"Optional homework"},{"location":"homework/#optional-homework","text":"These exercises are optional . You can earn extra points that are added to your exam score. In the exercises and the evaluation results, you will see a text \u201ciMsc\u201d; please ignore that (it is for the Hungarian students). All exercises are available for extra points on this course. Here you find the exercise descriptions; the submission of the solutions is expected via GitHub Classroom. Working code You are expected to write code that actually works! Your code will be executed, and it is required to fulfill the specified task.","title":"Optional homework"},{"location":"homework/#the-exercises","text":"MSSQL server-side programming ADO.NET data access Entity Framework MongoDB REST API and Web API","title":"The exercises"},{"location":"homework/#submission","text":"Each homework must be submitted in a personal git repository. Please refer to the detailed guideline here . You must carefully study these guidelines! IMPORTANT The submissions of the homework must follow these guidelines. Submissions not adhering to the expected format are not considered. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized.","title":"Submission"},{"location":"homework/#screenshots","text":"Some of the exercises require you to create a screenshot. This screenshot is proof of the completion of the exercise. The expected content of these screenshots is detailed in the exercise description. The screenshot may include the entire desktop or just the required portion of the screen. The screenshots must be submitted as part of the solution code, uploaded to the git repository. The repositories are private; only you and the instructions can access them. If there is any content on the screenshot that is not relevant to the exercise and you would like to remove, you can obscure these parts.","title":"Screenshots"},{"location":"homework/#required-tools","text":"Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. GitHub account and a git client. For homework using the MSSQL platform: Microsoft SQL Server. The free Express version is sufficient, or you may also use localdb installed with Visual Studio. A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql For homework using a MongoDB database: MongoDB Community Server Robo 3T Sample database initialization script: mongo.js For the REST API homework: Postman For writing C# code (most homework, except the first one): Microsoft Visual Studio 2022 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET SDK, and dotnet CLI . .NET 6.0 SDK .NET 6.0 Mind the version! You need .NET SDK version 6.0 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.","title":"Required tools"},{"location":"homework/#submission-evaluation","text":"The evaluation of the exercises is semi-automatic . Your code will be executed; therefore, it is vital to follow the exercise descriptions precisely (e.g., use the provided code skeleton, change only the allowed parts of the code, etc.). You will receive a preliminary result about your submission in GitHub; see in the guideline here ). If there are some issues you need to diagnose, the entire log of the execution is available for you on the GitHub Actions web page. A short introduction is provided here . Verification In some of the exercises, where the technology permits, you will find unit tests. These tests help you verify your work, but these are no substitution for your validation . When you upload your work, more exhaustive testing will evaluate your submission.","title":"Submission evaluation"},{"location":"homework/GitHub-Actions/","text":"Using GitHub Actions \u00b6 The semi-automatic evaluation of the exercises uses GitHub Actions . It is a CI system capable of running jobs on git repositories. We use this system, for example, to compile your code and test it. You will receive a notification about the results in a pull request. But if you need more details, such as check the application logs, you can access these using the web interface of GitHub under Actions . Here, you will see a list of Workflows . Each evaluation (each commit) is a separate item here (so the history is also available). By selecting one (e.g., the last one is always at the top of the list), you see this workflow's details. To get to the logs, you need to click once more on the left. The log will be on the right side. Each green checkmark is a successful step. These steps do not correspond to the exercises; these describe the evaluation process. These steps include preparations, such as setting up the .NET environment for compiling your code (since each workflow starts in a clean environment, these steps are performed each time). Most of these steps should be successful, even if your submission contains an error. The two exceptions when these tasks might fail due to your changes are: (1) if neptun.txt is missing, or (2) your C# code does not compile. The neptun.txt is mandatory, and no evaluation is performed until that is provided. The C# compilation is a step that must succeed; otherwise, your application cannot be started. There might be transient errors in these workflows. An example is when a download, such as the download of the .NET environment fails. The workflow execution can be repeated if this occurs. Retrying the execution may only help if the problem is indeed transient; a retry will not resolve a C# compilation error. (You can deduce the cause from the name of the step and the error message.) You might also be able to access the application logs. E.g., when testing a .NET application, it is started, and the logs will be printed here. The image below shows the initialization of an Entity Framework application, where you can also see the translated and executed SQL commands. (You would see the same in Visual Studio Output while debugging.) The content here, obviously, depends on the actual exercise.","title":"Using GitHub Actions"},{"location":"homework/GitHub-Actions/#using-github-actions","text":"The semi-automatic evaluation of the exercises uses GitHub Actions . It is a CI system capable of running jobs on git repositories. We use this system, for example, to compile your code and test it. You will receive a notification about the results in a pull request. But if you need more details, such as check the application logs, you can access these using the web interface of GitHub under Actions . Here, you will see a list of Workflows . Each evaluation (each commit) is a separate item here (so the history is also available). By selecting one (e.g., the last one is always at the top of the list), you see this workflow's details. To get to the logs, you need to click once more on the left. The log will be on the right side. Each green checkmark is a successful step. These steps do not correspond to the exercises; these describe the evaluation process. These steps include preparations, such as setting up the .NET environment for compiling your code (since each workflow starts in a clean environment, these steps are performed each time). Most of these steps should be successful, even if your submission contains an error. The two exceptions when these tasks might fail due to your changes are: (1) if neptun.txt is missing, or (2) your C# code does not compile. The neptun.txt is mandatory, and no evaluation is performed until that is provided. The C# compilation is a step that must succeed; otherwise, your application cannot be started. There might be transient errors in these workflows. An example is when a download, such as the download of the .NET environment fails. The workflow execution can be repeated if this occurs. Retrying the execution may only help if the problem is indeed transient; a retry will not resolve a C# compilation error. (You can deduce the cause from the name of the step and the error message.) You might also be able to access the application logs. E.g., when testing a .NET application, it is started, and the logs will be printed here. The image below shows the initialization of an Entity Framework application, where you can also see the translated and executed SQL commands. (You would see the same in Visual Studio Output while debugging.) The content here, obviously, depends on the actual exercise.","title":"Using GitHub Actions"},{"location":"homework/GitHub/","text":"Submitting your work (GitHub) \u00b6 We are using GitHub to submit the solutions. Each homework is submitted in a GitHub repository. The repository is created through a link included in the exercise description. The solution of the exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the instructor (with GitHub username akosdudas ). IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded. Short version, aka. TL;DR \u00b6 The detailed description below shows the entire procedure. This summary is an overview of the whole process. The exercises are solved in a dedicated GitHub repository created using a GitHub Classroom invitation link published in Moodle. Your solution is submitted on a new branch, not on master. You can create any number of committed on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your instructor use the @name annotation in the comment text. Starting your work: git checkout \u00b6 Register a GitHub account if you don't have one yet. Open the course page in Moodle and find the invitation URL. This link is different for each homework; make sure to use the right one. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone your repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the shell or the console, the following command performs the clone (if the git command is available): git clone <repository link> If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. Check the name used for committing Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" To avoid having to set this for all repositories, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push futher commits git push Submitting the solution \u00b6 When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the instructor to evaluate your submission more easily by seeing all changes at once. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result of this evaluation as a comment added to the pull request thread. This will be different for each homework. Your code will be executed and tested, and you will receive a preliminary result too. If you need more information about the evaluation and the results, GitHub Actions can provide you more. A short introduction is provided here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to the instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request. Questions and complaints regarding the final result \u00b6 If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Submitting your work (GitHub)"},{"location":"homework/GitHub/#submitting-your-work-github","text":"We are using GitHub to submit the solutions. Each homework is submitted in a GitHub repository. The repository is created through a link included in the exercise description. The solution of the exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the instructor (with GitHub username akosdudas ). IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded.","title":"Submitting your work (GitHub)"},{"location":"homework/GitHub/#short-version-aka-tldr","text":"The detailed description below shows the entire procedure. This summary is an overview of the whole process. The exercises are solved in a dedicated GitHub repository created using a GitHub Classroom invitation link published in Moodle. Your solution is submitted on a new branch, not on master. You can create any number of committed on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your instructor use the @name annotation in the comment text.","title":"Short version, aka. TL;DR"},{"location":"homework/GitHub/#starting-your-work-git-checkout","text":"Register a GitHub account if you don't have one yet. Open the course page in Moodle and find the invitation URL. This link is different for each homework; make sure to use the right one. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone your repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the shell or the console, the following command performs the clone (if the git command is available): git clone <repository link> If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. Check the name used for committing Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" To avoid having to set this for all repositories, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push futher commits git push","title":"Starting your work: git checkout"},{"location":"homework/GitHub/#submitting-the-solution","text":"When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the instructor to evaluate your submission more easily by seeing all changes at once. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result of this evaluation as a comment added to the pull request thread. This will be different for each homework. Your code will be executed and tested, and you will receive a preliminary result too. If you need more information about the evaluation and the results, GitHub Actions can provide you more. A short introduction is provided here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to the instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request.","title":"Submitting the solution"},{"location":"homework/GitHub/#questions-and-complaints-regarding-the-final-result","text":"If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Questions and complaints regarding the final result"},{"location":"homework/VisualStudio/","text":"Install Visual Studio & .NET SDK \u00b6 In some of the exercises require Microsoft Visual Studio version 2022 . The free Community edition is sufficient for solving these exercises. VS Code The exercises can also be solved using the platform-independent Visual Studio Code . The skeletons of the exercises are prepared for Visual Studio. If you are working with VS Code, you need to configure your environment. Visual Studio workloads \u00b6 When installing Visual Studio, the ASP.NET and web development workload have to be selected. An existing installation can be modified using the Visual Studio Installer . Check and install .NET SDK \u00b6 Visual Studio might install certain versions of the .NET SDK. To check if you have the right version, use the dotnet CLI: in a console, execute the dotnet --list-sdks command. This command works on Linux and Mac too. It will print something similar: C:\\>dotnet --list-sdks 6.0.300 [C:\\Program Files\\dotnet\\sdk] If you see version 6.0 in this list, then you are good to go. Otherwise, install the SDK from here .","title":"Install Visual Studio & .NET SDK"},{"location":"homework/VisualStudio/#install-visual-studio-net-sdk","text":"In some of the exercises require Microsoft Visual Studio version 2022 . The free Community edition is sufficient for solving these exercises. VS Code The exercises can also be solved using the platform-independent Visual Studio Code . The skeletons of the exercises are prepared for Visual Studio. If you are working with VS Code, you need to configure your environment.","title":"Install Visual Studio &amp; .NET SDK"},{"location":"homework/VisualStudio/#visual-studio-workloads","text":"When installing Visual Studio, the ASP.NET and web development workload have to be selected. An existing installation can be modified using the Visual Studio Installer .","title":"Visual Studio workloads"},{"location":"homework/VisualStudio/#check-and-install-net-sdk","text":"Visual Studio might install certain versions of the .NET SDK. To check if you have the right version, use the dotnet CLI: in a console, execute the dotnet --list-sdks command. This command works on Linux and Mac too. It will print something similar: C:\\>dotnet --list-sdks 6.0.300 [C:\\Program Files\\dotnet\\sdk] If you see version 6.0 in this list, then you are good to go. Otherwise, install the SDK from here .","title":"Check and install .NET SDK"},{"location":"homework/adonet/","text":"Exercise: ADO.NET data access \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . This homework uses MSSQL database. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Product repository (2 points) \u00b6 Create repository class for managing the Product entities; use ADO.NET Connection technology. Open the sln file from the checked-out folder using Visual Studio. Find classes Repository.ProductRepository and Model.Product . Implement the following methods of class ProductRepository : Search(string name) : find all products in the database matching the provided name, and return them as C# objects. If the name filter argument is null , the method should return all products; otherwise, it should match names that contain the specified string in a case-insensitive manner! FindById(int id) : returns a single product matched by the ID, or returns null if not found. Update(Product p) updates the properties of a product in the database based on the values received as parameter. Update the Name , Price , and Stock values, and you may ignore the rest. Delete(int id) should delete the product specified by the ID from the database - if such product exists. The method shall return whether the delete was successful. (You need to delete the product record only; do not remove other referenced records. In case deletion is blocked due to foreign key constraints, do not catch the error; let the caller see the error.) You should mind the following requirements: Only make changes to class ProductRepository ! In the repository code open the ADO.NET connection using the connection string in field connectionString (and do not use TestConnectionStringHelper here). You need to find the tax percentage of the product too. In the returned instance of Model.Product you must include the percentage of the referenced VAT record and not the ID of this VAT record! The name of the category of the product has to be retrieved similarly. You may only use ADO.NET. You must prohibit SQL injection. Make no changes to Model.Product in this exercise! Do not change the definition of class ProductRepository (do not change the class's name, nor the constructor or method declarations); only write the method bodies. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code and dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository (as much as you can fit on the screenshot), and the test execution outcome ! Save the screenshot as f1.png and upload it as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Optimistic concurrency handling (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. When updating a product in the database, the code shall identify and prohibit overwriting a previously unseen modification. Implement this behavior in ProductRepository.UpdateWithConcurrencyCheck . This method shall deny the update if it discovers a lost update concurrency issue. The specific sequence of events that we want to prohibit: User A queries a product. User B fetches the same product. User A changes a property, such as the price, then updates the database with this change. User B makes a change to the product properties (either the price property, or another one), and overwrites the changes made by user A without noticing it. Optimistic concurrency handling Use the technique of optimistic concurrency handling to resolve this issue. You must not use transactions here, since the query and the data update happens over a longer period without maintaining a database connection. Do not use multiple SQL statements either, as in-between the execution of multiple statements the database content can change resulting in your application not working with the latest data. Implement the method ProductRepository.UpdateWithConcurrencyCheck , and also update Model.Product as needed. You may not add any new columns to the database. You should mind the following requirements: Only make changes to the method ProductRepository.UpdateWithConcurrencyCheck and class Model.Product ! The method shall indicate as return value whether the change was saved (that it, it discovered no concurrency issues). Explain the behavior in a C# comment in method UpdateWithConcurrencyCheck (in 2-3 sentences). Solve the exercise with using a single SQL command! You may only use ADO.NET. You must prohibit SQL injection. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the single method body. Do not change the constructor signature of the class Model.Product (number, order, or names of the parameters), but you may change the body. Do not alter any existing properties of the class, but you can add new ones. SUBMISSION Upload the changed C# source code. Do not forget the explanation comment!","title":"Exercise: ADO.NET data access"},{"location":"homework/adonet/#exercise-adonet-data-access","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . This homework uses MSSQL database.","title":"Exercise: ADO.NET data access"},{"location":"homework/adonet/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/adonet/#exercise-1-product-repository-2-points","text":"Create repository class for managing the Product entities; use ADO.NET Connection technology. Open the sln file from the checked-out folder using Visual Studio. Find classes Repository.ProductRepository and Model.Product . Implement the following methods of class ProductRepository : Search(string name) : find all products in the database matching the provided name, and return them as C# objects. If the name filter argument is null , the method should return all products; otherwise, it should match names that contain the specified string in a case-insensitive manner! FindById(int id) : returns a single product matched by the ID, or returns null if not found. Update(Product p) updates the properties of a product in the database based on the values received as parameter. Update the Name , Price , and Stock values, and you may ignore the rest. Delete(int id) should delete the product specified by the ID from the database - if such product exists. The method shall return whether the delete was successful. (You need to delete the product record only; do not remove other referenced records. In case deletion is blocked due to foreign key constraints, do not catch the error; let the caller see the error.) You should mind the following requirements: Only make changes to class ProductRepository ! In the repository code open the ADO.NET connection using the connection string in field connectionString (and do not use TestConnectionStringHelper here). You need to find the tax percentage of the product too. In the returned instance of Model.Product you must include the percentage of the referenced VAT record and not the ID of this VAT record! The name of the category of the product has to be retrieved similarly. You may only use ADO.NET. You must prohibit SQL injection. Make no changes to Model.Product in this exercise! Do not change the definition of class ProductRepository (do not change the class's name, nor the constructor or method declarations); only write the method bodies. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code and dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository (as much as you can fit on the screenshot), and the test execution outcome ! Save the screenshot as f1.png and upload it as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Product repository (2 points)"},{"location":"homework/adonet/#exercise-2-optimistic-concurrency-handling-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. When updating a product in the database, the code shall identify and prohibit overwriting a previously unseen modification. Implement this behavior in ProductRepository.UpdateWithConcurrencyCheck . This method shall deny the update if it discovers a lost update concurrency issue. The specific sequence of events that we want to prohibit: User A queries a product. User B fetches the same product. User A changes a property, such as the price, then updates the database with this change. User B makes a change to the product properties (either the price property, or another one), and overwrites the changes made by user A without noticing it. Optimistic concurrency handling Use the technique of optimistic concurrency handling to resolve this issue. You must not use transactions here, since the query and the data update happens over a longer period without maintaining a database connection. Do not use multiple SQL statements either, as in-between the execution of multiple statements the database content can change resulting in your application not working with the latest data. Implement the method ProductRepository.UpdateWithConcurrencyCheck , and also update Model.Product as needed. You may not add any new columns to the database. You should mind the following requirements: Only make changes to the method ProductRepository.UpdateWithConcurrencyCheck and class Model.Product ! The method shall indicate as return value whether the change was saved (that it, it discovered no concurrency issues). Explain the behavior in a C# comment in method UpdateWithConcurrencyCheck (in 2-3 sentences). Solve the exercise with using a single SQL command! You may only use ADO.NET. You must prohibit SQL injection. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the single method body. Do not change the constructor signature of the class Model.Product (number, order, or names of the parameters), but you may change the body. Do not alter any existing properties of the class, but you can add new ones. SUBMISSION Upload the changed C# source code. Do not forget the explanation comment!","title":"Exercise 2: Optimistic concurrency handling (2 points)"},{"location":"homework/ef/","text":"Exercise: Entity Framework \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . This homework uses MSSQL database. Entity Framework Core We are using Entity Framework Core in this exercise. This is different from Entity Framework used in the seminar exercises; this is a platform-independent technology. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Database mapping using Code First model and queries (2 points) \u00b6 Prepare the (partial) mapping of the database using Entity Framework Code First modeling. The Entity Framework Core package is part of the project, so you can start coding. The central class for database access is the DbContext. This class already exists with the name ProductDBContext . Map the product entity. Create a new class with the name DbProduct with the following code. (The Db prefix indicates that this class is within the scope of the database. This will be relevant in the next exercise.) We rely on conventions as much as possible: use property names that match the column names to make mapping automatic. using System.ComponentModel.DataAnnotations.Schema ; namespace ef { [Table(\"Product\")] public class DbProduct { [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int ID { get ; set ; } public string Name { get ; set ; } public double Price { get ; set ; } public int Stock { get ; set ; } } } Open the source code of class ProductDbContext and uncomment the Products property. Create a new class with the name DbVat in namespace ef for mapping the VAT database table similarly as seen before. Do not forget to add a new DbSet property into ProductContext with the name Vat . Map the Product - VAT connection. Add a new get-set property into class DbProduct with name Vat and type DbVat . Use the ForeignKey attribute on this property , to indicate the foreign key used to store the relationship (\"VatID\"). Create the \u201cother side\u201d of this one-to-many connection from class DbVat to DbProduct . This should be a new property of type System.Collections.Generic.List with name Products . (See an example in the link above.) There are unit tests available in the solution. The test codes are commented out because it does not compile until you write the code. Select the whole test code and use Edit / Advanced / Uncomment Selection . You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code, or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. If the tests do not compile If the test code does not compile, you may have used slightly different property names. Fix these in your code and not in the tests ! OnConfiguring You need no connection string in the DbContext . The constructor handles the connection to the database. Do not create OnConfiguring method in this class! SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the DbContext and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes here and there. That is, if the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Repository implementation using Entity Framework (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The Entity Framework DbContext created above has some drawbacks. For example, we need to trigger loading related entities using Include in every query, and the mapped entities are bound to precisely match the database schema. In complex applications, the DbContext is frequently wrapped in a repository that handles all peculiarities of the data access layer. Implement class ProductRepository that helps with listing and inserting products. You are provided with a so-called model class representing the product entity, only in a more user-friendly way: it contains the tax percentage value directly. An instance of this class is built from database entities, but represents all information in one instance instead of having to handle a product and a VAT record separately. Class Model.Product contains most properties of class DbProduct , but instead of the navigation property to DbVat it contains the referenced percentage value ( VAT.Percentage ) directly. Implement the methods of class `ProductRepository. List shall return all products mapped to instances of Model.Product . Insert shall insert a new product into the database. This method shall find the matching VAT record in the database based on the tax percentage value in the model class; if there is no match, it shall insert a new VAT record too! The method shall return the ID of the newly inserted ID (as generated by the database). Delete should delete a product record matched by the ID. You should only delete the product record - no referenced records shall be removed. If delete is blocked by foreign keys, let the caller handle the error. The return value of the method should be false if no record with the specified ID exist; a true return value shall indicate successful deletion. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the method bodies. In the repository code use ProductRepository.createDbContext() to instantiate the DbContext (do not use TestConnectionStringHelper here). SUBMISSION Upload the changed C# source code.","title":"Exercise: Entity Framework"},{"location":"homework/ef/#exercise-entity-framework","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . This homework uses MSSQL database. Entity Framework Core We are using Entity Framework Core in this exercise. This is different from Entity Framework used in the seminar exercises; this is a platform-independent technology.","title":"Exercise: Entity Framework"},{"location":"homework/ef/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/ef/#exercise-1-database-mapping-using-code-first-model-and-queries-2-points","text":"Prepare the (partial) mapping of the database using Entity Framework Code First modeling. The Entity Framework Core package is part of the project, so you can start coding. The central class for database access is the DbContext. This class already exists with the name ProductDBContext . Map the product entity. Create a new class with the name DbProduct with the following code. (The Db prefix indicates that this class is within the scope of the database. This will be relevant in the next exercise.) We rely on conventions as much as possible: use property names that match the column names to make mapping automatic. using System.ComponentModel.DataAnnotations.Schema ; namespace ef { [Table(\"Product\")] public class DbProduct { [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int ID { get ; set ; } public string Name { get ; set ; } public double Price { get ; set ; } public int Stock { get ; set ; } } } Open the source code of class ProductDbContext and uncomment the Products property. Create a new class with the name DbVat in namespace ef for mapping the VAT database table similarly as seen before. Do not forget to add a new DbSet property into ProductContext with the name Vat . Map the Product - VAT connection. Add a new get-set property into class DbProduct with name Vat and type DbVat . Use the ForeignKey attribute on this property , to indicate the foreign key used to store the relationship (\"VatID\"). Create the \u201cother side\u201d of this one-to-many connection from class DbVat to DbProduct . This should be a new property of type System.Collections.Generic.List with name Products . (See an example in the link above.) There are unit tests available in the solution. The test codes are commented out because it does not compile until you write the code. Select the whole test code and use Edit / Advanced / Uncomment Selection . You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code, or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. If the tests do not compile If the test code does not compile, you may have used slightly different property names. Fix these in your code and not in the tests ! OnConfiguring You need no connection string in the DbContext . The constructor handles the connection to the database. Do not create OnConfiguring method in this class! SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the DbContext and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes here and there. That is, if the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Database mapping using Code First model and queries (2 points)"},{"location":"homework/ef/#exercise-2-repository-implementation-using-entity-framework-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The Entity Framework DbContext created above has some drawbacks. For example, we need to trigger loading related entities using Include in every query, and the mapped entities are bound to precisely match the database schema. In complex applications, the DbContext is frequently wrapped in a repository that handles all peculiarities of the data access layer. Implement class ProductRepository that helps with listing and inserting products. You are provided with a so-called model class representing the product entity, only in a more user-friendly way: it contains the tax percentage value directly. An instance of this class is built from database entities, but represents all information in one instance instead of having to handle a product and a VAT record separately. Class Model.Product contains most properties of class DbProduct , but instead of the navigation property to DbVat it contains the referenced percentage value ( VAT.Percentage ) directly. Implement the methods of class `ProductRepository. List shall return all products mapped to instances of Model.Product . Insert shall insert a new product into the database. This method shall find the matching VAT record in the database based on the tax percentage value in the model class; if there is no match, it shall insert a new VAT record too! The method shall return the ID of the newly inserted ID (as generated by the database). Delete should delete a product record matched by the ID. You should only delete the product record - no referenced records shall be removed. If delete is blocked by foreign keys, let the caller handle the error. The return value of the method should be false if no record with the specified ID exist; a true return value shall indicate successful deletion. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the method bodies. In the repository code use ProductRepository.createDbContext() to instantiate the DbContext (do not use TestConnectionStringHelper here). SUBMISSION Upload the changed C# source code.","title":"Exercise 2: Repository implementation using Entity Framework (2 points)"},{"location":"homework/mongodb/","text":"Exercise: MongoDB \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . Before you begin, create and initialize the database; use the steps the seminar exercises describe. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Product with the largest total value (2 points) \u00b6 The task is to find the product that has the largest total value within a product category. The total value is the price of the product multiplied by the amount of the product in stock . You need to implement the following method in class ProductRepository . ( string , double? ) ProductWithLargestTotalValue ( ObjectId categoryId ) Let us check the test related to this exercise in file TestExercise1.cs to understand what is expected here. The method accepts a category filter as an argument; products have to be filtered for this category. The return value should be the name of the product (with the largest total value) and the total value itself. If there are no products in the specified category, the return value should be (null, null) . Use the aggregation pipeline of MongoDB. To see how this aggregation pipeline works, you can refer to the seminar material. You should build a pipeline consisting of the following stages: Filter the products for the specified category. Use a $match ( Match ) stage to specify the filter. Calculate for each product the total value (multiply the price and the stock) using a $project ( Project ) stage. Make sure to include the name of the product, you will need it for the final result. Order the items based on this calculated total value descending. Use a $sort ( SortByDescending ) stage. Since it is the largest value that we need, take the first item after sorting. Do not forget that there might not be any product in the specified category. Therefore you should use FirstOrDefault to fetch this item. If the syntax (string, double?) is unfamiliar: return ( \"test\" , 0.0 ); The function will return with these two values. Implement the repository method. The repository class receives the database as a parameter and saves the collection as a local variable in the class; use this field to manipulate the collection. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestDbFactory if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Estimating storage space (2 points) \u00b6 The company is moving to a new location. We need to know whether the current stock can be moved and will fit into the new storage facility. Implement the method that calculates the total volume of all products in stock ! The products have the necessary information in description.product.package_parameters : Use this to calculate the total volume of all items: Use the information from package_parameters (and not from product_size ). A product might have multiple packages; this information is available in package_parameters.number_of_packages . This number shall be used as a multiplicator. Each product has a single size, and if it has multiple packages, then all packages are of the same size. The final total: for all products \u03a3 (product stock * number of packages * width * height * depth). If a product does not have these information, it's volume should be calculated as 0. Mind, that the size also has a unit: either cm or m , but the final value is expected in cubic meter. Implement method double GetAllProductsCumulativeVolume() that returns a single scalar total of the volume in cubic meters . The calculation should be performed by the database (not in C#); use the aggregation pipeline. Sum aggregation You will need the $group aggregation stage. Although, we do not need to group the products, still, this will allow us to aggregate all of them. Map each product into the same group (that is, in the $group stage the id should be a constant for all items), then use the projection part to perform a $sum aggregation according to the formula above. Handling of the cm and m units can be solved with a conditional multiplication in the sum . If this does not work, you can use two aggregations: one for cm and one for m units, each with a filter in the pipeline then the aggregation afterwards. The required parts of the products are not mapped to C# classes yet. You need to do this. Note, that the field names in the BSON do not conform to the usual syntax, thus, when mapping to C# properties, you have to take care of name the properties identically, or use the [BsonElement(elementName: \"...\")] attribute. Use Fluent Api You must use the C# Fluent Api! Do not write the query using BsonDocument ! You may test your implementation with the tests provided in class TestExercise2 . The tests presume that the database is in its initial state. SUBMISSION Upload the changed C# source code.","title":"Exercise: MongoDB"},{"location":"homework/mongodb/#exercise-mongodb","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . Before you begin, create and initialize the database; use the steps the seminar exercises describe.","title":"Exercise: MongoDB"},{"location":"homework/mongodb/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/mongodb/#exercise-1-product-with-the-largest-total-value-2-points","text":"The task is to find the product that has the largest total value within a product category. The total value is the price of the product multiplied by the amount of the product in stock . You need to implement the following method in class ProductRepository . ( string , double? ) ProductWithLargestTotalValue ( ObjectId categoryId ) Let us check the test related to this exercise in file TestExercise1.cs to understand what is expected here. The method accepts a category filter as an argument; products have to be filtered for this category. The return value should be the name of the product (with the largest total value) and the total value itself. If there are no products in the specified category, the return value should be (null, null) . Use the aggregation pipeline of MongoDB. To see how this aggregation pipeline works, you can refer to the seminar material. You should build a pipeline consisting of the following stages: Filter the products for the specified category. Use a $match ( Match ) stage to specify the filter. Calculate for each product the total value (multiply the price and the stock) using a $project ( Project ) stage. Make sure to include the name of the product, you will need it for the final result. Order the items based on this calculated total value descending. Use a $sort ( SortByDescending ) stage. Since it is the largest value that we need, take the first item after sorting. Do not forget that there might not be any product in the specified category. Therefore you should use FirstOrDefault to fetch this item. If the syntax (string, double?) is unfamiliar: return ( \"test\" , 0.0 ); The function will return with these two values. Implement the repository method. The repository class receives the database as a parameter and saves the collection as a local variable in the class; use this field to manipulate the collection. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestDbFactory if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Product with the largest total value (2 points)"},{"location":"homework/mongodb/#exercise-2-estimating-storage-space-2-points","text":"The company is moving to a new location. We need to know whether the current stock can be moved and will fit into the new storage facility. Implement the method that calculates the total volume of all products in stock ! The products have the necessary information in description.product.package_parameters : Use this to calculate the total volume of all items: Use the information from package_parameters (and not from product_size ). A product might have multiple packages; this information is available in package_parameters.number_of_packages . This number shall be used as a multiplicator. Each product has a single size, and if it has multiple packages, then all packages are of the same size. The final total: for all products \u03a3 (product stock * number of packages * width * height * depth). If a product does not have these information, it's volume should be calculated as 0. Mind, that the size also has a unit: either cm or m , but the final value is expected in cubic meter. Implement method double GetAllProductsCumulativeVolume() that returns a single scalar total of the volume in cubic meters . The calculation should be performed by the database (not in C#); use the aggregation pipeline. Sum aggregation You will need the $group aggregation stage. Although, we do not need to group the products, still, this will allow us to aggregate all of them. Map each product into the same group (that is, in the $group stage the id should be a constant for all items), then use the projection part to perform a $sum aggregation according to the formula above. Handling of the cm and m units can be solved with a conditional multiplication in the sum . If this does not work, you can use two aggregations: one for cm and one for m units, each with a filter in the pipeline then the aggregation afterwards. The required parts of the products are not mapped to C# classes yet. You need to do this. Note, that the field names in the BSON do not conform to the usual syntax, thus, when mapping to C# properties, you have to take care of name the properties identically, or use the [BsonElement(elementName: \"...\")] attribute. Use Fluent Api You must use the C# Fluent Api! Do not write the query using BsonDocument ! You may test your implementation with the tests provided in class TestExercise2 . The tests presume that the database is in its initial state. SUBMISSION Upload the changed C# source code.","title":"Exercise 2: Estimating storage space (2 points)"},{"location":"homework/mssql/","text":"Exercise: MSSQL server-side programming \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . Prepare the database \u00b6 Create a new database with a name that matches your Neptun code . Run the database initialization script to create the tables in this database. Neptun code is important The exercise will ask you for a screenshot that must contain the database name with your Neptun code! Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Password expiry maintenance (2 points) \u00b6 Due to security reasons, we would like to enforce password expiry. For this, we will record the date when the password was last updated. Add a new column to the Customer table with the name PasswordExpiry storing a date: alter table [Customer] add [PasswordExpiry] datetime . Create a trigger that automatically fills the PasswordExpiry date column when the password value is updated. The new value should be the current date plus one year. The trigger shall calculate the value. When a new Customer is registered (inserted into the table), the column should always be populated automatically. However, when data is updated, only update the date if the password is changed. (E.g. if only the address is altered, the date should not be updated.) The trigger should only update the date for the inserted/modified record (it should not set it for all records in the table)! In this exercise, you may suppose that there is always one record inserted/modified at any time. Make sure to verify the behavior of the trigger under various circumstances. SUBMISSION Submit the code of the trigger in file f1.sql . This sql file should contain a single statement (a single create trigger command) without any use or go commands. Create a screenshot that displays sample records in the Customer table with the automatically populated date values. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f1.png and upload it as part of your submission! Exercise 2: Product recommended age (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The database contains an xml column with the name Description in the Product table. This column has values for some of the records. An example for the content is below: <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product> We want to extract the recommended_age and move it to a new column in the table. Add a new column to the Product table with name RecommendedAge storing a text: alter table [Product] add [RecommendedAge] nvarchar(200) . (Do not submit this statement in the solution.) Create a T-SQL script that extracts the content of the <recommended_age> tag from the xml and moves the value into the RecommendedAge column of the table. If the xml description is empty or there is no <recommended_age> tag, the column's value should be NULL . Otherwise, take the tag's text content (without the tag name), copy the value into the column, and remove the tag from the xml. You can presume that there is at most one <recommended_age> element in the xml. SUBMISSION Submit the T-SQL code in file f2.sql . Do not use a stored procedure in this exercise; create a simple T-SQL code block. This sql file should be executable by itself and should not contain any use or go commands. Create a screenshot that displays the content of the Product table after running the script. The new column and the populated values should be visible on the screenshot. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise: MSSQL server-side programming"},{"location":"homework/mssql/#exercise-mssql-server-side-programming","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here .","title":"Exercise: MSSQL server-side programming"},{"location":"homework/mssql/#prepare-the-database","text":"Create a new database with a name that matches your Neptun code . Run the database initialization script to create the tables in this database. Neptun code is important The exercise will ask you for a screenshot that must contain the database name with your Neptun code!","title":"Prepare the database"},{"location":"homework/mssql/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/mssql/#exercise-1-password-expiry-maintenance-2-points","text":"Due to security reasons, we would like to enforce password expiry. For this, we will record the date when the password was last updated. Add a new column to the Customer table with the name PasswordExpiry storing a date: alter table [Customer] add [PasswordExpiry] datetime . Create a trigger that automatically fills the PasswordExpiry date column when the password value is updated. The new value should be the current date plus one year. The trigger shall calculate the value. When a new Customer is registered (inserted into the table), the column should always be populated automatically. However, when data is updated, only update the date if the password is changed. (E.g. if only the address is altered, the date should not be updated.) The trigger should only update the date for the inserted/modified record (it should not set it for all records in the table)! In this exercise, you may suppose that there is always one record inserted/modified at any time. Make sure to verify the behavior of the trigger under various circumstances. SUBMISSION Submit the code of the trigger in file f1.sql . This sql file should contain a single statement (a single create trigger command) without any use or go commands. Create a screenshot that displays sample records in the Customer table with the automatically populated date values. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f1.png and upload it as part of your submission!","title":"Exercise 1: Password expiry maintenance (2 points)"},{"location":"homework/mssql/#exercise-2-product-recommended-age-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The database contains an xml column with the name Description in the Product table. This column has values for some of the records. An example for the content is below: <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product> We want to extract the recommended_age and move it to a new column in the table. Add a new column to the Product table with name RecommendedAge storing a text: alter table [Product] add [RecommendedAge] nvarchar(200) . (Do not submit this statement in the solution.) Create a T-SQL script that extracts the content of the <recommended_age> tag from the xml and moves the value into the RecommendedAge column of the table. If the xml description is empty or there is no <recommended_age> tag, the column's value should be NULL . Otherwise, take the tag's text content (without the tag name), copy the value into the column, and remove the tag from the xml. You can presume that there is at most one <recommended_age> element in the xml. SUBMISSION Submit the T-SQL code in file f2.sql . Do not use a stored procedure in this exercise; create a simple T-SQL code block. This sql file should be executable by itself and should not contain any use or go commands. Create a screenshot that displays the content of the Product table after running the script. The new column and the populated values should be visible on the screenshot. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise 2: Product recommended age (2 points)"},{"location":"homework/rest/","text":"Exercise: REST API and Web API \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here . Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Operations on products (2 points) \u00b6 The repository you cloned contains a skeleton application. Open the provided Visual Studio solution and start the application. A console window should appear that hosts the web application. While the web app is running, test it: open a browser to http://localhost:5000/api/product . The page should display a list of products in JSON format. Check the source code. Startup.cs initializes your application. This is an ASP.NET Core web application. There is no database used in this project to make things simpler. Class ProductRepository contains hard-wired data used for testing. ProductsController uses dependency injection to instantiate IProductRepository . Exercises: In class DAL.ProductRepository edit the field Neptun and add your Neptun code here. The string should contain the 6 characters of your Neptun code. IMPORTANT The data altered this way will be displayed on a screenshot in a later exercise; hence this step is essential. Create a new API endpoint for verifying whether a particular product specified by its ID exists. This new endpoint should respond to a HEAD HTTP query at URL /api/product/{id} . The HTTP response should be status code 200 or 404 (without any body either case). Create a new API endpoint that returns a single Product specified by its ID; the query is a GET query at URL /api/product/{id} and the response should be either 200 OK with the product as body, or 404 when the product is not found. Create a new API endpoint that deletes a Product specified by its ID; the query is a DELETE query at URL /api/product/{id} and the response should be either 204 with no content, or 404 when the product is not found. Create a new API endpoint for querying the total number of products. (Such an endpoint could be used, for example, by the UI for paging the list of products.) This should be a GET HTTP request to URL /api/product/-/count . The returned result should be a JSON serialized CountResult object with the correct count. Why is there a /- in the URL? In order to understand the need for this, let us consider what the URL should look like: we are querying products, so /api/product is the prefix, but what is the end of the URL? It could be /api/product/count . However, this clashes with /api/product/123 where we can get a particular product. In practice, the two URLs could work side-by-side here since the product ID is an integer, and the framework would recognize that an URL ending in /123 is to get a product and the /count is to get the counts. But this works only as long as the ID is an integer. If the product ID were a string, this would be more problematic. Our solution makes sure that the URLs do not clash. The /- is to indicate that there is no product ID. Note: the way URLs are matched to controller methods is more complicated. ASP.NET Core has a notion of priorities when trying to find a controller method for an URL. This priority can be modified on the [Http*] attributes by setting the Order property . SUBMISSION Upload the changed source code. Create a screenshot from Postman (or any alternative tool you used for testing) that shows a successful query that fetches an existing product. The screenshot should display both the request and response with all information (request type, URL, response code, response body). Save the screenshot as f1.png and upload it as part of your submission! The response body must contain your Neptun code . Exercise 2: OpenAPI documentation (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. OpenAPI (formerly Swagger) is a REST API documentation tool. It is similar to the WSDL for Web Services. Its goal is to describe the API in a standardized format. In this exercise, you are required to create a OpenAPI specification and documentation for your REST API. Please follow the official Microsoft tutorial at: https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle Make sure to use Swashbuckle . The swagger.json should be generated by your application (you need not write it), and it should be available at URL /swagger/v1/swagger.json . Set up Swagger UI , which should be available at URL /neptun . To achieve this, when configuring UseSwaggerUI set the RoutePrefix as your Neptun code all lower-case . (You can ignore the \"Customize and extend\" parts in the tutorial.) When ready, start the web application and check swagger.json at URL http://localhost:5000/swagger/v1/swagger.json , then open SwaggerUI too at http://localhost:5000/neptun . Test the \u201cTry it out\u201d in SwaggerUI: it will send out the query that your backend will serve. SUBMISSION Upload the changed source code. Make sure to upload the altered csproj file too; it contains a new NuGet package added in this exercise. Create a screenshot of SwaggerUI open in the browser. Make sure that the URL is visible and that it contains /neptun with your Neptun code. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise: REST API and Web API"},{"location":"homework/rest/#exercise-rest-api-and-web-api","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Check the required software and tools here .","title":"Exercise: REST API and Web API"},{"location":"homework/rest/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/rest/#exercise-1-operations-on-products-2-points","text":"The repository you cloned contains a skeleton application. Open the provided Visual Studio solution and start the application. A console window should appear that hosts the web application. While the web app is running, test it: open a browser to http://localhost:5000/api/product . The page should display a list of products in JSON format. Check the source code. Startup.cs initializes your application. This is an ASP.NET Core web application. There is no database used in this project to make things simpler. Class ProductRepository contains hard-wired data used for testing. ProductsController uses dependency injection to instantiate IProductRepository . Exercises: In class DAL.ProductRepository edit the field Neptun and add your Neptun code here. The string should contain the 6 characters of your Neptun code. IMPORTANT The data altered this way will be displayed on a screenshot in a later exercise; hence this step is essential. Create a new API endpoint for verifying whether a particular product specified by its ID exists. This new endpoint should respond to a HEAD HTTP query at URL /api/product/{id} . The HTTP response should be status code 200 or 404 (without any body either case). Create a new API endpoint that returns a single Product specified by its ID; the query is a GET query at URL /api/product/{id} and the response should be either 200 OK with the product as body, or 404 when the product is not found. Create a new API endpoint that deletes a Product specified by its ID; the query is a DELETE query at URL /api/product/{id} and the response should be either 204 with no content, or 404 when the product is not found. Create a new API endpoint for querying the total number of products. (Such an endpoint could be used, for example, by the UI for paging the list of products.) This should be a GET HTTP request to URL /api/product/-/count . The returned result should be a JSON serialized CountResult object with the correct count. Why is there a /- in the URL? In order to understand the need for this, let us consider what the URL should look like: we are querying products, so /api/product is the prefix, but what is the end of the URL? It could be /api/product/count . However, this clashes with /api/product/123 where we can get a particular product. In practice, the two URLs could work side-by-side here since the product ID is an integer, and the framework would recognize that an URL ending in /123 is to get a product and the /count is to get the counts. But this works only as long as the ID is an integer. If the product ID were a string, this would be more problematic. Our solution makes sure that the URLs do not clash. The /- is to indicate that there is no product ID. Note: the way URLs are matched to controller methods is more complicated. ASP.NET Core has a notion of priorities when trying to find a controller method for an URL. This priority can be modified on the [Http*] attributes by setting the Order property . SUBMISSION Upload the changed source code. Create a screenshot from Postman (or any alternative tool you used for testing) that shows a successful query that fetches an existing product. The screenshot should display both the request and response with all information (request type, URL, response code, response body). Save the screenshot as f1.png and upload it as part of your submission! The response body must contain your Neptun code .","title":"Exercise 1: Operations on products (2 points)"},{"location":"homework/rest/#exercise-2-openapi-documentation-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. OpenAPI (formerly Swagger) is a REST API documentation tool. It is similar to the WSDL for Web Services. Its goal is to describe the API in a standardized format. In this exercise, you are required to create a OpenAPI specification and documentation for your REST API. Please follow the official Microsoft tutorial at: https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle Make sure to use Swashbuckle . The swagger.json should be generated by your application (you need not write it), and it should be available at URL /swagger/v1/swagger.json . Set up Swagger UI , which should be available at URL /neptun . To achieve this, when configuring UseSwaggerUI set the RoutePrefix as your Neptun code all lower-case . (You can ignore the \"Customize and extend\" parts in the tutorial.) When ready, start the web application and check swagger.json at URL http://localhost:5000/swagger/v1/swagger.json , then open SwaggerUI too at http://localhost:5000/neptun . Test the \u201cTry it out\u201d in SwaggerUI: it will send out the query that your backend will serve. SUBMISSION Upload the changed source code. Make sure to upload the altered csproj file too; it contains a new NuGet package added in this exercise. Create a screenshot of SwaggerUI open in the browser. Make sure that the URL is visible and that it contains /neptun with your Neptun code. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise 2: OpenAPI documentation (2 points)"},{"location":"lecture-notes/adonet/","text":"ADO.NET data access \u00b6 What is ADO.NET ? \u00b6 In data-driven applications, it is important for the data access layer to provide simple and convenient access to the database, making it easier to create complex queries. And all this should be as independent of the database engine itself as possible. The ADO.NET ( ActiveX Data Object ) developed by Microsoft is a data access class library designed to meet these needs. As part of .NET , it provides a rich toolset for building data-driven applications, providing easy access to relational databases, regardless of the specific type of database . ADO.NET is a powerful tool because it enables unified, database engine-independent access. The library contains interfaces and abstract classes that have many implementations (e.g., for Microsoft SQL Server or OleDB ). And there are also third-party implementations compatible with ADO.NET for the databases that there is no built-in support. The Entity Framework is also based on ADO.NET in the background. The place of ADO.NET in a data-driven application architecture is as follows: ADO.NET provides services in the data access layer and handles communication with the database engine in the background using drivers installed on the system and operating system services (such as a network connection). The typical building blocks of data access libraries are: Connection - the database specific connection to the server Command - injection-safe command sent to the server ResultSet - result set returned by the server Exception - errors thrown by the library These will be discussed in depth below. Connection \u00b6 The ADO.NET library provides the IDbConnection interface to represent database connections. It contains the functions needed to manage the connection to the database, such as Open() , Close() , or BeginTransaction() . This interface is implemented by database engine-specific connection classes, such as the SqlConnection class, which implements the connection to Microsoft SQL Server. We need to know that setting up a new connection to a database is relatively expensive (opening a new network connection, negotiating protocols with the server, authentication, etc.). Therefore, we use connection pooling , where, after using and closing a connection, we do not discard it but put it back in a pool to reuse it later. The availability of connection pool depends on the implementation; it supported by the MS SQL Server and OleDD implementations. Connection pools are created for every connection string (so not per database). This is important if the application does not use a single static connection string (but connects on behalf of the user, for example). We also have to understand the problem of connection leak , which means that a connection is left open after use (we do not call Close() ), so it is not returned to the pool, which prohibits future reuse. If we do leak connections this way, the pool will soon run out of connections, and the application will stop working due to not being able to talk to the database. This problem must be avoided by closing or disposing of connections safely (see sample code later). We need the aforementioned connection string to connect to the database. It is a text variable that describes the parameters used to connect to the database, such as a username, password, or server address. Connection strings have database server-specific syntax and may also be a point of attacks . The connection string can be stored as text in the configuration file, or the application code can build it. Below is a sample code creating a connection and using a ConnectionStringBuilder : var builder = new SqlConnectionStringBuilder (); builder . UserID = \"User\" ; builder . Password = \"Pw\" ; builder . DataSource = \"database.server.hu\" ; builder . InitialCatalog = \"datadriven\" ; var conn = new SqlConnection ( builder . ConnectionString ); conn . Open (); ... // queries conn . Close (); // must close at the end - see a better solution later Command \u00b6 After establishing the connection to the database, we want to run queries. To do this, ADO.NET provides the IDbCommand interface, which represents a command. The implementations of this interface, such as SqlCommand , just like the connection, are specific to the database server. Creating the command \u00b6 By setting the following main properties of an IDbCommand we can configure how the given command will be interpreted: CommandType : there are three types StoredProcedure query the entire table ( TableDirect ) SQL query ( Text ) - default CommandText : the text of the command or the name of a stored procedure Connection : database connection Transaction : the transaction CommandTimeout : timeout for waiting to the result ( 30 seconds by default ) Parameters : parameters to prevent SQL injection attack Note that the command must specify the connection. Also note, that the transaction is also a property of the command. This is because it is up to the developer to decide whether to consider a particular command as part of a transaction. Execution \u00b6 Once we have a command object, we execute it. Depending on the expected return value, we can choose from several options (methods on the command object): ExecuteReader : query multiple records ExecuteScalar : query a single scalar value ExecuteNonQuery : a query that does not return a result (e.g., INSERT ) - instead, it returns the number of rows affected, e.g., in case of deletion, it is possible to decide whether the operation was successful (whether the record to be deleted was found) ExecuteXmlReader (MS SQL Server): returns an XML document ( XmlReader object), the result is a single XML field of a record You can also reuse commands after calling Command.Prepare() . It prepares the command to run on the server-side, but it is only worth it if we run the same statement (possibly with different parameter values). Sample code for using the command: // establish the connection ... var command = new SqlCommand (); command . Connection = connection ; // setting command.Connection it not enecessary if we use the connection to instantiate // the command as: command = connection.CreateCommand() command . CommandType = CommandType . StoredProcedure ; command . CommandText = \"SalesByCategory\" ; // name of the stored procedure /* equivalent syntax var command = new SqlCommand() { Connection = connection, CommandType = CommandType.StoredProcedure, CommandText = \"SalesByCategory\" }*/ // protection agains SQL injection var parameter = new SqlParameter (); parameter . ParameterName = \"@CategoryName\" ; // matches the stored procedures argument name parameter . SqlDbType = SqlDbType . NVarChar ; parameter . Value = categoryName ; // assign value from a C# variable command . Parameters . Add ( parameter ); var reader = command . ExecuteReader (); // see processing the results later Transactions \u00b6 Transactions in ADO.NET do not need to be initiated with the begin tran SQL statement. ADO.NET provides methods to create and manage the transaction, as shown in the code snippet below. Also, note the using blocks to properly and securely close resources. // ... creating the connection string using ( var connection = new SqlConnection ( connectionString )) { connection . Open (); // let us not forget this, instantiation does not open the connection var transaction = connection . BeginTransaction (); // parameter might include a name for the transaction and/or the isolation level var command = new SqlCommand () { Connection = connection , CommandText = \"INSERT into CarTable (Description) VALUES('...')\" , Transaction = transaction , } try { command . ExecuteNonQuery (); // MUST commit a successful transaction transaction . Commit (); Console . WriteLine ( \"Transaction finished!\" ); } catch ( Exception commitException ) { Console . WriteLine ( \"Commit Exception: \" , commitException . ToString ()); // The rollback below is not necessary. The system performs it automatically // for any non-committed transaction. The code below is only a possibility. try { transaction . RollBack (); } catch ( Exception rollBackException ) { Console . WriteLine ( \"Rollback Exception: \" , rollBackException . ToString ()); } } } Transaction timeout The total time of all ADO.NET transactions is limited by the setting in the MachineConfig . This is a system-wide setting that applies to all .NET applications running on a system, so it is not a good idea to change this setting. Long-running transactions are to be avoided anyway. A transaction usually belongs to a single Connection object, but we can also create a transaction involving multiple persistent resource managers (other databases, message queues, anything that supports transactions). At this point, we would be talking about distributed transaction management, which requires an external transaction manager, such as the Microsoft Distributed Transaction Coordinator ( MS DTC ). Such cases should also be avoided generally. NULL values \u00b6 How do we know that the result of our query is an empty set? And how do we know that a column contains no value? In .NET we usually check if a value equals null . However, a NULL value in a database is represented differently by ADO.NET \u200b\u200bdepending on the underlying type (int, string, etc.). So how do we check to see if there is a value in the query result? If we want to check if the result set of a query contains any records, we can do so by examining the bool property DataReader.HasRows . To examine the value of a particular column in the result set, we can use reader[\"column name\"].IsNull or reader.IsDbNull(index) . And if we want to manually insert a value of NULL into a new record, we can use, for example, SqlString.Null or DBNull.Value in the C# code. Query result \u00b6 ADO.NET offers two ways of fetching data from a database and working with it: DataReader and DataSet . The main difference between the two solutions is how these use the database connection. These two models are, in general, called: connection-based ( DataReader ) and connection-less ( DataSet ) data access. In the connection-based model, the connection to the database is maintained throughout the queries as long as we work with the data. While in the connection-less model modifications are performed in a DataSet , which is synchronized with the database itself (establishing the connection only for the duration of the synchronization). Both options have advantages and disadvantages, which we will discuss in the following sections. DataReader \u00b6 Here, we need a connection to the database to fetch the required data from the database. The connection remains open only for a short time, during with we query fresh data and usually convert it to some other internal representation. Processing steps: open the connection run command(s) to query data process the results (typically: convert the data to business entities) close the reader close the connection The flow of data using the DataReader is as follows: Sample code using a DataReader : using ( var conn = new SqlConnection ( connectionString )) { var command = new SqlCommand () { Connection = conn , CommandText = \"SELECT ID, NAME FROM Product\" } conn . Open (); using ( var reader = command . ExecuteReader ()) { while ( reader . Read ()) { Console . WriteLine ( \"{0}\\t{1}\" , reader [ \"ID\" ], reader [ \"Name\" ]); // typically rather create a business entity and add it to a list in memory } // no need for reader.Close() thanks to the using block } // no need for conn.Close() thanks to the using block } There are a few things worth paying attention to! The value of reader[\"ID\"] is an object , not a string or an int. We can use reader.Get***(query_in_column_index) instead, where we must specify the data type ( String , Int32 , etc.). If the types are not compatible (e.g., the column is nvarchar in the database, but we want to read it as int32 ), we will get a runtime exception . If there is a NULL value in the database, we will also get a runtime error when using the reader.Get*** methods. Instead, we should use reader.IsDBNull(query_in_column_index) to verify, and if it is true , we can use the appropriate null value instead. Advantages the data is fetched directly from the database; hence, it is up-to-date less painful concurrency management, as fresh the data is fetched needs less memory (compared to DataSets; see later) Disadvantages needs an open network connection while operations are being performed - thus, it should not be too long poor scalability with the number of connections - therefore, the connections should be used only for a short time DataSet \u00b6 A DataSet can be considered as a kind of cache , or in-memory data storage. We use an adapter (such as SqlDataAdapter ) to retrieve data from the database and store it in the Dataset , then we close the connection to the database. We can then make work with the data, event make changes to it within the DataSet , and then update the database with the changes using a new connection. It is worth noting that during the time between retrieval and update anyone can modify the same data in the database, thus the disadvantages of DataSet is having to manage conflicts and concurrent data access issues covered previously in transaction management. The steps of working with a DataSet: Open a connection Fill the DataSet with part of the database Close the connection Work with the DataSet (e.g., display and edit in a user interface) - this may take longer Open a new connection Synchronize changes Close the connection The operation of data access in this model using an adapter is shown in the figure below. The flow of data using the DataSet is as follows: Sample code for working with a DataSet : var dataSet = new DataSet (); var adapter = new SqlDataAdapter (); // open connection, populate the dataset, close the connection using ( var conn = new SqlConnection ( connectionString )) { adapter . SelectCommand = new SqlCommand ( \"SELECT * FROM Product\" , conn ); conn . Open (); adapter . Fill ( dataSet ); } ------------------------------------------------------- // working with the data // typically uinvolves UI; this is just a sample foreach ( var row in dataSet . Tables [ \"Product\" ]. Rows ) { Console . WriteLine ( \"{0}\\t{1}\" , row [ \"ID\" ], row [ \"Name\" ]); row [ \"Name\" ] = \"new value\" ; } ------------------------------------------------------- // at a later point in time, such as after a \"Save\" button in clicked // open connection, synchronize data, close connection using ( var conn = new SqlConnection ( connectionString )) { conn . Open (); adapter . Update ( dataSet ); //dataSet.AcceptChanges(); -- would only update the dataset, but not the database } It is worth noting that the adapter only communicates with the database via Command s. An adapter can use multiple such Command s, so we can even work with multiple Connections towards multiple databases with the same DataSet . Advantages does not need a long-running connection Disadvantages there may be conflicts during saving the changes data in the DataSet may be stale has larger memory footprint - the reason why we do not use it in server applications Risks \u00b6 SQL injection \u00b6 SQL injection is a severe error in an application when a query is created without sanitizing the values of parameters. Parameter values can come from the client side, with user-selected or user-specified data. This can cause a problem if a malicious user writes an SQL command into a field from which we would expect something else. For example, we would expect a username, but instead Monkey92); DROP TABLE Users; - value is received. If we were to include this text and insert it into our SQL statement, we would also execute drop table , thereby deleting an entire table. This is a serious mistake ! SOLUTION Using parameters (see the Command section for an example). Connection string \u00b6 Creating a connection string has a flaw similar to SQL injection . Suppose we ask the user for some kind of data ( e.g., username, password ). In this case, we do not know exactly what we will get. The connection string consists of key-value pairs and many databases apply the last-wins principle. In practice, this means that if more than one value is specified in a string for the same key, the last one takes effect. That is, if after the username and password a key-value pair is added that already appears in the string before, the new value overwrites the old one. This carries a risk, since a malicious user is able to inject specified parameters into the connection string. SOLUTION Using ConnectionStringBuilder (see Connection section ). Connection leak \u00b6 If we do not close all Connections , any time the code containing the not closed connection is executed, we will retrieve a Connection from the pool without returning it. When the pool is emptied, the application will be stuck not being able to talk to the database at all. This is an error that is hard to spot because it \"only\" happens after the application running for a certain amount of time - and almost never on the developer's machine. SOLUTION using block to open the connection, as this will close the connection at the end of the block (see Transaction section example , or DataReader , or DataSet ) A DataReader must be closed in the same way.","title":"ADO.NET data access"},{"location":"lecture-notes/adonet/#adonet-data-access","text":"","title":"ADO.NET data access"},{"location":"lecture-notes/adonet/#what-is-adonet","text":"In data-driven applications, it is important for the data access layer to provide simple and convenient access to the database, making it easier to create complex queries. And all this should be as independent of the database engine itself as possible. The ADO.NET ( ActiveX Data Object ) developed by Microsoft is a data access class library designed to meet these needs. As part of .NET , it provides a rich toolset for building data-driven applications, providing easy access to relational databases, regardless of the specific type of database . ADO.NET is a powerful tool because it enables unified, database engine-independent access. The library contains interfaces and abstract classes that have many implementations (e.g., for Microsoft SQL Server or OleDB ). And there are also third-party implementations compatible with ADO.NET for the databases that there is no built-in support. The Entity Framework is also based on ADO.NET in the background. The place of ADO.NET in a data-driven application architecture is as follows: ADO.NET provides services in the data access layer and handles communication with the database engine in the background using drivers installed on the system and operating system services (such as a network connection). The typical building blocks of data access libraries are: Connection - the database specific connection to the server Command - injection-safe command sent to the server ResultSet - result set returned by the server Exception - errors thrown by the library These will be discussed in depth below.","title":"What is ADO.NET?"},{"location":"lecture-notes/adonet/#connection","text":"The ADO.NET library provides the IDbConnection interface to represent database connections. It contains the functions needed to manage the connection to the database, such as Open() , Close() , or BeginTransaction() . This interface is implemented by database engine-specific connection classes, such as the SqlConnection class, which implements the connection to Microsoft SQL Server. We need to know that setting up a new connection to a database is relatively expensive (opening a new network connection, negotiating protocols with the server, authentication, etc.). Therefore, we use connection pooling , where, after using and closing a connection, we do not discard it but put it back in a pool to reuse it later. The availability of connection pool depends on the implementation; it supported by the MS SQL Server and OleDD implementations. Connection pools are created for every connection string (so not per database). This is important if the application does not use a single static connection string (but connects on behalf of the user, for example). We also have to understand the problem of connection leak , which means that a connection is left open after use (we do not call Close() ), so it is not returned to the pool, which prohibits future reuse. If we do leak connections this way, the pool will soon run out of connections, and the application will stop working due to not being able to talk to the database. This problem must be avoided by closing or disposing of connections safely (see sample code later). We need the aforementioned connection string to connect to the database. It is a text variable that describes the parameters used to connect to the database, such as a username, password, or server address. Connection strings have database server-specific syntax and may also be a point of attacks . The connection string can be stored as text in the configuration file, or the application code can build it. Below is a sample code creating a connection and using a ConnectionStringBuilder : var builder = new SqlConnectionStringBuilder (); builder . UserID = \"User\" ; builder . Password = \"Pw\" ; builder . DataSource = \"database.server.hu\" ; builder . InitialCatalog = \"datadriven\" ; var conn = new SqlConnection ( builder . ConnectionString ); conn . Open (); ... // queries conn . Close (); // must close at the end - see a better solution later","title":"Connection"},{"location":"lecture-notes/adonet/#command","text":"After establishing the connection to the database, we want to run queries. To do this, ADO.NET provides the IDbCommand interface, which represents a command. The implementations of this interface, such as SqlCommand , just like the connection, are specific to the database server.","title":"Command"},{"location":"lecture-notes/adonet/#creating-the-command","text":"By setting the following main properties of an IDbCommand we can configure how the given command will be interpreted: CommandType : there are three types StoredProcedure query the entire table ( TableDirect ) SQL query ( Text ) - default CommandText : the text of the command or the name of a stored procedure Connection : database connection Transaction : the transaction CommandTimeout : timeout for waiting to the result ( 30 seconds by default ) Parameters : parameters to prevent SQL injection attack Note that the command must specify the connection. Also note, that the transaction is also a property of the command. This is because it is up to the developer to decide whether to consider a particular command as part of a transaction.","title":"Creating the command"},{"location":"lecture-notes/adonet/#execution","text":"Once we have a command object, we execute it. Depending on the expected return value, we can choose from several options (methods on the command object): ExecuteReader : query multiple records ExecuteScalar : query a single scalar value ExecuteNonQuery : a query that does not return a result (e.g., INSERT ) - instead, it returns the number of rows affected, e.g., in case of deletion, it is possible to decide whether the operation was successful (whether the record to be deleted was found) ExecuteXmlReader (MS SQL Server): returns an XML document ( XmlReader object), the result is a single XML field of a record You can also reuse commands after calling Command.Prepare() . It prepares the command to run on the server-side, but it is only worth it if we run the same statement (possibly with different parameter values). Sample code for using the command: // establish the connection ... var command = new SqlCommand (); command . Connection = connection ; // setting command.Connection it not enecessary if we use the connection to instantiate // the command as: command = connection.CreateCommand() command . CommandType = CommandType . StoredProcedure ; command . CommandText = \"SalesByCategory\" ; // name of the stored procedure /* equivalent syntax var command = new SqlCommand() { Connection = connection, CommandType = CommandType.StoredProcedure, CommandText = \"SalesByCategory\" }*/ // protection agains SQL injection var parameter = new SqlParameter (); parameter . ParameterName = \"@CategoryName\" ; // matches the stored procedures argument name parameter . SqlDbType = SqlDbType . NVarChar ; parameter . Value = categoryName ; // assign value from a C# variable command . Parameters . Add ( parameter ); var reader = command . ExecuteReader (); // see processing the results later","title":"Execution"},{"location":"lecture-notes/adonet/#transactions","text":"Transactions in ADO.NET do not need to be initiated with the begin tran SQL statement. ADO.NET provides methods to create and manage the transaction, as shown in the code snippet below. Also, note the using blocks to properly and securely close resources. // ... creating the connection string using ( var connection = new SqlConnection ( connectionString )) { connection . Open (); // let us not forget this, instantiation does not open the connection var transaction = connection . BeginTransaction (); // parameter might include a name for the transaction and/or the isolation level var command = new SqlCommand () { Connection = connection , CommandText = \"INSERT into CarTable (Description) VALUES('...')\" , Transaction = transaction , } try { command . ExecuteNonQuery (); // MUST commit a successful transaction transaction . Commit (); Console . WriteLine ( \"Transaction finished!\" ); } catch ( Exception commitException ) { Console . WriteLine ( \"Commit Exception: \" , commitException . ToString ()); // The rollback below is not necessary. The system performs it automatically // for any non-committed transaction. The code below is only a possibility. try { transaction . RollBack (); } catch ( Exception rollBackException ) { Console . WriteLine ( \"Rollback Exception: \" , rollBackException . ToString ()); } } } Transaction timeout The total time of all ADO.NET transactions is limited by the setting in the MachineConfig . This is a system-wide setting that applies to all .NET applications running on a system, so it is not a good idea to change this setting. Long-running transactions are to be avoided anyway. A transaction usually belongs to a single Connection object, but we can also create a transaction involving multiple persistent resource managers (other databases, message queues, anything that supports transactions). At this point, we would be talking about distributed transaction management, which requires an external transaction manager, such as the Microsoft Distributed Transaction Coordinator ( MS DTC ). Such cases should also be avoided generally.","title":"Transactions"},{"location":"lecture-notes/adonet/#null-values","text":"How do we know that the result of our query is an empty set? And how do we know that a column contains no value? In .NET we usually check if a value equals null . However, a NULL value in a database is represented differently by ADO.NET \u200b\u200bdepending on the underlying type (int, string, etc.). So how do we check to see if there is a value in the query result? If we want to check if the result set of a query contains any records, we can do so by examining the bool property DataReader.HasRows . To examine the value of a particular column in the result set, we can use reader[\"column name\"].IsNull or reader.IsDbNull(index) . And if we want to manually insert a value of NULL into a new record, we can use, for example, SqlString.Null or DBNull.Value in the C# code.","title":"NULL values"},{"location":"lecture-notes/adonet/#query-result","text":"ADO.NET offers two ways of fetching data from a database and working with it: DataReader and DataSet . The main difference between the two solutions is how these use the database connection. These two models are, in general, called: connection-based ( DataReader ) and connection-less ( DataSet ) data access. In the connection-based model, the connection to the database is maintained throughout the queries as long as we work with the data. While in the connection-less model modifications are performed in a DataSet , which is synchronized with the database itself (establishing the connection only for the duration of the synchronization). Both options have advantages and disadvantages, which we will discuss in the following sections.","title":"Query result"},{"location":"lecture-notes/adonet/#datareader","text":"Here, we need a connection to the database to fetch the required data from the database. The connection remains open only for a short time, during with we query fresh data and usually convert it to some other internal representation. Processing steps: open the connection run command(s) to query data process the results (typically: convert the data to business entities) close the reader close the connection The flow of data using the DataReader is as follows: Sample code using a DataReader : using ( var conn = new SqlConnection ( connectionString )) { var command = new SqlCommand () { Connection = conn , CommandText = \"SELECT ID, NAME FROM Product\" } conn . Open (); using ( var reader = command . ExecuteReader ()) { while ( reader . Read ()) { Console . WriteLine ( \"{0}\\t{1}\" , reader [ \"ID\" ], reader [ \"Name\" ]); // typically rather create a business entity and add it to a list in memory } // no need for reader.Close() thanks to the using block } // no need for conn.Close() thanks to the using block } There are a few things worth paying attention to! The value of reader[\"ID\"] is an object , not a string or an int. We can use reader.Get***(query_in_column_index) instead, where we must specify the data type ( String , Int32 , etc.). If the types are not compatible (e.g., the column is nvarchar in the database, but we want to read it as int32 ), we will get a runtime exception . If there is a NULL value in the database, we will also get a runtime error when using the reader.Get*** methods. Instead, we should use reader.IsDBNull(query_in_column_index) to verify, and if it is true , we can use the appropriate null value instead. Advantages the data is fetched directly from the database; hence, it is up-to-date less painful concurrency management, as fresh the data is fetched needs less memory (compared to DataSets; see later) Disadvantages needs an open network connection while operations are being performed - thus, it should not be too long poor scalability with the number of connections - therefore, the connections should be used only for a short time","title":"DataReader"},{"location":"lecture-notes/adonet/#dataset","text":"A DataSet can be considered as a kind of cache , or in-memory data storage. We use an adapter (such as SqlDataAdapter ) to retrieve data from the database and store it in the Dataset , then we close the connection to the database. We can then make work with the data, event make changes to it within the DataSet , and then update the database with the changes using a new connection. It is worth noting that during the time between retrieval and update anyone can modify the same data in the database, thus the disadvantages of DataSet is having to manage conflicts and concurrent data access issues covered previously in transaction management. The steps of working with a DataSet: Open a connection Fill the DataSet with part of the database Close the connection Work with the DataSet (e.g., display and edit in a user interface) - this may take longer Open a new connection Synchronize changes Close the connection The operation of data access in this model using an adapter is shown in the figure below. The flow of data using the DataSet is as follows: Sample code for working with a DataSet : var dataSet = new DataSet (); var adapter = new SqlDataAdapter (); // open connection, populate the dataset, close the connection using ( var conn = new SqlConnection ( connectionString )) { adapter . SelectCommand = new SqlCommand ( \"SELECT * FROM Product\" , conn ); conn . Open (); adapter . Fill ( dataSet ); } ------------------------------------------------------- // working with the data // typically uinvolves UI; this is just a sample foreach ( var row in dataSet . Tables [ \"Product\" ]. Rows ) { Console . WriteLine ( \"{0}\\t{1}\" , row [ \"ID\" ], row [ \"Name\" ]); row [ \"Name\" ] = \"new value\" ; } ------------------------------------------------------- // at a later point in time, such as after a \"Save\" button in clicked // open connection, synchronize data, close connection using ( var conn = new SqlConnection ( connectionString )) { conn . Open (); adapter . Update ( dataSet ); //dataSet.AcceptChanges(); -- would only update the dataset, but not the database } It is worth noting that the adapter only communicates with the database via Command s. An adapter can use multiple such Command s, so we can even work with multiple Connections towards multiple databases with the same DataSet . Advantages does not need a long-running connection Disadvantages there may be conflicts during saving the changes data in the DataSet may be stale has larger memory footprint - the reason why we do not use it in server applications","title":"DataSet"},{"location":"lecture-notes/adonet/#risks","text":"","title":"Risks"},{"location":"lecture-notes/adonet/#sql-injection","text":"SQL injection is a severe error in an application when a query is created without sanitizing the values of parameters. Parameter values can come from the client side, with user-selected or user-specified data. This can cause a problem if a malicious user writes an SQL command into a field from which we would expect something else. For example, we would expect a username, but instead Monkey92); DROP TABLE Users; - value is received. If we were to include this text and insert it into our SQL statement, we would also execute drop table , thereby deleting an entire table. This is a serious mistake ! SOLUTION Using parameters (see the Command section for an example).","title":"SQL injection"},{"location":"lecture-notes/adonet/#connection-string","text":"Creating a connection string has a flaw similar to SQL injection . Suppose we ask the user for some kind of data ( e.g., username, password ). In this case, we do not know exactly what we will get. The connection string consists of key-value pairs and many databases apply the last-wins principle. In practice, this means that if more than one value is specified in a string for the same key, the last one takes effect. That is, if after the username and password a key-value pair is added that already appears in the string before, the new value overwrites the old one. This carries a risk, since a malicious user is able to inject specified parameters into the connection string. SOLUTION Using ConnectionStringBuilder (see Connection section ).","title":"Connection string"},{"location":"lecture-notes/adonet/#connection-leak","text":"If we do not close all Connections , any time the code containing the not closed connection is executed, we will retrieve a Connection from the pool without returning it. When the pool is emptied, the application will be stuck not being able to talk to the database at all. This is an error that is hard to spot because it \"only\" happens after the application running for a certain amount of time - and almost never on the developer's machine. SOLUTION using block to open the connection, as this will close the connection at the end of the block (see Transaction section example , or DataReader , or DataSet ) A DataReader must be closed in the same way.","title":"Connection leak"},{"location":"lecture-notes/architecture/","text":"Data-driven systems and the three- or multi-tier architecture \u00b6 What is a data-driven system? \u00b6 Every software handles data in some sense since the computer memory stores data, and the software manipulates this data. But not all applications are data-driven. A system or an application is called data-driven if its main purpose is to manage data. In other words, the data-drive application is created to store, display, and manage data. The end-user uses this application to access the data within. A chess game app also stores data in memory: the state of the chessboard. But the chess game app is not created to manipulate this data. The game is designed so that a user can play chess. In a data-driven system, the data itself defines how the application operates. For example, based on a data record's specific attributes, deleting this record may be allowed or may be prohibited. Another example is how the Neptun system enables registration to an exam. The semester schedule, which defines the exam period, is data stored within the system itself. This schedule, stored as data, determines whether the end-user (here: the student) can register for an exam. The fact that the exam period starts on a different day each year does not mean that the software logic (i.e., software code) changes; the data makes the software behave differently. Data-driven system example \u00b6 The Neptun system is a typical example of a data-driven system. Its purpose is the management of all data related to courses, students, grades. Another example is Gmail : it manages emails, attachments, contacts, etc. Every functionality of the application is about managing and displaying these data. And, of course, the data is stored securely, and every change in the system is persisted (i.e., not lost). The structure of a data-driven system \u00b6 Let us consider Gmail as an example. We would like to build a system which is capable of: sending and receiving emails, has a web application and a connected mobile app too, the UI supports multiple display languages, we can attach files, attachments can be referenced from Google Drive too, we can delay sending a composed email, etc. Let us design How do you start developing such a complex application? This might be a complicated question. Let us begin with a more straightforward question. Supposing that the system already works, and now we want to add the delayed sending feature. How do we do this? We could start a timer when the send button is clicked, and this timer, after a minute, sends the email. This will not work if the browser is closed before the countdown is over. We could add the scheduled date of sending to the email as data. We can translate this as an architectural decision: delayed sending is not the responsibility of the user interface. We did not decide yet, which part of the application will be responsible, but we already know it must not be the UI. Let us consider a similar question. The received date of an email should be displayed to the user according to their preference, i.e., in Europe, 15:23 is the preferred date format, while other parts of the world might prefer 3:23 PM. Does this mean that the email as a data record has multiple received dates? Obviously not. The received date is a single date in a universal representation that is transformed by the UI to the appropriate format. To summarize, we established that there are functionalities that the UI is responsible for, and there are other functionalities that the UI has nothing to do with. Now we arrive at the three- or multi-layer architecture. The three- or multi-layered architecture \u00b6 Data-driven systems are usually built on the three- or multi-layered architecture. The two names are treaded as synonyms here. This architecture defines three main components: the presentation layer (or UI), the business layer, and the data access layer. Besides, the architecture also includes: a database or external data sources; and the so-called cross-cutting concerns (see later). The application components are organized such that each component belongs to a single layer, and each layer has its responsibilities. This logical grouping of components enables the software developer to design components with clear responsibilities and well-defined boundaries. Why multi-layered when it only has three? The multi-layered terminology enables each of the previously listed layers to be further decomposed into sub-layers depending on complexity. In other words, an architecture is multi-layered if it has more than two layers. (In the two-layered architecture, the UI and the business logic are not separated.) The layers not only have their responsibilities , but also define their interface provided to the layers on top of them. The data access layer specifies the operations the business layer can use to retrieve data; similarly the business layer defines the functionalities the presentation layer can build upon. Each layer is allowed to communicate only with the layer directly beneath . For example, the presentation layer is not allowed to execute a SQL query in the database. At the same time, the implementation behind the well-defined communication interface can change enabling easier maintenance of the software codebase. By having the software split into layers, we can also move the layers to multiple servers (e.g., to handle larger loads). The simplest form is when the presentation layer runs in a browser on the user's machine, while the rest is hosted on a remote server. The database is also frequently offloaded to a dedicated server. Running the various layers on separate servers is usually motivated by performance reasons. Layer / tier The name of the architecture distinguishes the logical and physical separation of the components. Layers mean logical separation hosted on a single machine. Tiers indicate that at least some of the components have dedicated servers. A system built on a well-designed architecture can be used and maintained over a long period. The separation of layers is neither a burden nor a set of mandatory rules. Instead, the layered architecture is a helpful guide for the software developer. When we develop a three-layered architecture, we must understand the layers, roles, and responsibilities. The layered architecture does not mean that a functionality is present in a single layer. Most features offered to the end-user have some display in the presentation layer, handle data in the business logic layer, and store data in the database. The codebase of a three-layered architecture also reflects the separation of the layers. Depending on the capabilities and the conventions on the given platform, the layers all have a dedicated project or package. This structure also enforced one-way dependency, as the dependency-graph of projects/packages usually does not allow circles. That is, if the business layer uses the data access layer, the latter one cannot use the former one. The three-layered architecture is not the only possibility for implementing a data-driven application. Small and simple applications can be build using the two-layered architecture, while larger and more complex applications usually need further separation (e.g., using the microservices architecture). The responsibilities of the layers \u00b6 Let us examine the layers in more detail. The following diagram represents the architecture. Source Microsoft Application Architecture Guide, 2 nd Edition, https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658109%28v%3dpandp.10%29 We will discuss the layers from bottom-to-top. Data sources \u00b6 The most common data source is a database . It can be a relational-, or a NoSQL database. Its main purpose is the stable, reliable and persistent storage of data. This database is usually a software from a well-known third-party. This component is often hosted on a dedicated server accessible through a local network. Sometimes our application might also work with data outside of our database, hosted by third-party services , that we use similarly to databases. For example, you can attach files in Gmail from Google Drive. Gmail, in this example, fetches the list of available files from Google Drive for the user to select the attachment. Google drive is not a database, yet it is used as a data source. These kinds of external services are grouped with our database, in the architectural sense, because they provide data storage and retrieval services, just like a traditional database. We have no information about their internal operations, and there is no need for users to understand it either. Thus, these services are treated similarly in our architecture. Recently, more and more modern database management systems communicate over HTTP and often offer REST-like interfaces. These trends tend to blur the line between databases and external data sources. Data access layer \u00b6 The responsibility of the data access layer, DAL in short, is to provide convenient access to our data. The main functionality offered here is the storage, retrieval, and modification of data items, data records. The data sources and the data access layer combined is often called the data layer . The data access components provide a bridge towards the databases. Their role is to hide the inherent complexity of data management, and provide these as a convenient service to the upper layers. This includes working with SQL commands, as well as mapping the scheme of the database to a different scheme, consumed by the business layer. When the data is not inside our database, the service agents provide similar services and handle the communication aspects with the external service. This entire layer is often built on a particular technology used to communicate with the database, such as ADO.NET, Entity Framework, or JDBC, JPA, etc. The source code in this layer is often tightly coupled with these data access technologies. It is essential to keep these implementations inside this layer and not let it \"leak\" out of here. IMPORTANT In well-designed systems SQL commands appear only in the data access layer; under no circumstances do other layers assemble or execute SQL queries. Since the data modeling scheme used in databases (i.e., the relational model) and the object-oriented modeling are based on different concepts, this layer is responsible for providing a mapping between the two worlds. The foreign keys used by the relational scheme are transformed into associations and compositions, and we may even need to perform data conversion between data types supported by the various systems. We will re-visit these issues later. Communication with an external system, whether is it is a database or a third-party service, requires specific techniques. For example, establishing network connections, performing handshakes, and managing the lifetime of these connections is important for performance reasons. Establishing certain kinds of connections, such as HTTP, are usually simple; but connecting to a database server using proprietary protocols may be more complex. Therefore it is the responsibility of the data access layer to manage these connections and use appropriate techniques, such as connection pooling, when necessary. These details are often automatically controlled by the libraries we use. The management of concurrent data accesses and related problems is also the responsibility if this layer. We will discuss this in detail later. We should keep in mind that multiple users usually use a three-layered application/system at the same time (just think of the Neptun system or a webshop), thus concurrent modifications can happen. We will discuss how this is handled and what type of issues we have to resolve. Business Layer \u00b6 The business layer is the \"heart\" of our application. The databases, the data access layer and the presentation layer are created so that the system can provide the services implemented in the business logic layer. This layer is always specific to the problem domain. The Neptun system manages exams, semester schedules, grades, etc.; a webshop will, on the other hand, manage products, orders, searches, etc. From a high-level point of view this layer is built of: business entities , business components , and business workflows . The entities contain the data of our domain. Depending on the goal of the system, entities might cover products and product rating (e.g., in a webshop), or courses and exams (e.g., in Neptun). The entities only store data; the components are responsible for manipulating these entities. The components implement the building blocks of the complex services offered by our system. Such a building block is, for example, finding a product in a webshop by name. The workflows are built on these basic services. The workflows represent the functionalities that the end-users will carry out. A workflow may use multiple components. A classic example of a workflow is the checkout procedure in a webshop: check the products, finalize the order, produce an invoice, send a confirmation email, etc. Service interfaces \u00b6 The architecture diagram above has a services sub-layer. This is considered to be part of the business layer. Its purpose is to provide an interface through which the business logic layer's services can be accessed. Generally, all layers have such interfaces towards the layer built on top of them. The business layer is not unique in this sense. However, it is common nowadays that a business layer has not one, but multiple such interfaces published. The reason for multiple service interfaces is the presence of multiple presentation layers. Just take Gmail as an example: it has a web frontend and mobile apps too. The UIs are similar, but they do not provide identical behavior; therefore, the services consumed by the presentation layers also vary. It is equally common that our application offers a UI and has public API (application programming interface) for allowing third-party integration. These APIs often offer different functionalities than the user interface and also frequently use other transport technologies; hence the need for a dedicated service interface. By our application publishing an API, it can effectively act as a data source for third-party applications. We will talk more about publishing services over various APIs. We will consider the web services and the REST technologies too. Presentation layer \u00b6 The terminologies presentation layer, UI, and user interface are commonly used interchanged. The responsibility of this layer is the presentation of the data to the end-user in a convenient fashion and the triggering of operations on these data. Visualizing the data must consider how the data is \"consumed.\" For example, when listing lots of records, the UI shall provide filtering and grouping too. Sorting and filtering Depending on the chosen technology stack, sorting and filtering may also involve other layers. When dealing with large data sets, it is usually not useful to send every record to the UI to perform filtering and sorting there. It would be an unnecessary network overhead, and some/most UI technologies are not exactly designed to handle large data sets. On the other hand, if the data set is not extensive, it is convenient to let the UI handle these aspects to provide faster and more fluent responses (not having to forward filtering to the database). While displaying the data, the presentation layer is also responsible for performing simple data transformations, such as the user-friendly display of dates. As we discussed previously, a date might be printed as \"15:23\" or as \"3:23 PM,\" or better yet, as \"15 minutes ago.\" Furthermore, the presentation layer also handles localization. Localization is about displaying all pieces of information according to a chosen culture, such as dates, currencies, numbers. And finally, the UI handles user interactions. When a button is clicked, the UI translates this to an operation it will request from the business layer. User input must be validated. Validation covers filling required fields, accepting only valid email addresses, handling expected number ranges, etc. Validation It is not enough to perform validation only in the user interface. Depending on the technology used, the UI can often be easily bypassed, and the services in the background can be called directly. If this happens and only the UI performs validation, invalid data can get into the system. Therefore the validation is repeated by the business layer too. Regardless, the UI should still perform validation to give instant feedback to the user. This layer is not discussed further in this course. Cross-cutting services / Cross-cutting concerns \u00b6 Cross-cutting services or cross-cutting concerns cover aspects of the application that are not specific to a single layer. When designing our system, we strive to have a unified solution to the problems raised here. Security \u00b6 Security-related services covert user authentication, authorization, tracing and auditing. Authentication answers the question \"who are you\" while authorization determines \"what you are allowed to do in this system.\" Authentication covers not only the authentication on the user interface. We need to authenticate ourselves in the database too, not to mention accessing a third-party external system. Therefore this aspect is present in multiple layers. We have various options. We can use custom authentication, directory-based authentication, or OAuth. After our system has authenticated a user, we can decide to use this identity in external services (e.g. Gmail fetches the user's files from Google Drive ) or use a central identity (e.g. sending an email notification in the name of a send-only central account). Authorization is about access control: whether users can perform specific actions in the system. The UI usually performs some level of authorization (e.g., to hide unavailable functionality), but as discussed with input validation, the business layer must repeat this process. It is crucial, of course, that these two procedures use the same ruleset. Tracing and auditing make sure that we can check who made specific changes in the system. Its main goal is to keep malicious users from erasing their tracks. Recording the steps and operations of a user may be formed in the business login layer as well as in the database. Operation \u00b6 Keeping operational aspects in mind helps build maintainable software. The operational aspect usually covers error handling, logging, monitoring, and configuration management. Centralized error management should catch all types of errors that are raised in an application. These errors need to be recorded (e.g., by logging them), and usually, the end-user needs to be notified (e.g., whether she should retry or wait for something else). Recording all exceptions is vital because errors raised in the lower layers of the application are not \"seen\" by anyone (but the end-user probably) unless these are adequately treated and recorded. Logging and monitoring help both diagnostics and seeing whether a system behaves as intended. Logging is usually performed by writing a text log file. Monitoring, on the other hand, records so-called KPIs, key performance indicators. For example, KPIs are the memory usage, the number of errors, the number of pending requests, etc. And finally, configuration management is about the control of the system configuration. Such configuration is, for example, server addresses (e.g., the database IP). Or we may also want to configure and change the background color of our UI centrally. The standard approach regarding configuration is to not hard-code them but instead offer re-configuration without re-compiling the application when the operational circumstances change. This might involve configuration files or more complicated configuration management tools. We will not deal with these operational aspects in more detail. Often the chosen platform offers solutions to these problems. Communication \u00b6 By communication, we mean the method and format of data exchange between the layers and the components. Choosing the correct approach depends not only on the architecture but on the deployment model too. If the layers are moved to separate servers, network-based communication is needed, while communication between components on the same server can be achieved with simpler methods. Today, most systems use network communication: most commonly to reach the database and other data sources, and frequently between the presentation layer and the service interfaces. Nowadays, most communication is HTTP-based, however when performance is a concern, TPC-based binary communication methods provide better alternatives. And in more complex systems where the layers themselves are distributed across servers too, messages queues are often used. Encryption is also a factor in communication. Communication over public networks must be encrypted. In case of the communication between the UI and the service interface, this typically means HTTPS/TLS. Backend and frontend \u00b6 When we are talking about data-driven systems we often speak about backend and frontend . The frontend is mostly the user interface, that is, the presentation layer (a web application hosted in a browser, a native mobile app, a thick-client desktop app, etc). This is what the user interacts with. The backend is the service that provides the data to the UI: the APIs, the business layer, the data access, and the databases. Depending on the chosen frontend technology, parts of the user interface might be created by the backend, though. This is called server-side rendering . In this course, we will talk about backend technologies. Questions to test your knowledge \u00b6 What are the layers in the three-layered architecture? What are their responsibilities? What are the cross-cutting services? Decide, whether the following statements are true or false: The presentation layer is responsible for validating data input. We shall try to avoid using SQL commands in the business layer. The layers in the three-layered architecture are always hosted on separate servers. The three-layered architecture becomes a multi-layered one when the business layer is moved to its own server. The layered architecture ensures that the implementation of the layers can change without this affecting the other layers. The frontend and the presentation layer are one and the same. Exception handling is important only in the business logic layer.","title":"Data-driven systems and the three- or multi-tier architecture"},{"location":"lecture-notes/architecture/#data-driven-systems-and-the-three-or-multi-tier-architecture","text":"","title":"Data-driven systems and the three- or multi-tier architecture"},{"location":"lecture-notes/architecture/#what-is-a-data-driven-system","text":"Every software handles data in some sense since the computer memory stores data, and the software manipulates this data. But not all applications are data-driven. A system or an application is called data-driven if its main purpose is to manage data. In other words, the data-drive application is created to store, display, and manage data. The end-user uses this application to access the data within. A chess game app also stores data in memory: the state of the chessboard. But the chess game app is not created to manipulate this data. The game is designed so that a user can play chess. In a data-driven system, the data itself defines how the application operates. For example, based on a data record's specific attributes, deleting this record may be allowed or may be prohibited. Another example is how the Neptun system enables registration to an exam. The semester schedule, which defines the exam period, is data stored within the system itself. This schedule, stored as data, determines whether the end-user (here: the student) can register for an exam. The fact that the exam period starts on a different day each year does not mean that the software logic (i.e., software code) changes; the data makes the software behave differently.","title":"What is a data-driven system?"},{"location":"lecture-notes/architecture/#data-driven-system-example","text":"The Neptun system is a typical example of a data-driven system. Its purpose is the management of all data related to courses, students, grades. Another example is Gmail : it manages emails, attachments, contacts, etc. Every functionality of the application is about managing and displaying these data. And, of course, the data is stored securely, and every change in the system is persisted (i.e., not lost).","title":"Data-driven system example"},{"location":"lecture-notes/architecture/#the-structure-of-a-data-driven-system","text":"Let us consider Gmail as an example. We would like to build a system which is capable of: sending and receiving emails, has a web application and a connected mobile app too, the UI supports multiple display languages, we can attach files, attachments can be referenced from Google Drive too, we can delay sending a composed email, etc. Let us design How do you start developing such a complex application? This might be a complicated question. Let us begin with a more straightforward question. Supposing that the system already works, and now we want to add the delayed sending feature. How do we do this? We could start a timer when the send button is clicked, and this timer, after a minute, sends the email. This will not work if the browser is closed before the countdown is over. We could add the scheduled date of sending to the email as data. We can translate this as an architectural decision: delayed sending is not the responsibility of the user interface. We did not decide yet, which part of the application will be responsible, but we already know it must not be the UI. Let us consider a similar question. The received date of an email should be displayed to the user according to their preference, i.e., in Europe, 15:23 is the preferred date format, while other parts of the world might prefer 3:23 PM. Does this mean that the email as a data record has multiple received dates? Obviously not. The received date is a single date in a universal representation that is transformed by the UI to the appropriate format. To summarize, we established that there are functionalities that the UI is responsible for, and there are other functionalities that the UI has nothing to do with. Now we arrive at the three- or multi-layer architecture.","title":"The structure of a data-driven system"},{"location":"lecture-notes/architecture/#the-three-or-multi-layered-architecture","text":"Data-driven systems are usually built on the three- or multi-layered architecture. The two names are treaded as synonyms here. This architecture defines three main components: the presentation layer (or UI), the business layer, and the data access layer. Besides, the architecture also includes: a database or external data sources; and the so-called cross-cutting concerns (see later). The application components are organized such that each component belongs to a single layer, and each layer has its responsibilities. This logical grouping of components enables the software developer to design components with clear responsibilities and well-defined boundaries. Why multi-layered when it only has three? The multi-layered terminology enables each of the previously listed layers to be further decomposed into sub-layers depending on complexity. In other words, an architecture is multi-layered if it has more than two layers. (In the two-layered architecture, the UI and the business logic are not separated.) The layers not only have their responsibilities , but also define their interface provided to the layers on top of them. The data access layer specifies the operations the business layer can use to retrieve data; similarly the business layer defines the functionalities the presentation layer can build upon. Each layer is allowed to communicate only with the layer directly beneath . For example, the presentation layer is not allowed to execute a SQL query in the database. At the same time, the implementation behind the well-defined communication interface can change enabling easier maintenance of the software codebase. By having the software split into layers, we can also move the layers to multiple servers (e.g., to handle larger loads). The simplest form is when the presentation layer runs in a browser on the user's machine, while the rest is hosted on a remote server. The database is also frequently offloaded to a dedicated server. Running the various layers on separate servers is usually motivated by performance reasons. Layer / tier The name of the architecture distinguishes the logical and physical separation of the components. Layers mean logical separation hosted on a single machine. Tiers indicate that at least some of the components have dedicated servers. A system built on a well-designed architecture can be used and maintained over a long period. The separation of layers is neither a burden nor a set of mandatory rules. Instead, the layered architecture is a helpful guide for the software developer. When we develop a three-layered architecture, we must understand the layers, roles, and responsibilities. The layered architecture does not mean that a functionality is present in a single layer. Most features offered to the end-user have some display in the presentation layer, handle data in the business logic layer, and store data in the database. The codebase of a three-layered architecture also reflects the separation of the layers. Depending on the capabilities and the conventions on the given platform, the layers all have a dedicated project or package. This structure also enforced one-way dependency, as the dependency-graph of projects/packages usually does not allow circles. That is, if the business layer uses the data access layer, the latter one cannot use the former one. The three-layered architecture is not the only possibility for implementing a data-driven application. Small and simple applications can be build using the two-layered architecture, while larger and more complex applications usually need further separation (e.g., using the microservices architecture).","title":"The three- or multi-layered architecture"},{"location":"lecture-notes/architecture/#the-responsibilities-of-the-layers","text":"Let us examine the layers in more detail. The following diagram represents the architecture. Source Microsoft Application Architecture Guide, 2 nd Edition, https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658109%28v%3dpandp.10%29 We will discuss the layers from bottom-to-top.","title":"The responsibilities of the layers"},{"location":"lecture-notes/architecture/#data-sources","text":"The most common data source is a database . It can be a relational-, or a NoSQL database. Its main purpose is the stable, reliable and persistent storage of data. This database is usually a software from a well-known third-party. This component is often hosted on a dedicated server accessible through a local network. Sometimes our application might also work with data outside of our database, hosted by third-party services , that we use similarly to databases. For example, you can attach files in Gmail from Google Drive. Gmail, in this example, fetches the list of available files from Google Drive for the user to select the attachment. Google drive is not a database, yet it is used as a data source. These kinds of external services are grouped with our database, in the architectural sense, because they provide data storage and retrieval services, just like a traditional database. We have no information about their internal operations, and there is no need for users to understand it either. Thus, these services are treated similarly in our architecture. Recently, more and more modern database management systems communicate over HTTP and often offer REST-like interfaces. These trends tend to blur the line between databases and external data sources.","title":"Data sources"},{"location":"lecture-notes/architecture/#data-access-layer","text":"The responsibility of the data access layer, DAL in short, is to provide convenient access to our data. The main functionality offered here is the storage, retrieval, and modification of data items, data records. The data sources and the data access layer combined is often called the data layer . The data access components provide a bridge towards the databases. Their role is to hide the inherent complexity of data management, and provide these as a convenient service to the upper layers. This includes working with SQL commands, as well as mapping the scheme of the database to a different scheme, consumed by the business layer. When the data is not inside our database, the service agents provide similar services and handle the communication aspects with the external service. This entire layer is often built on a particular technology used to communicate with the database, such as ADO.NET, Entity Framework, or JDBC, JPA, etc. The source code in this layer is often tightly coupled with these data access technologies. It is essential to keep these implementations inside this layer and not let it \"leak\" out of here. IMPORTANT In well-designed systems SQL commands appear only in the data access layer; under no circumstances do other layers assemble or execute SQL queries. Since the data modeling scheme used in databases (i.e., the relational model) and the object-oriented modeling are based on different concepts, this layer is responsible for providing a mapping between the two worlds. The foreign keys used by the relational scheme are transformed into associations and compositions, and we may even need to perform data conversion between data types supported by the various systems. We will re-visit these issues later. Communication with an external system, whether is it is a database or a third-party service, requires specific techniques. For example, establishing network connections, performing handshakes, and managing the lifetime of these connections is important for performance reasons. Establishing certain kinds of connections, such as HTTP, are usually simple; but connecting to a database server using proprietary protocols may be more complex. Therefore it is the responsibility of the data access layer to manage these connections and use appropriate techniques, such as connection pooling, when necessary. These details are often automatically controlled by the libraries we use. The management of concurrent data accesses and related problems is also the responsibility if this layer. We will discuss this in detail later. We should keep in mind that multiple users usually use a three-layered application/system at the same time (just think of the Neptun system or a webshop), thus concurrent modifications can happen. We will discuss how this is handled and what type of issues we have to resolve.","title":"Data access layer"},{"location":"lecture-notes/architecture/#business-layer","text":"The business layer is the \"heart\" of our application. The databases, the data access layer and the presentation layer are created so that the system can provide the services implemented in the business logic layer. This layer is always specific to the problem domain. The Neptun system manages exams, semester schedules, grades, etc.; a webshop will, on the other hand, manage products, orders, searches, etc. From a high-level point of view this layer is built of: business entities , business components , and business workflows . The entities contain the data of our domain. Depending on the goal of the system, entities might cover products and product rating (e.g., in a webshop), or courses and exams (e.g., in Neptun). The entities only store data; the components are responsible for manipulating these entities. The components implement the building blocks of the complex services offered by our system. Such a building block is, for example, finding a product in a webshop by name. The workflows are built on these basic services. The workflows represent the functionalities that the end-users will carry out. A workflow may use multiple components. A classic example of a workflow is the checkout procedure in a webshop: check the products, finalize the order, produce an invoice, send a confirmation email, etc.","title":"Business Layer"},{"location":"lecture-notes/architecture/#service-interfaces","text":"The architecture diagram above has a services sub-layer. This is considered to be part of the business layer. Its purpose is to provide an interface through which the business logic layer's services can be accessed. Generally, all layers have such interfaces towards the layer built on top of them. The business layer is not unique in this sense. However, it is common nowadays that a business layer has not one, but multiple such interfaces published. The reason for multiple service interfaces is the presence of multiple presentation layers. Just take Gmail as an example: it has a web frontend and mobile apps too. The UIs are similar, but they do not provide identical behavior; therefore, the services consumed by the presentation layers also vary. It is equally common that our application offers a UI and has public API (application programming interface) for allowing third-party integration. These APIs often offer different functionalities than the user interface and also frequently use other transport technologies; hence the need for a dedicated service interface. By our application publishing an API, it can effectively act as a data source for third-party applications. We will talk more about publishing services over various APIs. We will consider the web services and the REST technologies too.","title":"Service interfaces"},{"location":"lecture-notes/architecture/#presentation-layer","text":"The terminologies presentation layer, UI, and user interface are commonly used interchanged. The responsibility of this layer is the presentation of the data to the end-user in a convenient fashion and the triggering of operations on these data. Visualizing the data must consider how the data is \"consumed.\" For example, when listing lots of records, the UI shall provide filtering and grouping too. Sorting and filtering Depending on the chosen technology stack, sorting and filtering may also involve other layers. When dealing with large data sets, it is usually not useful to send every record to the UI to perform filtering and sorting there. It would be an unnecessary network overhead, and some/most UI technologies are not exactly designed to handle large data sets. On the other hand, if the data set is not extensive, it is convenient to let the UI handle these aspects to provide faster and more fluent responses (not having to forward filtering to the database). While displaying the data, the presentation layer is also responsible for performing simple data transformations, such as the user-friendly display of dates. As we discussed previously, a date might be printed as \"15:23\" or as \"3:23 PM,\" or better yet, as \"15 minutes ago.\" Furthermore, the presentation layer also handles localization. Localization is about displaying all pieces of information according to a chosen culture, such as dates, currencies, numbers. And finally, the UI handles user interactions. When a button is clicked, the UI translates this to an operation it will request from the business layer. User input must be validated. Validation covers filling required fields, accepting only valid email addresses, handling expected number ranges, etc. Validation It is not enough to perform validation only in the user interface. Depending on the technology used, the UI can often be easily bypassed, and the services in the background can be called directly. If this happens and only the UI performs validation, invalid data can get into the system. Therefore the validation is repeated by the business layer too. Regardless, the UI should still perform validation to give instant feedback to the user. This layer is not discussed further in this course.","title":"Presentation layer"},{"location":"lecture-notes/architecture/#cross-cutting-services-cross-cutting-concerns","text":"Cross-cutting services or cross-cutting concerns cover aspects of the application that are not specific to a single layer. When designing our system, we strive to have a unified solution to the problems raised here.","title":"Cross-cutting services / Cross-cutting concerns"},{"location":"lecture-notes/architecture/#security","text":"Security-related services covert user authentication, authorization, tracing and auditing. Authentication answers the question \"who are you\" while authorization determines \"what you are allowed to do in this system.\" Authentication covers not only the authentication on the user interface. We need to authenticate ourselves in the database too, not to mention accessing a third-party external system. Therefore this aspect is present in multiple layers. We have various options. We can use custom authentication, directory-based authentication, or OAuth. After our system has authenticated a user, we can decide to use this identity in external services (e.g. Gmail fetches the user's files from Google Drive ) or use a central identity (e.g. sending an email notification in the name of a send-only central account). Authorization is about access control: whether users can perform specific actions in the system. The UI usually performs some level of authorization (e.g., to hide unavailable functionality), but as discussed with input validation, the business layer must repeat this process. It is crucial, of course, that these two procedures use the same ruleset. Tracing and auditing make sure that we can check who made specific changes in the system. Its main goal is to keep malicious users from erasing their tracks. Recording the steps and operations of a user may be formed in the business login layer as well as in the database.","title":"Security"},{"location":"lecture-notes/architecture/#operation","text":"Keeping operational aspects in mind helps build maintainable software. The operational aspect usually covers error handling, logging, monitoring, and configuration management. Centralized error management should catch all types of errors that are raised in an application. These errors need to be recorded (e.g., by logging them), and usually, the end-user needs to be notified (e.g., whether she should retry or wait for something else). Recording all exceptions is vital because errors raised in the lower layers of the application are not \"seen\" by anyone (but the end-user probably) unless these are adequately treated and recorded. Logging and monitoring help both diagnostics and seeing whether a system behaves as intended. Logging is usually performed by writing a text log file. Monitoring, on the other hand, records so-called KPIs, key performance indicators. For example, KPIs are the memory usage, the number of errors, the number of pending requests, etc. And finally, configuration management is about the control of the system configuration. Such configuration is, for example, server addresses (e.g., the database IP). Or we may also want to configure and change the background color of our UI centrally. The standard approach regarding configuration is to not hard-code them but instead offer re-configuration without re-compiling the application when the operational circumstances change. This might involve configuration files or more complicated configuration management tools. We will not deal with these operational aspects in more detail. Often the chosen platform offers solutions to these problems.","title":"Operation"},{"location":"lecture-notes/architecture/#communication","text":"By communication, we mean the method and format of data exchange between the layers and the components. Choosing the correct approach depends not only on the architecture but on the deployment model too. If the layers are moved to separate servers, network-based communication is needed, while communication between components on the same server can be achieved with simpler methods. Today, most systems use network communication: most commonly to reach the database and other data sources, and frequently between the presentation layer and the service interfaces. Nowadays, most communication is HTTP-based, however when performance is a concern, TPC-based binary communication methods provide better alternatives. And in more complex systems where the layers themselves are distributed across servers too, messages queues are often used. Encryption is also a factor in communication. Communication over public networks must be encrypted. In case of the communication between the UI and the service interface, this typically means HTTPS/TLS.","title":"Communication"},{"location":"lecture-notes/architecture/#backend-and-frontend","text":"When we are talking about data-driven systems we often speak about backend and frontend . The frontend is mostly the user interface, that is, the presentation layer (a web application hosted in a browser, a native mobile app, a thick-client desktop app, etc). This is what the user interacts with. The backend is the service that provides the data to the UI: the APIs, the business layer, the data access, and the databases. Depending on the chosen frontend technology, parts of the user interface might be created by the backend, though. This is called server-side rendering . In this course, we will talk about backend technologies.","title":"Backend and frontend"},{"location":"lecture-notes/architecture/#questions-to-test-your-knowledge","text":"What are the layers in the three-layered architecture? What are their responsibilities? What are the cross-cutting services? Decide, whether the following statements are true or false: The presentation layer is responsible for validating data input. We shall try to avoid using SQL commands in the business layer. The layers in the three-layered architecture are always hosted on separate servers. The three-layered architecture becomes a multi-layered one when the business layer is moved to its own server. The layered architecture ensures that the implementation of the layers can change without this affecting the other layers. The frontend and the presentation layer are one and the same. Exception handling is important only in the business logic layer.","title":"Questions to test your knowledge"},{"location":"lecture-notes/async/","text":"Asynchronous queries and DTOs (sample WebAPI application) \u00b6 This topic discusses server-side asynchronous queries and the use of DTOs (Data Transfer Objects) through an example application. The web application is an ASP.NET Core WebApi server using Entity Framework data access. The functionality discussed here is the management of a webshop cart. Author The original author of this lecture note in Hungarian is M\u00e1t\u00e9 ZERGI. Asynchronous execution \u00b6 Most of our web applications use a database. When communicating with the database, we have to be aware that: the database might not always be available, the connection might not be stable and fast, and the database might be slow to respond. Therefore, we need to prepare to wait for the results queried from the database in our application. Using asynchronous techniques in the web application, we can make sure that the resources, such as the web server's threads, are used efficiently even while waiting for the database. Asynchronous execution vs. concurrent execution Asynchronous execution is not the same as concurrent execution. A web server always processes incoming requests concurrently (i.e., multiple requests are in the system at all times). On the other hand, asynchronous execution is about handling a single request efficiently by not blocking any thread for waiting to complete an I/O operation (such as database access, file access, or network communication). The database of the sample application \u00b6 The sample application presented here uses a simplified database structure as follows. For simplicity, the UserId of the carts is not a foreign key to a Users table, but a fixed constant of 1. Obviously, in a real-life example, UserId would be a foreign key. The Products stores the things the webshop sells; the Manufactureres contain the producers of these products; finally, OrderItems stores the content of the cart. Server application \u00b6 We would like to create a REST-compatible service for managing the webshop cart using ASP.NET Core WebApi and Entity Framework. Let us follow these steps: Create the C# model of the database tables, Create the database context, Create Data Transfer Objects that represent the information queried by the client in a format convenient for the client, Create WebApi controllers We will go through each of these steps. Create the C# model of the database tables \u00b6 The C# classes that map the database tables are usually placed into a folder often called Models in ASP.NET Core. The following is the class for the Products table. namespace WebshopApi.Models { public class Product { public string Name { get ; set ; } public int ManufacturerID { get ; set ; } public int Price { get ; set ; } public int ID { get ; set ; } } } The following is the class for the Manufacturers table. namespace WebshopApi.Models { public class Manufacturer { public string Name { get ; set ; } public int ID { get ; set ; } } } The following is the class for the OrderItems table. namespace WebshopApi.Models { public class OrderItem { public int ID { get ; set ; } public int ProductID { get ; set ; } public int CartID { get ; set ; } public int Pieces { get ; set ; } } } The following is the class for the Carts table. namespace WebshopApi.Models { public class Cart { public int ID { get ; set ; } public int UserID { get ; set ; } } } Note, that the sole purpose of these classes is to map the data exactly as in the database. Create the database context \u00b6 After mapping the tables, we can now create the class that will represent our database: the DbContext class. This class must inherit from the Entity Framework Core DbContext class. namespace WebshopApi.Models { public class WebshopContext : DbContext { public WebshopContext ( DbContextOptions < WebshopContext > options ) : base ( options ) { } public DbSet < Product > Products { get ; set ; } public DbSet < Manufacturer > Manufacturers { get ; set ; } public DbSet < Cart > Carts { get ; set ; } public DbSet < OrderItem > OrderItems { get ; set ; } } } Each table in the database corresponds to a DbSet property as defined above. Each DbSet specified the type of the entity it stores; e.g., DbSet<Products> will store entities of type Product . The DbContextOptions configures the access to the database, such as the connection string. This is usually configured in the Startup class: public class Startup { // ... // This method is called by the runtime to populate the services of the DI container public void ConfigureServices ( IServiceCollection services ) { services . AddDbContext < WebshopContext >( opt => opt . UseSqlServer ( @\"Data Source=(localdb)\\mssqllocaldb;Initial Catalog=Webshop;Integrated Security=True\" )); // ... } } Defining Data Transfer Objects \u00b6 We have the direct mapping of the database into C# classes. Let us now consider how does the cart of the webshop usually look like: it may contain multiple items. While the OrderItem class can represent a single item, our cart is a list of items. This list of items is what we shall describe with a so-called Data Transfer Object : it is a class that gathers data for the client . Definition: Data Transfer Object A container object that transfers data between application (here: between the client and the server). With the use of DTOs, we can pack all necessary information into one object, making it not only more convenient for the client, but also better in terms of performance: We only send information to the client that it really needs. Furthermore, a DTO can gather various information and send them all in one go. Let us consider, what information do we need to display the cart in the client: the products, the amount in the cart for each, and the total number of items. Class OrderItem has superfluous data that the client does not need: CartID and ID . Removing these properties we can arrive at a class very similar to OrderItem ; but it is still just one item of the cart. The class we can create this way is called CartItem . This CartItem has a Product that also stores some unnecessary data, and some properties that might need adding . For example, the manufacturer of the Product should contain the name of the manufacturer and not the ManufacturerID . Let us, therefore, create a new Product class, and our CartItem should store this class instead. These CartItem objects are gathered in a list , and let us add the total number of items in the cart. This will give us our last DTO, the UserCart . An instance of this UserCart is what that the client will receive. The DTO classes are usually separated from the database entities. Let us put these classes in a DTOs folder. CartItem class contains the data from an OrderItem without the unnecessary properties. namespace WebshopApi.DTOs { public class CartItem { public Product Product { get ; set ; } // This is the product that no longer has the ID of the manufacturer, but the name instead public int Amount { get ; set ; } // The amount in the cart } } The the matching Product DTO: namespace WebshopApi.DTOs { public class Product { public string ProductName { get ; set ; } // The product name, e.g., AB123 Full HD TV public string Manufacturer { get ; set ; } // A !!name!! of the manufacturer, e.g., BMETV public int Price { get ; set ; } // Price of the product public int ID { get ; set ; } // ID of the product } } Why is there an ID here? We might be curious why there is an ID here. An item in the cart is identified by the product itself. E.g., further details of the product in the cart can be queried by knowing this ID. We could create a new identifier for the item in the cart; but the product's ID is sufficient. UserCart collects all items, and adds a total number. namespace WebshopApi.DTOs { public class UserCart { public List < CartItem > CartPieces { get ; set ; } public int NumberOfItems { get ; set ; } } } By storing the CartItem s in a list, we make the job of the client easier. When rendering the contents of the cart the client code only needs to iterate through the array contents. This UserCart is created by gathering the required information, such as the product details, then adding up the number of items. Creating the WebApi controller \u00b6 The controllers are usually placed into the Controllers folder. Here, we have a single controller that uses the WebshopDbContext directly and handles the HTTP queries. This is where we can introduce asynchronous queries. Let us see an example right away: The following is a GET query to fetch all carts. [HttpGet] public async Task < ActionResult < IEnumerable < Cart >>> GetCarts () { var carts = await _context . Carts . ToListAsync (); return carts ; } Let us note the async keyword in the declaration and the Task return type, along with the await instruction in the body. Together, these are called async-await . Let us make sense of all these: The method returns a list of Cart instances, that is, IEnumerable<Cart> ; Which, according to WebApi controller conventions, is wrapped in an ActionResult ; And this whole thing is wrapped in a Task . This one is due to the asynchronous behavior. Although this seems complicated, every part of this is for a different reason. Let us examine the asynchronous behavior: the Task type, and the await keyword. This definition of the method yields a so-called promise (some languages use this terminology) that represents the result of a task that will be completed in the future. Why do we need this? Because this makes the execution of the controller method asynchronous. When the execution arrives at an await keyword, the thread that processes the request, will stop further processing of this query and will start processing a new query instead. Ok, but again, why? Because we know that the operation \"behind\" the await will take time: it has to go to the database and fetch data from there. If the thread stopped here to wait for the result, it would be wasting resources. Instead of having the thread wait for the result, the task is handed off to a system in the background (the operating system and the .NET asynchronous I/O subsystem - will not go into details here), and we request notification when the results are available. Once this happens (the results from the database are, in fact, ready), the processing of the query will continue. In other words, the threads used by our application will always do useful work instead of waiting (or being suspended due to waiting). Consequently, this means that serving the HTTP requests need fewer operating system threads, therefore making better use of available computational resources. The previous method can be simplified in syntax by getting rid of the local variable and returning the Task directly. Functionally, this implementation works (almost) identically, but the one above makes the explanation easier. [HttpGet] public Task < ActionResult < IEnumerable < Carts >>> GetCarts () { return _context . Carts . ToListAsync (); // no await and the method declaration has no async } The ***Async methods The methods to fetch data from the database (e.g., ToList , First , All , Find , etc.) all have their ...Async pairs. These methods provide the basis for asynchronous execution. We will not discuss the execution in more details. What we need to remember, is that in order for our controller method to be asynchronous, there must be an asynchronous operation \"underneath\" (here: in Entity Framework). Let us also see a complex example: gather all data of the cart: [HttpGet(\"{id}\")] public async Task < ActionResult < UserCart >> GetCart ( int id ) { // asynchronous query to find the cart var cartRecord = await _context . Carts . FindAsync ( id ); if ( cartRecord == null ) return NotFound (); // build the query var productsquery = from p1 in _context . Products join m1 in _context . Manufacturers on p1 . ManufacturerID equals m1 . ID select new Product ( p1 . ID , m1 . Name , p1 . Name , p1 . Price ); // create the Product DTO // asynchronous evaluation var products = await productsquery . ToListAsync (). ConfigureAwait ( false ); // asynchronous request to get order items var orderitemsquery = from oi in _context . OrderItems where oi . CartID == cartRecord . ID select oi ; var orderitems = await orderitemsquery . ToListAsync (). ConfigureAwait ( false ); // further operations are synchronous, as every result in in memory already // Find the products in the cart // match them to the order items and crate a CartItem DTO var cartitems = products . Join ( orderitems , p => p . ID , oi => oi . ProductID , ( p , v ) => new CartItem ( p , v . Pieces )). ToList (); // Finally, the result is a UserCart DTO return new UserCart () { CartPieces = cartitems , NumberOfItems = cartitems . Count () } } Note, how all asynchronous method calls are await -ed! But once we have all the data from the database, we can continue in a synchronous fashion. The ConfigureAwait method The ConfigureAwait(false) gives us further options regarding performance optimization. With this option we signal that the await -ed result set can be processed by any available thread, not just the one that started the processing originally. In server-side applications this is usually the correct behavior, however, this is not true for all asynchronous operations (e.g., UI threads are usually special and in that case not any thread can continue). For more details, see: https://devblogs.microsoft.com/dotnet/configureawait-faq/ . Finally, let us see an example using FirstOrDefaultAsync to process a POST query that alters the contents of the cart (adds or removed items): // DTO describing the inputs of the operation namespace WebshopApi.Models { public class PostCartItemArgs { public int CartId { get ; set ; } public int ProductId { get ; set ; } public int Amount { get ; set ; } } } ////////////////////////////////////////////////////////////////////////////// // The HTTP handler in the controller [HttpPost] public async Task < IActionResult > PostCartItem ([ FromBody ] PostCartItemArgs data ) { // find the cart by ID var cart = await _context . Carts . FindAsync ( data . CartId ). ConfigureAwait ( false ); if ( cart == null ) return NotFound (); // find the order items in this cart matching the provided product var orderitemquery = from oi in _context . OrderItems where ( oi . CartID == data . Id && oi . ProductID == data . ProductId ) select oi ; // FirstOrDefault so that if there is no match, the result is null var orderitem = await orderitemquery . FirstOrDefaultAsync (). ConfigureAwait ( false ); if ( orderitem == null ) { // If there was no such item in the cart, add a new OrderItem _context . OrderItems . Add ( new OrderItems { CartID = data . Id , Amount = data . Amount , ProductID = data . ProductId }); } else { // If there is an item in the cart orderitem . Amount += data . Amount ; // If the amount is zero, it means, removed from the cart if ( orderitem . Amount == 0 ) _context . OrderItems . Remove ( orderitem ); } await _context . SaveChangesAsync (); // await here too, since this will need to go to the database return NoContent (); } Who waits for the Task result of the controller? Every async method has to be await -ed somewhere. When it comes to a WebApi controller, it will be the ASP.NET Core framework that invokes this method, and it will \"wait\" for the result before serializing it to JSON to send to the client. Sample code \u00b6 The source of the sample application is available here: https://github.com/mzergi/WebshopApi/","title":"Asynchronous queries and DTOs (sample WebAPI application)"},{"location":"lecture-notes/async/#asynchronous-queries-and-dtos-sample-webapi-application","text":"This topic discusses server-side asynchronous queries and the use of DTOs (Data Transfer Objects) through an example application. The web application is an ASP.NET Core WebApi server using Entity Framework data access. The functionality discussed here is the management of a webshop cart. Author The original author of this lecture note in Hungarian is M\u00e1t\u00e9 ZERGI.","title":"Asynchronous queries and DTOs (sample WebAPI application)"},{"location":"lecture-notes/async/#asynchronous-execution","text":"Most of our web applications use a database. When communicating with the database, we have to be aware that: the database might not always be available, the connection might not be stable and fast, and the database might be slow to respond. Therefore, we need to prepare to wait for the results queried from the database in our application. Using asynchronous techniques in the web application, we can make sure that the resources, such as the web server's threads, are used efficiently even while waiting for the database. Asynchronous execution vs. concurrent execution Asynchronous execution is not the same as concurrent execution. A web server always processes incoming requests concurrently (i.e., multiple requests are in the system at all times). On the other hand, asynchronous execution is about handling a single request efficiently by not blocking any thread for waiting to complete an I/O operation (such as database access, file access, or network communication).","title":"Asynchronous execution"},{"location":"lecture-notes/async/#the-database-of-the-sample-application","text":"The sample application presented here uses a simplified database structure as follows. For simplicity, the UserId of the carts is not a foreign key to a Users table, but a fixed constant of 1. Obviously, in a real-life example, UserId would be a foreign key. The Products stores the things the webshop sells; the Manufactureres contain the producers of these products; finally, OrderItems stores the content of the cart.","title":"The database of the sample application"},{"location":"lecture-notes/async/#server-application","text":"We would like to create a REST-compatible service for managing the webshop cart using ASP.NET Core WebApi and Entity Framework. Let us follow these steps: Create the C# model of the database tables, Create the database context, Create Data Transfer Objects that represent the information queried by the client in a format convenient for the client, Create WebApi controllers We will go through each of these steps.","title":"Server application"},{"location":"lecture-notes/async/#create-the-c-model-of-the-database-tables","text":"The C# classes that map the database tables are usually placed into a folder often called Models in ASP.NET Core. The following is the class for the Products table. namespace WebshopApi.Models { public class Product { public string Name { get ; set ; } public int ManufacturerID { get ; set ; } public int Price { get ; set ; } public int ID { get ; set ; } } } The following is the class for the Manufacturers table. namespace WebshopApi.Models { public class Manufacturer { public string Name { get ; set ; } public int ID { get ; set ; } } } The following is the class for the OrderItems table. namespace WebshopApi.Models { public class OrderItem { public int ID { get ; set ; } public int ProductID { get ; set ; } public int CartID { get ; set ; } public int Pieces { get ; set ; } } } The following is the class for the Carts table. namespace WebshopApi.Models { public class Cart { public int ID { get ; set ; } public int UserID { get ; set ; } } } Note, that the sole purpose of these classes is to map the data exactly as in the database.","title":"Create the C# model of the database tables"},{"location":"lecture-notes/async/#create-the-database-context","text":"After mapping the tables, we can now create the class that will represent our database: the DbContext class. This class must inherit from the Entity Framework Core DbContext class. namespace WebshopApi.Models { public class WebshopContext : DbContext { public WebshopContext ( DbContextOptions < WebshopContext > options ) : base ( options ) { } public DbSet < Product > Products { get ; set ; } public DbSet < Manufacturer > Manufacturers { get ; set ; } public DbSet < Cart > Carts { get ; set ; } public DbSet < OrderItem > OrderItems { get ; set ; } } } Each table in the database corresponds to a DbSet property as defined above. Each DbSet specified the type of the entity it stores; e.g., DbSet<Products> will store entities of type Product . The DbContextOptions configures the access to the database, such as the connection string. This is usually configured in the Startup class: public class Startup { // ... // This method is called by the runtime to populate the services of the DI container public void ConfigureServices ( IServiceCollection services ) { services . AddDbContext < WebshopContext >( opt => opt . UseSqlServer ( @\"Data Source=(localdb)\\mssqllocaldb;Initial Catalog=Webshop;Integrated Security=True\" )); // ... } }","title":"Create the database context"},{"location":"lecture-notes/async/#defining-data-transfer-objects","text":"We have the direct mapping of the database into C# classes. Let us now consider how does the cart of the webshop usually look like: it may contain multiple items. While the OrderItem class can represent a single item, our cart is a list of items. This list of items is what we shall describe with a so-called Data Transfer Object : it is a class that gathers data for the client . Definition: Data Transfer Object A container object that transfers data between application (here: between the client and the server). With the use of DTOs, we can pack all necessary information into one object, making it not only more convenient for the client, but also better in terms of performance: We only send information to the client that it really needs. Furthermore, a DTO can gather various information and send them all in one go. Let us consider, what information do we need to display the cart in the client: the products, the amount in the cart for each, and the total number of items. Class OrderItem has superfluous data that the client does not need: CartID and ID . Removing these properties we can arrive at a class very similar to OrderItem ; but it is still just one item of the cart. The class we can create this way is called CartItem . This CartItem has a Product that also stores some unnecessary data, and some properties that might need adding . For example, the manufacturer of the Product should contain the name of the manufacturer and not the ManufacturerID . Let us, therefore, create a new Product class, and our CartItem should store this class instead. These CartItem objects are gathered in a list , and let us add the total number of items in the cart. This will give us our last DTO, the UserCart . An instance of this UserCart is what that the client will receive. The DTO classes are usually separated from the database entities. Let us put these classes in a DTOs folder. CartItem class contains the data from an OrderItem without the unnecessary properties. namespace WebshopApi.DTOs { public class CartItem { public Product Product { get ; set ; } // This is the product that no longer has the ID of the manufacturer, but the name instead public int Amount { get ; set ; } // The amount in the cart } } The the matching Product DTO: namespace WebshopApi.DTOs { public class Product { public string ProductName { get ; set ; } // The product name, e.g., AB123 Full HD TV public string Manufacturer { get ; set ; } // A !!name!! of the manufacturer, e.g., BMETV public int Price { get ; set ; } // Price of the product public int ID { get ; set ; } // ID of the product } } Why is there an ID here? We might be curious why there is an ID here. An item in the cart is identified by the product itself. E.g., further details of the product in the cart can be queried by knowing this ID. We could create a new identifier for the item in the cart; but the product's ID is sufficient. UserCart collects all items, and adds a total number. namespace WebshopApi.DTOs { public class UserCart { public List < CartItem > CartPieces { get ; set ; } public int NumberOfItems { get ; set ; } } } By storing the CartItem s in a list, we make the job of the client easier. When rendering the contents of the cart the client code only needs to iterate through the array contents. This UserCart is created by gathering the required information, such as the product details, then adding up the number of items.","title":"Defining Data Transfer Objects"},{"location":"lecture-notes/async/#creating-the-webapi-controller","text":"The controllers are usually placed into the Controllers folder. Here, we have a single controller that uses the WebshopDbContext directly and handles the HTTP queries. This is where we can introduce asynchronous queries. Let us see an example right away: The following is a GET query to fetch all carts. [HttpGet] public async Task < ActionResult < IEnumerable < Cart >>> GetCarts () { var carts = await _context . Carts . ToListAsync (); return carts ; } Let us note the async keyword in the declaration and the Task return type, along with the await instruction in the body. Together, these are called async-await . Let us make sense of all these: The method returns a list of Cart instances, that is, IEnumerable<Cart> ; Which, according to WebApi controller conventions, is wrapped in an ActionResult ; And this whole thing is wrapped in a Task . This one is due to the asynchronous behavior. Although this seems complicated, every part of this is for a different reason. Let us examine the asynchronous behavior: the Task type, and the await keyword. This definition of the method yields a so-called promise (some languages use this terminology) that represents the result of a task that will be completed in the future. Why do we need this? Because this makes the execution of the controller method asynchronous. When the execution arrives at an await keyword, the thread that processes the request, will stop further processing of this query and will start processing a new query instead. Ok, but again, why? Because we know that the operation \"behind\" the await will take time: it has to go to the database and fetch data from there. If the thread stopped here to wait for the result, it would be wasting resources. Instead of having the thread wait for the result, the task is handed off to a system in the background (the operating system and the .NET asynchronous I/O subsystem - will not go into details here), and we request notification when the results are available. Once this happens (the results from the database are, in fact, ready), the processing of the query will continue. In other words, the threads used by our application will always do useful work instead of waiting (or being suspended due to waiting). Consequently, this means that serving the HTTP requests need fewer operating system threads, therefore making better use of available computational resources. The previous method can be simplified in syntax by getting rid of the local variable and returning the Task directly. Functionally, this implementation works (almost) identically, but the one above makes the explanation easier. [HttpGet] public Task < ActionResult < IEnumerable < Carts >>> GetCarts () { return _context . Carts . ToListAsync (); // no await and the method declaration has no async } The ***Async methods The methods to fetch data from the database (e.g., ToList , First , All , Find , etc.) all have their ...Async pairs. These methods provide the basis for asynchronous execution. We will not discuss the execution in more details. What we need to remember, is that in order for our controller method to be asynchronous, there must be an asynchronous operation \"underneath\" (here: in Entity Framework). Let us also see a complex example: gather all data of the cart: [HttpGet(\"{id}\")] public async Task < ActionResult < UserCart >> GetCart ( int id ) { // asynchronous query to find the cart var cartRecord = await _context . Carts . FindAsync ( id ); if ( cartRecord == null ) return NotFound (); // build the query var productsquery = from p1 in _context . Products join m1 in _context . Manufacturers on p1 . ManufacturerID equals m1 . ID select new Product ( p1 . ID , m1 . Name , p1 . Name , p1 . Price ); // create the Product DTO // asynchronous evaluation var products = await productsquery . ToListAsync (). ConfigureAwait ( false ); // asynchronous request to get order items var orderitemsquery = from oi in _context . OrderItems where oi . CartID == cartRecord . ID select oi ; var orderitems = await orderitemsquery . ToListAsync (). ConfigureAwait ( false ); // further operations are synchronous, as every result in in memory already // Find the products in the cart // match them to the order items and crate a CartItem DTO var cartitems = products . Join ( orderitems , p => p . ID , oi => oi . ProductID , ( p , v ) => new CartItem ( p , v . Pieces )). ToList (); // Finally, the result is a UserCart DTO return new UserCart () { CartPieces = cartitems , NumberOfItems = cartitems . Count () } } Note, how all asynchronous method calls are await -ed! But once we have all the data from the database, we can continue in a synchronous fashion. The ConfigureAwait method The ConfigureAwait(false) gives us further options regarding performance optimization. With this option we signal that the await -ed result set can be processed by any available thread, not just the one that started the processing originally. In server-side applications this is usually the correct behavior, however, this is not true for all asynchronous operations (e.g., UI threads are usually special and in that case not any thread can continue). For more details, see: https://devblogs.microsoft.com/dotnet/configureawait-faq/ . Finally, let us see an example using FirstOrDefaultAsync to process a POST query that alters the contents of the cart (adds or removed items): // DTO describing the inputs of the operation namespace WebshopApi.Models { public class PostCartItemArgs { public int CartId { get ; set ; } public int ProductId { get ; set ; } public int Amount { get ; set ; } } } ////////////////////////////////////////////////////////////////////////////// // The HTTP handler in the controller [HttpPost] public async Task < IActionResult > PostCartItem ([ FromBody ] PostCartItemArgs data ) { // find the cart by ID var cart = await _context . Carts . FindAsync ( data . CartId ). ConfigureAwait ( false ); if ( cart == null ) return NotFound (); // find the order items in this cart matching the provided product var orderitemquery = from oi in _context . OrderItems where ( oi . CartID == data . Id && oi . ProductID == data . ProductId ) select oi ; // FirstOrDefault so that if there is no match, the result is null var orderitem = await orderitemquery . FirstOrDefaultAsync (). ConfigureAwait ( false ); if ( orderitem == null ) { // If there was no such item in the cart, add a new OrderItem _context . OrderItems . Add ( new OrderItems { CartID = data . Id , Amount = data . Amount , ProductID = data . ProductId }); } else { // If there is an item in the cart orderitem . Amount += data . Amount ; // If the amount is zero, it means, removed from the cart if ( orderitem . Amount == 0 ) _context . OrderItems . Remove ( orderitem ); } await _context . SaveChangesAsync (); // await here too, since this will need to go to the database return NoContent (); } Who waits for the Task result of the controller? Every async method has to be await -ed somewhere. When it comes to a WebApi controller, it will be the ASP.NET Core framework that invokes this method, and it will \"wait\" for the result before serializing it to JSON to send to the client.","title":"Creating the WebApi controller"},{"location":"lecture-notes/async/#sample-code","text":"The source of the sample application is available here: https://github.com/mzergi/WebshopApi/","title":"Sample code"},{"location":"lecture-notes/di/","text":"Dependency Injection ASP.NET Core environment \u00b6 Definition Dependency Injection (DI) is programming technique that makes a class independent of its dependencies. It's a key enabler for decomposing an application into loosely coupled components. More precisely: Dependency Injection is a mechanism to decouple the creation of dependency graphs for a class from its class definition . Of course, the above definition is very abstract, and based on the short definition it's hard to understand what problems DI is trying to solve, and how DI is trying to solve them. In the following chapters, we will use an example to put DI into context and to learn the basics of the DI related services build into ASP.NET Core. Goals of DI Facilitated extensibility and maintainability Improved unit testability Facilitated code reuse Sample application The sample C# code is available here: https://github.com/bmeviauac01/todoapi-di-sample Example phase 1 - service class with wired in dependencies \u00b6 In this example, based on code snippets we look at parts of a to-do list (TODO) application that sends to-do item related email notifications. Note: The code is minimalistic for succinctness. The \"entry point\" of our example is the SendReminderIfNeeded operation of the ToDoService class. // Class for managing todo items public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // It checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { NotificationService notificationService = new NotificationService ( smtpAddress ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } // ... } // Entity class, encapsulates information about a todo task public class TodoItem { // Database key public long Id { get ; set ; } // Name/description of the task public string Name { get ; set ; } // Indicates if the task has been completed public bool IsComplete { get ; set ; } // It's possible to assign a contact person to a task: -1 indicated no contact // person is assigned, otherwise the id of the contact person public int LinkedContactId { get ; set ; } = - 1 ; } In the code above ( ToDoService.SendReminderIfNeeded ) we see that the essential logic of sending an e-mail is to be found in the NotificationService class. Indeed, this class is at the center of our investigation. The following code snippet describes the code for the NotificationService class and its dependencies: // Class for sending notifications class NotificationService { // Dependencies of the class EMailSender _emailSender ; Logger _logger ; ContactRepository _contactRepository ; public NotificationService ( string smtpAddress ) { _logger = new Logger (); _emailSender = new EMailSender ( _logger , smtpAddress ); _contactRepository = new ContactRepository (); } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } // Class supporting loggin public class Logger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail notifications public class EMailSender { Logger _logger ; string _smtpAddress ; public EMailSender ( Logger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } A few general thoughts: The NotificationService class has several dependencies ( EMailSender , Logger , ContactRepository classes) and it implements its services based on these dependency classes. Dependency classes may have additional dependencies: EMailSender is a great example of this, it's dependent on the Logger class. Note: NotificationService , EMailSender , Logger , ContactRepository classes are considered service classes because they contain business logic, not just encapsulate data, such as TodoItem . As we could see the SendEmailReminder operation is actually served by an object graph, where NotificationService is the root object, it has three dependencies, and its dependencies have further dependencies. The following figure illustrates this object graph: Note One may ask why we considered NotificationService , and not ToDoService as the root object. Actually it just depends on our viewpoint: for simplicity we considered ToDoService as an entry point (a \"client\") for fulfilling a request, so that we have less classes to put under scrutiny. In a real life application we probably would consider ToDoService as part of the dependency graph as well. Let's review the key features of this solution: The class instantiates its dependencies itself Class depends on the specific type of its dependencies (and not on interfaces, \"abstractions\") This approach has a couple of significant and rather painful drawbacks: Rigidity, lack of extensibility . NotificationService (without modification) cannot work with other mailing, logging and contact repository implementations (but only with the the wired in EMailSender , Logger and ContactRepository classes). That is, e.g. we can't use it with any other logging component, or e.g. use it with a contact repository that operates via a different data source/storage mechanism. Lack of unit testability . The NotificationService (without modification) cannot be unit tested. This would require replacing the EMailSender , Logger and ContactRepository dependencies with variants that provide fixed/expected responses for a given input. Keep in mind that unit testing is about testing the behavior of a class independently from its dependencies. In our example, instead of using the database base ContactRepository, we would need a ContactRepository implementation that could serve requests very quickly from memory with values supporting the specific test cases. There is one more subtle inconvenience that is hard to notice at first sight. In our example we had to provide the smtpAddress parameter to the NotificationService constructor, so that it can forward it to its EMailSender dependency. However, smtpAddress is a parameter completely meaningless for NotificationService , it has nothing to do with this piece of information. Unfortunately, we are forced to pass smtpAddress thorough NotificationService , as NotificationService is the class instantiating the EMailSender object. We could eliminate this by somehow instantiating EMailSender independently of NotificationService . In the next steps, we redesign our solution so that we can eliminate most of the downsides of the current rigid approach. Example phase 2 - service class with manual dependency injection \u00b6 Redesign our former solution the functional requirements are unchanged. The most important principles of transformation are the following: Dependencies will be based on abstractions/interfaces Classes will no longer instantiate their dependencies themselves Let's jump right into the code of the improved solution and analyze the differences: public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // Checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { var logger = new Logger (); var emailSender = new EMailSender ( logger , smtpAddress ); var contactRepository = new ContactRepository (); NotificationService notificationService = new NotificationService ( logger , emailSender , contactRepository ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } } // Class for sending notifications class NotificationService { // Dependencies of the class IEMailSender _emailSender ; ILogger _logger ; IContactRepository _contactRepository ; public NotificationService ( ILogger logger , IEMailSender emailSender , IContactRepository contactRepository ) { _logger = logger ; _emailSender = emailSender ; _contactRepository = contactRepository ; } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } #region Contracts (abstractions) // Interface for logging public interface ILogger { void LogInformation ( string text ); void LogError ( string text ); } // Interface for sending e-mail public interface IEMailSender { void SendMail ( string to , string subject , string message ); } // Interface for Contact entity persistence public interface IContactRepository { string GetContactEMailAddress ( int contactId ); } #endregion #region Implementations // Class for logging public class Logger : ILogger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail public class EMailSender : IEMailSender { ILogger _logger ; string _smtpAddress ; public EMailSender ( ILogger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository : IContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } #endregion We improved out previous solution in the following points: The NotificationService class no longer instantiates its dependencies itself, but receives them in constructor parameters. Interfaces (abstractions) have been introduced to manage dependencies The NotificationService class gets its dependencies in the form of interfaces. When a class receives its dependencies externally (e.g. via constructor parameters), it is called DEPENDENCY INJECTION (DI). In our case, the classes get their class dependencies in constructor parameters: this specific form of DI is called CONSTRUCTOR INJECTION . This is the most common - and most recommended - way to inject dependency. (Alternatively, for example, we could use property injection, which is based on a public property setter to set a specific dependency of a class). In our current solution, NotificationService dependencies are instantiated by the (direct) USER of the class (which is the ToDoService class). Primarily this is the reason why we are still facing with a few problems: The user of NotificationService objects, which is the ToDoService class, is still dependent on the implementation types (since it has to instantiate the Logger , EMailSender and ContactRepository classes). If we use the Logger , EMailSender and ContactRepository classes at multiple places in your application, we must instantiate them explicitly. In other words: at each and every place where have to create an ILogger , IEMailSender or IContactRepository implementation class, we have to make a decision which implementation to choose. This is essentially a special case of code duplication, the decision should appear only once in our code. Our goal, in contrast, would be to determine at a single central location what type implementation to use for an abstraction (interface type) everywhere in the application (e.g. for ILogger create an Logger instance everywhere, for IMailSender create an EMailSender everywhere). This would allow us to easily review our abstraction-to-implementation mappings at one place. Moreover, if we want to change one of the mappings (e.g. using AdvancedLogger instead of Logger for ILogger ) we could achieve that by making a single change at a central location. Example phase 3 - dependency injection based on .NET Core Dependency Injection \u00b6 We need some extra help from our framework to solve the two problems we concluded the previous chapter with: an Inversion of Control (IoC) container. Dependency Injection container is a widely used alternative name for the same tool/technique. In an IoC container we can store abstraction type -> implementation type mappings, such as ILogger->Logger, IMailSender->EMailSender, etc. This is called the REGISTER step. And then based on these mappings create an implementation type for a specific abstraction type (e.g. Logger for an ILogger ). This is called the RESOLVE step. In more detail: REGISTER : Register dependency mappings (e.g. ILogger -> Logger , IMailSender -> EMailSender ) into an IoC container, once, at a centralized location, at application startup. This is the REGISTER step of the DI process. Note: This solves \"problem 2\" pointed out at the end of the previous chapter: the mappings are centralized, and not scattered all over the application code base. RESOLVE : When we need an implementation object at runtime in our application, we ask the container for an implementation by specifying the abstraction (interface) type (e.g., by providing ILogger as a key, the container returns an object of class Logger ). The resolve step is typically done at the \" entry point \" of the application (e.g. in case of WebApi on the receival of web requests, we will look into this later). The resolve step is performed only for the ROOT OBJECT (e.g. for the appropriate Controller class in case of WebApi). The container creates and returns a root object and all its dependencies and all its indirect dependencies: an entire object graph is generated. This process is called AUTOWIRING . Note: In case of Web API calls, the Resolve step is executed by the Asp.Net framework and is mostly hidden from the developer: all we see is that our controller class is automatically instantiated and all constructor parameters are automatically populated (with the help of the IoC container based on the mappings of the REGISTER step). Fortunately, .NET Core has a built in IoC container based dependency injection service. Now we elucidate and illustrate the complete mechanism (register and resolve steps) using our enhanced e-mail notification solution as an example. 1) REGISTER step (registering dependencies) \u00b6 In an Asp.Net Core environment, dependencies are registered by the ConfigureServices (IServiceCollection services) function of our Startup class, namely by the AddSingleton , AddTransient and AddScoped operations of IServiceCollection . First, let's focus on the most exciting parts of ConfigureServices : public class Startup { public void ConfigureServices ( IServiceCollection services ) { // ... services . AddSingleton < ILogger , Logger >(); services . AddTransient < INotificationService , NotificationService >(); services . AddScoped < IContactRepository , ContactRepository >(); services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); // ... } Startup.ConfigureServices is called by the framework at application startup. We receive an IServiceCollection services object as parameter: this represents the IoC container created and initialized by the framework. We can register our own dependency mappings into this container. The services . AddSingleton < ILogger , Logger >(); line registers an ILogger -> Logger type mapping, and the Logger is registered as a singleton, as we used the AddSingleton operation for registration. This means that if we later ask the container for an ILogger object (provide ILogger as key at the resolve step), we will get a Logger object from the container, and always the same instance . The services . AddTransient < INotificationService , NotificationService >(); line registers an INotificationService -> NotificationService transient type mapping, as we used the AddTransient operation for registration. This means that if we later ask the container for an INotificationService object (provide INotificationService as key at the resolve step), we will get a separate newly created instance of NotificationService object from the container, for each query/resolve. services . AddScoped < IContactRepository , ContactRepository >(); line registers an IContactRepository -> ContactRepository scoped type mapping, as we used the AddScoped operation for registration. This means that if we later ask the container for an IContactRepository object (provide IContactRepository as key at the resolve step), we will get a NotificationService object, which will be the same instance for the same scope , and a different instance for different scopes. For a Web API based application one web request is handled within one scope. Consequently, we receive the same instance of a class turning to the container multiple times within the same web request, but different ones when the web requests are different. We can see additional registrations in the sample application's Startup.ConfigureServices method, which we will return to later. 2) RESOLVE step (resolving dependencies) \u00b6 The basics \u00b6 Let's sum up where we are now: we have our abstraction to implementation type mappings registered into the ASP.NET Core IoC container at application startup. Our mappings are the following: ILogger -> Logger as singleton INotificationService -> NotificationService as transient IContactRepository -> ContactRepository as scoped IEMailSender -> EMailSender as singleton From now on, whenever we need an instance of an implementation type for an abstraction, we can ask the container for it using the abstraction type as the key. How do we specifically do it in a .NET Core application? .NET Core provides an IServiceProvider reference to us, and we can use different forms of the GetService operation of this interface. E.g.: void SimpleResolve ( IServiceProvider sp ) { // Returns an instance of the Logger class, as we have // registered the Logger implementation type for our ILogger abstraction. var logger1 = sp . GetService ( typeof ( ILogger )); // Same as the previous example. The difference is that we have provided // the type as a generic parameter. This is a more convenient approach. // To use this we have to import the Microsoft.Extensions.DependencyInjection // namespace via the using statement. // Returns an instance of the Logger class, see explanation above. var logger2 = sp . GetService < ILogger >(); // GetService returns null if no type mapping is found for the specific type (ILogger) // GetRequiredService throws an exception instead. var logger3 = sp . GetRequiredService < ILogger >(); // ... } In the above example, code comments explain the behavior in detail. In each case, an abstraction type is to be provided for the GetService / GetRequiredService operation (either via the typeof operator, or via a generic parameter), and the operation returns with an instance of an implementation type based on the type mappings registered in the container. Object graph resolution, autowiring \u00b6 In the previous example, the container was able to instantiate the Logger class at the resolve step without any major 'headaches', since it has no additional dependencies: it has a single default constructor. Now consider the resolution of INotificationService : public void ObjectGraphResolve ( IServiceProvider sp ) { var notifService = sp . GetService < INotificationService >(); // ... } At the resolve step (GetService call), the container must create a NotificationService object. In doing so, it has to provide valid values for its constructor parameters, which actually means that has to resolve the class's direct and indirect dependencies, recursively: The NotificationService class has a three-parameter constructor (that is, it has three dependencies): NotificationService (ILogger logger, IEMailSender emailSender, IContactRepository contactRepository) . The GetService resolves constructor parameters one by one based on IoC container mapping registrations: ILogger logger: a Logger object is provided by the container, always the same instance (as ILogger->Logger mapping is registered as singleton) IEMailSender emailSender: an EMailSender object is provided by the container, a different instance in each case (as mapping is registered as transient) The EMailSender constructor has an ILogger logger parameter, that has to be resolved as well: a Logger object is provided by the container, always the same instance (as registered as singleton) IContactRepository contactRepository: a ContactRepository object is provided by the container, a different instance for different scopes (Web API e.g. for different Web API calls), as mapping is registered as scoped. Summing up: the GetService<INotificationService>() call above creates a fully parameterized NotificationService object with all of its direct and indirect dependencies, the call returns an object graph for us: As we have seen in this example, IoC containers/DI frameworks are capable of determining the dependency requirements of objects (by examining at their constructor parameters), and then creating entire object graphs based on upfront abstraction->implementation container type mappings. This process is called autowiring . Dependency resolution for ASP.NET Web API classes \u00b6 Besides making our solution IoC container based, we make a few further changes to our todo app. We eliminate our ToDoService class, and move its functionality in a slightly different form into an Asp.Net Core based ControllerBase derived class. This controller class will serve as our entry point and also as a root object, bringing our solution very close to a real life example (let it be a Web API, Web MVC app or a Web Razor Pages app). We could also have kept ToDoService in the middle of our call/dependency chain, but we try to keep things as simple as possible for our demonstration purposes. Furthermore, we also introduce an Entity Framework DbContext derived class called TodoContext to be able to demonstrate how it can be injected into repository classes in a typical application. Our new object graph will look like this: In the previous two chapters, we have assumed that a IServiceProvider object is available to call GetService . If we create a container ourselves, then this assumption is valid. However, only in the rarest cases do we create a container directly. In a typical ASP.NET Web API application, the container is created by the framework and is not directly accessible to us. Consequently, access to `IServiceProvider ', with the exception of a few startup and configuration points, is not available. The good news is that actually we don't need access to the container. The core concept of DI is that we perform dependency resolution only at the application entry point for the \"root object\". In case of Web API apps, the entry point is a call to an operation of a Controller class serving the specific API request. When a request is received, the framework determines and creates the Controller / ControllerBase child class based on the Url and rooting rules. If the controller class has dependencies (has constructor parameters), they are also resolved based on the container registration mappings, including indirect dependencies. The complete object graph is created, the root object is the controller class . Let's take a look at this in practice by refining our previous example with the addition of a TodoController class: [Route(\"api/[controller] \")] [ApiController] public class TodoController : ControllerBase { // Dependencies of the TodoController class private readonly TodoContext _context ; // this is a DbContext private readonly INotificationService _notificationService ; // Dependencies are received as constructor parameters public TodoController ( TodoContext context , INotificationService notificationService ) { _context = context ; _notificationService = notificationService ; // Fill wit some initial data if ( _context . TodoItems . Count () == 0 ) { _context . TodoItems . Add ( new TodoItem { Name = \"Item1\" }); _context . TodoItems . Add ( new TodoItem { Name = \"Item2\" , LinkedContactId = 2 }); _context . SaveChanges (); } } // API call handling function for sending an e-mail notification // Example for use: a http post request to this url (e.g. via using PostMan): // http://localhost:58922/api/todo/2/reminder // , which sends an e-mail notif to the e-mail address appointed of the // contact person referenced by the todo item. [HttpPost(\"{id}/reminder\")] public IActionResult ReminderMessageToLinkedContact ( long id ) { // Look up todo item var item = _context . TodoItems . Find ( id ); if ( item == null ) return NotFound (); // Rend reminder e-mail _notificationService . SendEmailReminder ( item . LinkedContactId , item . Name ); // Actually we don't create anything here, simply return an OK return Ok (); } // ... further operations } Requests under the http://<base_address>/api/todo url are routed to the TodoController class based on the routing rules. The mail sending request ( http://<base_address>/api/todo/<todo-id>/reminder ) is routed to its TodoController.ReminderMessageToLinkedContact operation. A TodoController object is instantiated by the framework, creating a new instance for each request. The TodoController class has two dependencies provided as constructor parameters. The first is a TodoContext object, which is a DbContext derived class. The other is an INotificationService , (which we already covered in our previous example). As we saw in the previous section, the DI framework can create these objects based on the container registered mappings (with all their indirect dependencies), and then pass them to the TodoController as constructor parameter, where they are stored in member variables. The entire object graph is created, with TodoController as the root object. This object graph is to serve the specific web API request. Note The resolution of TodoContext is only possible if it's pre-registered in the IoC container. We will discuss this in the next chapter. Entity Framework DbContext container registration and resolution \u00b6 In applications, especially in Asp.Net Core based ones, there are two ways to use DbContext: Each time it is needed, we create and dispose it with the help of a using block. This can result in the creation of multiple DbContext instances serving an incoming request (which is absolutely OK). We create one DbContext for a specific incoming request and share it for the classes involved in serving the request. In this case, we think of the DbContext instance as a unit of work serving the request. To accomplish this latter approach, ASP.NET Core provides a handy built-in DI based solution: when we configure our container with the type mappings at startup, we also register our DbContext class, which is then later automatically injected for our Controller and other (typically repository) dependencies. Let's see how our TodoContext ( DbContext derived) class is registered in our example. The place of the registration is the usual Startup.ConfigureServices : public void ConfigureServices ( IServiceCollection services ) { // ... services . AddDbContext < TodoContext >( opt => opt . UseInMemoryDatabase ( \"TodoList\" )); // ... } AddDbContext is an extension method defined by the framework for the IServiceCollection interface. This allows convenient registration of our DbContext class. We do not see into the implementation of AddDbContext , but actually it simply performs a scoped registration of our context type into the container: services . AddScoped < TodoContext , TodoContext >(); As shown in the example, TodoContext is not registered via an abstraction (no ITodoContext interface exists) , but via the TodoContext implementation type itself. DI frameworks / IoC containers support the key part of a mapping to be a specific type, e.g. the implementation type itself . Use this approach only when justified, e.g. when we don't need extensibility for the specific type, and introducing an abstraction (interface) would only complicate the solution. In an Asp.Net Core environment, we don't introduce an interface for our DbContext derived class: instead, we always register it with the type of its class to the IoC container (in our example TodoContext -> TodoContext mapping). DbContext itself can work with many persistent providers (e.g. MSSQL, Oracle, in-memory, etc.), so in many cases it does not make sense to put it behind further abstractions. In those cases when we need to abstract data access, we do not introduce an interface to access DbContext . Instead, we use the Repository design pattern, and we introduce interfaces for each repository implementations classes, and then register their mappings to the IoC container (e.g. ITodoRepository -> TodoRepository ). The repository classes either instantiate the DbContext objects themselves or the DbContext is injected as constructor parameter). Note This document does not intend to make a standpoint over the often disputed question, whether it makes or does not make sense introducing a repository layer in an Entity Framework based application. For illustration purposes, our TodoApi application uses a mixed solution in this sense: controller/service classes use DbContext directly to persist TodoItem objects, and use the Repository pattern to handle Contacts. Don't mix the two approaches in a real-life application. The example above also shows that you can also provide a lambda expression when registering DbContext (in case TodoContext ) using AddDbContext : opt => opt . UseInMemoryDatabase ( \"TodoList\" ) This lambda expression is called by the container later at the resolve step - that is, every time when a TodoContext is instantiated. An option object is provided as a parameter (in this example, the opt argument): this allows us to configurate the instance created by the container. In our example, calling the UseInMemoryDatabase operation creates an in-memory based database called \"TodoList\". Advanced dependency injection registration example \u00b6 Not compulsory material. Let's cover code parts of Startup.ConfigureServices we skipped previously. The registration of EMailSender looks quite tricky: services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); Let's take a look at the constructor of EMailSender to be able to better understand the situation: ```csharp public EMailSender(ILogger logger, string smtpAddress) { _logger = logger; _smtpAddress = smtpAddress; } `EMailSender` will need to be instantiated by the container when resolving `IEMailSender`, and the constructor parameters must be specified appropriately. The logger parameter is completely \"OK\", and the container can resolve it based on the ILogger-> Logger container mapping registration. However, there is no way to find out the value of the `smtpAddress` parameter. To solve this problem, ASP.NET Core proposes an \"options\" mechanism for the framework, which allows us to retrieve the value from some configuration. Covering the \"options\" topic would be a far-reaching thread for us, so for simplification we applied another approach. The `AddSingleton` (and other Add ... operations) have an overload in which we can specify a lambda expression. This lambda is called by the container later at the resolve step (that is, when we ask the container for an `IEMailSender` implementation) for each instance. With the help of this lambda we manually create the `EMailSender` object, so we have the chance to provide the necessary constructor parameters. In fact, the container is really \"helpful\" with us: it provides an `IServiceCollection` object as the lambda parameter for us (in this example it's called `sp`), and based on container registrations we can conveniently resolve types with the help of the already covered `GetRequiredService` and `GetService` calls. ## Further topics ### Dependency Injection/IoC containers in general The particularities of the DI container built in ASP.NET Core: * It provides basic services required by most applications (e.g., does not support property injection). * If you need more DI related functionality, you can use another IoC container Asp.Net Core can work with. * Several Dependecy Injection / IoC container class libraries exist that can be used with .NET Core, with .NET Framework, or with both. A few examples: AutoFac, DryIoc, LightInject, Castle Windsor, Ninject, StructureMap, SimpleInjector, MEF, ... * It's implemented in the __Microsoft.Extensions.DependencyInjection__ NuGet package. * For Asp.Net Core applications, it is automatically installed when the Asp.Net project is created. In fact, as we have seen, Asp.Net Core middleware heavily relies on it, it's a key pillar of runtime configuration and extensibility. * For other .NET Core applications (e.g. a simple .NET Core based console app), you need to add it manually by installing the Microsoft.Extensions.DependencyInjection NuGet package for the project. * Note: the NuGet package can be used with the (full) .NET Framework as well as it supports .NET Standard. ### The Service Locator antipattern Dependency injection is not the only way of using an IoC container. Another technique called __Service Locator__ exists. Dependency Injection is based on the mechanism of passing the dependencies of a class as constructor parameters. Service Locator uses another approach: the classes directly access the IoC container in their methods to resolve their dependencies. Keep in mind that this approach is considered an __anti-pattern__. The reason is simple: every time time a class needs a dependency, it has to turn to a container, so much of our code will depend on the container itself! In contrast, when dependency injection is used, dependency resolution is performed \"once\" at the application entry point for \"root objects\" (e.g. for the controller class in case of a Web API call), the rest of our code is completely independent of the container. Note that in our previous example, in our TodoController, NotificationService, EMailSender, Logger, and ContactRepository classes, we did not refer the container (neither via an IServiceProvider, nor by any other means). ### Asp.Net Core framework services Asp.Net Core has several built in services. E.g. it has support for Web API, and support for Razor Pages or MVC based web applications. These all rely on the DI services of Asp.Net Core. The `Startup.ConfigureServices` method of an Asp.Net Web API application has to have this piece of code: ```csharp services.AddMvc() .SetCompatibilityVersion(CompatibilityVersion.Version_2_1); AddMvc is a built in extension method for the IServiceProvider interface, which registers numerous (far more than 100!) service and configuration classes into the container required by the internals of the Web API middleware/pipeline. Starting from .NET Core 3.0 the situation is different: instead of calling AddMvc() we typically call AddControllers(), which is a more lightweight option, resulting in significantly less container registrations. Disposing service objects \u00b6 The container calls Dispose for the objects it creates if the object implements the IDisposable interface. Resources \u00b6 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection https://stackify.com/net-core-dependency-injection/amp https://medium.com/volosoft/asp-net-core-dependency-injection-best-practices-tips-tricks-c6e9c67f9d96","title":"Dependency Injection ASP.NET Core environment"},{"location":"lecture-notes/di/#dependency-injection-aspnet-core-environment","text":"Definition Dependency Injection (DI) is programming technique that makes a class independent of its dependencies. It's a key enabler for decomposing an application into loosely coupled components. More precisely: Dependency Injection is a mechanism to decouple the creation of dependency graphs for a class from its class definition . Of course, the above definition is very abstract, and based on the short definition it's hard to understand what problems DI is trying to solve, and how DI is trying to solve them. In the following chapters, we will use an example to put DI into context and to learn the basics of the DI related services build into ASP.NET Core. Goals of DI Facilitated extensibility and maintainability Improved unit testability Facilitated code reuse Sample application The sample C# code is available here: https://github.com/bmeviauac01/todoapi-di-sample","title":"Dependency Injection ASP.NET Core environment"},{"location":"lecture-notes/di/#example-phase-1-service-class-with-wired-in-dependencies","text":"In this example, based on code snippets we look at parts of a to-do list (TODO) application that sends to-do item related email notifications. Note: The code is minimalistic for succinctness. The \"entry point\" of our example is the SendReminderIfNeeded operation of the ToDoService class. // Class for managing todo items public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // It checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { NotificationService notificationService = new NotificationService ( smtpAddress ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } // ... } // Entity class, encapsulates information about a todo task public class TodoItem { // Database key public long Id { get ; set ; } // Name/description of the task public string Name { get ; set ; } // Indicates if the task has been completed public bool IsComplete { get ; set ; } // It's possible to assign a contact person to a task: -1 indicated no contact // person is assigned, otherwise the id of the contact person public int LinkedContactId { get ; set ; } = - 1 ; } In the code above ( ToDoService.SendReminderIfNeeded ) we see that the essential logic of sending an e-mail is to be found in the NotificationService class. Indeed, this class is at the center of our investigation. The following code snippet describes the code for the NotificationService class and its dependencies: // Class for sending notifications class NotificationService { // Dependencies of the class EMailSender _emailSender ; Logger _logger ; ContactRepository _contactRepository ; public NotificationService ( string smtpAddress ) { _logger = new Logger (); _emailSender = new EMailSender ( _logger , smtpAddress ); _contactRepository = new ContactRepository (); } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } // Class supporting loggin public class Logger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail notifications public class EMailSender { Logger _logger ; string _smtpAddress ; public EMailSender ( Logger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } A few general thoughts: The NotificationService class has several dependencies ( EMailSender , Logger , ContactRepository classes) and it implements its services based on these dependency classes. Dependency classes may have additional dependencies: EMailSender is a great example of this, it's dependent on the Logger class. Note: NotificationService , EMailSender , Logger , ContactRepository classes are considered service classes because they contain business logic, not just encapsulate data, such as TodoItem . As we could see the SendEmailReminder operation is actually served by an object graph, where NotificationService is the root object, it has three dependencies, and its dependencies have further dependencies. The following figure illustrates this object graph: Note One may ask why we considered NotificationService , and not ToDoService as the root object. Actually it just depends on our viewpoint: for simplicity we considered ToDoService as an entry point (a \"client\") for fulfilling a request, so that we have less classes to put under scrutiny. In a real life application we probably would consider ToDoService as part of the dependency graph as well. Let's review the key features of this solution: The class instantiates its dependencies itself Class depends on the specific type of its dependencies (and not on interfaces, \"abstractions\") This approach has a couple of significant and rather painful drawbacks: Rigidity, lack of extensibility . NotificationService (without modification) cannot work with other mailing, logging and contact repository implementations (but only with the the wired in EMailSender , Logger and ContactRepository classes). That is, e.g. we can't use it with any other logging component, or e.g. use it with a contact repository that operates via a different data source/storage mechanism. Lack of unit testability . The NotificationService (without modification) cannot be unit tested. This would require replacing the EMailSender , Logger and ContactRepository dependencies with variants that provide fixed/expected responses for a given input. Keep in mind that unit testing is about testing the behavior of a class independently from its dependencies. In our example, instead of using the database base ContactRepository, we would need a ContactRepository implementation that could serve requests very quickly from memory with values supporting the specific test cases. There is one more subtle inconvenience that is hard to notice at first sight. In our example we had to provide the smtpAddress parameter to the NotificationService constructor, so that it can forward it to its EMailSender dependency. However, smtpAddress is a parameter completely meaningless for NotificationService , it has nothing to do with this piece of information. Unfortunately, we are forced to pass smtpAddress thorough NotificationService , as NotificationService is the class instantiating the EMailSender object. We could eliminate this by somehow instantiating EMailSender independently of NotificationService . In the next steps, we redesign our solution so that we can eliminate most of the downsides of the current rigid approach.","title":"Example phase 1 - service class with wired in dependencies"},{"location":"lecture-notes/di/#example-phase-2-service-class-with-manual-dependency-injection","text":"Redesign our former solution the functional requirements are unchanged. The most important principles of transformation are the following: Dependencies will be based on abstractions/interfaces Classes will no longer instantiate their dependencies themselves Let's jump right into the code of the improved solution and analyze the differences: public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // Checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { var logger = new Logger (); var emailSender = new EMailSender ( logger , smtpAddress ); var contactRepository = new ContactRepository (); NotificationService notificationService = new NotificationService ( logger , emailSender , contactRepository ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } } // Class for sending notifications class NotificationService { // Dependencies of the class IEMailSender _emailSender ; ILogger _logger ; IContactRepository _contactRepository ; public NotificationService ( ILogger logger , IEMailSender emailSender , IContactRepository contactRepository ) { _logger = logger ; _emailSender = emailSender ; _contactRepository = contactRepository ; } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } #region Contracts (abstractions) // Interface for logging public interface ILogger { void LogInformation ( string text ); void LogError ( string text ); } // Interface for sending e-mail public interface IEMailSender { void SendMail ( string to , string subject , string message ); } // Interface for Contact entity persistence public interface IContactRepository { string GetContactEMailAddress ( int contactId ); } #endregion #region Implementations // Class for logging public class Logger : ILogger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail public class EMailSender : IEMailSender { ILogger _logger ; string _smtpAddress ; public EMailSender ( ILogger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository : IContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } #endregion We improved out previous solution in the following points: The NotificationService class no longer instantiates its dependencies itself, but receives them in constructor parameters. Interfaces (abstractions) have been introduced to manage dependencies The NotificationService class gets its dependencies in the form of interfaces. When a class receives its dependencies externally (e.g. via constructor parameters), it is called DEPENDENCY INJECTION (DI). In our case, the classes get their class dependencies in constructor parameters: this specific form of DI is called CONSTRUCTOR INJECTION . This is the most common - and most recommended - way to inject dependency. (Alternatively, for example, we could use property injection, which is based on a public property setter to set a specific dependency of a class). In our current solution, NotificationService dependencies are instantiated by the (direct) USER of the class (which is the ToDoService class). Primarily this is the reason why we are still facing with a few problems: The user of NotificationService objects, which is the ToDoService class, is still dependent on the implementation types (since it has to instantiate the Logger , EMailSender and ContactRepository classes). If we use the Logger , EMailSender and ContactRepository classes at multiple places in your application, we must instantiate them explicitly. In other words: at each and every place where have to create an ILogger , IEMailSender or IContactRepository implementation class, we have to make a decision which implementation to choose. This is essentially a special case of code duplication, the decision should appear only once in our code. Our goal, in contrast, would be to determine at a single central location what type implementation to use for an abstraction (interface type) everywhere in the application (e.g. for ILogger create an Logger instance everywhere, for IMailSender create an EMailSender everywhere). This would allow us to easily review our abstraction-to-implementation mappings at one place. Moreover, if we want to change one of the mappings (e.g. using AdvancedLogger instead of Logger for ILogger ) we could achieve that by making a single change at a central location.","title":"Example phase 2 - service class with manual dependency injection"},{"location":"lecture-notes/di/#example-phase-3-dependency-injection-based-on-net-core-dependency-injection","text":"We need some extra help from our framework to solve the two problems we concluded the previous chapter with: an Inversion of Control (IoC) container. Dependency Injection container is a widely used alternative name for the same tool/technique. In an IoC container we can store abstraction type -> implementation type mappings, such as ILogger->Logger, IMailSender->EMailSender, etc. This is called the REGISTER step. And then based on these mappings create an implementation type for a specific abstraction type (e.g. Logger for an ILogger ). This is called the RESOLVE step. In more detail: REGISTER : Register dependency mappings (e.g. ILogger -> Logger , IMailSender -> EMailSender ) into an IoC container, once, at a centralized location, at application startup. This is the REGISTER step of the DI process. Note: This solves \"problem 2\" pointed out at the end of the previous chapter: the mappings are centralized, and not scattered all over the application code base. RESOLVE : When we need an implementation object at runtime in our application, we ask the container for an implementation by specifying the abstraction (interface) type (e.g., by providing ILogger as a key, the container returns an object of class Logger ). The resolve step is typically done at the \" entry point \" of the application (e.g. in case of WebApi on the receival of web requests, we will look into this later). The resolve step is performed only for the ROOT OBJECT (e.g. for the appropriate Controller class in case of WebApi). The container creates and returns a root object and all its dependencies and all its indirect dependencies: an entire object graph is generated. This process is called AUTOWIRING . Note: In case of Web API calls, the Resolve step is executed by the Asp.Net framework and is mostly hidden from the developer: all we see is that our controller class is automatically instantiated and all constructor parameters are automatically populated (with the help of the IoC container based on the mappings of the REGISTER step). Fortunately, .NET Core has a built in IoC container based dependency injection service. Now we elucidate and illustrate the complete mechanism (register and resolve steps) using our enhanced e-mail notification solution as an example.","title":"Example phase 3 - dependency injection based on .NET Core Dependency Injection"},{"location":"lecture-notes/di/#1-register-step-registering-dependencies","text":"In an Asp.Net Core environment, dependencies are registered by the ConfigureServices (IServiceCollection services) function of our Startup class, namely by the AddSingleton , AddTransient and AddScoped operations of IServiceCollection . First, let's focus on the most exciting parts of ConfigureServices : public class Startup { public void ConfigureServices ( IServiceCollection services ) { // ... services . AddSingleton < ILogger , Logger >(); services . AddTransient < INotificationService , NotificationService >(); services . AddScoped < IContactRepository , ContactRepository >(); services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); // ... } Startup.ConfigureServices is called by the framework at application startup. We receive an IServiceCollection services object as parameter: this represents the IoC container created and initialized by the framework. We can register our own dependency mappings into this container. The services . AddSingleton < ILogger , Logger >(); line registers an ILogger -> Logger type mapping, and the Logger is registered as a singleton, as we used the AddSingleton operation for registration. This means that if we later ask the container for an ILogger object (provide ILogger as key at the resolve step), we will get a Logger object from the container, and always the same instance . The services . AddTransient < INotificationService , NotificationService >(); line registers an INotificationService -> NotificationService transient type mapping, as we used the AddTransient operation for registration. This means that if we later ask the container for an INotificationService object (provide INotificationService as key at the resolve step), we will get a separate newly created instance of NotificationService object from the container, for each query/resolve. services . AddScoped < IContactRepository , ContactRepository >(); line registers an IContactRepository -> ContactRepository scoped type mapping, as we used the AddScoped operation for registration. This means that if we later ask the container for an IContactRepository object (provide IContactRepository as key at the resolve step), we will get a NotificationService object, which will be the same instance for the same scope , and a different instance for different scopes. For a Web API based application one web request is handled within one scope. Consequently, we receive the same instance of a class turning to the container multiple times within the same web request, but different ones when the web requests are different. We can see additional registrations in the sample application's Startup.ConfigureServices method, which we will return to later.","title":"1) REGISTER step (registering dependencies)"},{"location":"lecture-notes/di/#2-resolve-step-resolving-dependencies","text":"","title":"2) RESOLVE step (resolving dependencies)"},{"location":"lecture-notes/di/#the-basics","text":"Let's sum up where we are now: we have our abstraction to implementation type mappings registered into the ASP.NET Core IoC container at application startup. Our mappings are the following: ILogger -> Logger as singleton INotificationService -> NotificationService as transient IContactRepository -> ContactRepository as scoped IEMailSender -> EMailSender as singleton From now on, whenever we need an instance of an implementation type for an abstraction, we can ask the container for it using the abstraction type as the key. How do we specifically do it in a .NET Core application? .NET Core provides an IServiceProvider reference to us, and we can use different forms of the GetService operation of this interface. E.g.: void SimpleResolve ( IServiceProvider sp ) { // Returns an instance of the Logger class, as we have // registered the Logger implementation type for our ILogger abstraction. var logger1 = sp . GetService ( typeof ( ILogger )); // Same as the previous example. The difference is that we have provided // the type as a generic parameter. This is a more convenient approach. // To use this we have to import the Microsoft.Extensions.DependencyInjection // namespace via the using statement. // Returns an instance of the Logger class, see explanation above. var logger2 = sp . GetService < ILogger >(); // GetService returns null if no type mapping is found for the specific type (ILogger) // GetRequiredService throws an exception instead. var logger3 = sp . GetRequiredService < ILogger >(); // ... } In the above example, code comments explain the behavior in detail. In each case, an abstraction type is to be provided for the GetService / GetRequiredService operation (either via the typeof operator, or via a generic parameter), and the operation returns with an instance of an implementation type based on the type mappings registered in the container.","title":"The basics"},{"location":"lecture-notes/di/#object-graph-resolution-autowiring","text":"In the previous example, the container was able to instantiate the Logger class at the resolve step without any major 'headaches', since it has no additional dependencies: it has a single default constructor. Now consider the resolution of INotificationService : public void ObjectGraphResolve ( IServiceProvider sp ) { var notifService = sp . GetService < INotificationService >(); // ... } At the resolve step (GetService call), the container must create a NotificationService object. In doing so, it has to provide valid values for its constructor parameters, which actually means that has to resolve the class's direct and indirect dependencies, recursively: The NotificationService class has a three-parameter constructor (that is, it has three dependencies): NotificationService (ILogger logger, IEMailSender emailSender, IContactRepository contactRepository) . The GetService resolves constructor parameters one by one based on IoC container mapping registrations: ILogger logger: a Logger object is provided by the container, always the same instance (as ILogger->Logger mapping is registered as singleton) IEMailSender emailSender: an EMailSender object is provided by the container, a different instance in each case (as mapping is registered as transient) The EMailSender constructor has an ILogger logger parameter, that has to be resolved as well: a Logger object is provided by the container, always the same instance (as registered as singleton) IContactRepository contactRepository: a ContactRepository object is provided by the container, a different instance for different scopes (Web API e.g. for different Web API calls), as mapping is registered as scoped. Summing up: the GetService<INotificationService>() call above creates a fully parameterized NotificationService object with all of its direct and indirect dependencies, the call returns an object graph for us: As we have seen in this example, IoC containers/DI frameworks are capable of determining the dependency requirements of objects (by examining at their constructor parameters), and then creating entire object graphs based on upfront abstraction->implementation container type mappings. This process is called autowiring .","title":"Object graph resolution, autowiring"},{"location":"lecture-notes/di/#dependency-resolution-for-aspnet-web-api-classes","text":"Besides making our solution IoC container based, we make a few further changes to our todo app. We eliminate our ToDoService class, and move its functionality in a slightly different form into an Asp.Net Core based ControllerBase derived class. This controller class will serve as our entry point and also as a root object, bringing our solution very close to a real life example (let it be a Web API, Web MVC app or a Web Razor Pages app). We could also have kept ToDoService in the middle of our call/dependency chain, but we try to keep things as simple as possible for our demonstration purposes. Furthermore, we also introduce an Entity Framework DbContext derived class called TodoContext to be able to demonstrate how it can be injected into repository classes in a typical application. Our new object graph will look like this: In the previous two chapters, we have assumed that a IServiceProvider object is available to call GetService . If we create a container ourselves, then this assumption is valid. However, only in the rarest cases do we create a container directly. In a typical ASP.NET Web API application, the container is created by the framework and is not directly accessible to us. Consequently, access to `IServiceProvider ', with the exception of a few startup and configuration points, is not available. The good news is that actually we don't need access to the container. The core concept of DI is that we perform dependency resolution only at the application entry point for the \"root object\". In case of Web API apps, the entry point is a call to an operation of a Controller class serving the specific API request. When a request is received, the framework determines and creates the Controller / ControllerBase child class based on the Url and rooting rules. If the controller class has dependencies (has constructor parameters), they are also resolved based on the container registration mappings, including indirect dependencies. The complete object graph is created, the root object is the controller class . Let's take a look at this in practice by refining our previous example with the addition of a TodoController class: [Route(\"api/[controller] \")] [ApiController] public class TodoController : ControllerBase { // Dependencies of the TodoController class private readonly TodoContext _context ; // this is a DbContext private readonly INotificationService _notificationService ; // Dependencies are received as constructor parameters public TodoController ( TodoContext context , INotificationService notificationService ) { _context = context ; _notificationService = notificationService ; // Fill wit some initial data if ( _context . TodoItems . Count () == 0 ) { _context . TodoItems . Add ( new TodoItem { Name = \"Item1\" }); _context . TodoItems . Add ( new TodoItem { Name = \"Item2\" , LinkedContactId = 2 }); _context . SaveChanges (); } } // API call handling function for sending an e-mail notification // Example for use: a http post request to this url (e.g. via using PostMan): // http://localhost:58922/api/todo/2/reminder // , which sends an e-mail notif to the e-mail address appointed of the // contact person referenced by the todo item. [HttpPost(\"{id}/reminder\")] public IActionResult ReminderMessageToLinkedContact ( long id ) { // Look up todo item var item = _context . TodoItems . Find ( id ); if ( item == null ) return NotFound (); // Rend reminder e-mail _notificationService . SendEmailReminder ( item . LinkedContactId , item . Name ); // Actually we don't create anything here, simply return an OK return Ok (); } // ... further operations } Requests under the http://<base_address>/api/todo url are routed to the TodoController class based on the routing rules. The mail sending request ( http://<base_address>/api/todo/<todo-id>/reminder ) is routed to its TodoController.ReminderMessageToLinkedContact operation. A TodoController object is instantiated by the framework, creating a new instance for each request. The TodoController class has two dependencies provided as constructor parameters. The first is a TodoContext object, which is a DbContext derived class. The other is an INotificationService , (which we already covered in our previous example). As we saw in the previous section, the DI framework can create these objects based on the container registered mappings (with all their indirect dependencies), and then pass them to the TodoController as constructor parameter, where they are stored in member variables. The entire object graph is created, with TodoController as the root object. This object graph is to serve the specific web API request. Note The resolution of TodoContext is only possible if it's pre-registered in the IoC container. We will discuss this in the next chapter.","title":"Dependency resolution  for ASP.NET Web API classes"},{"location":"lecture-notes/di/#entity-framework-dbcontext-container-registration-and-resolution","text":"In applications, especially in Asp.Net Core based ones, there are two ways to use DbContext: Each time it is needed, we create and dispose it with the help of a using block. This can result in the creation of multiple DbContext instances serving an incoming request (which is absolutely OK). We create one DbContext for a specific incoming request and share it for the classes involved in serving the request. In this case, we think of the DbContext instance as a unit of work serving the request. To accomplish this latter approach, ASP.NET Core provides a handy built-in DI based solution: when we configure our container with the type mappings at startup, we also register our DbContext class, which is then later automatically injected for our Controller and other (typically repository) dependencies. Let's see how our TodoContext ( DbContext derived) class is registered in our example. The place of the registration is the usual Startup.ConfigureServices : public void ConfigureServices ( IServiceCollection services ) { // ... services . AddDbContext < TodoContext >( opt => opt . UseInMemoryDatabase ( \"TodoList\" )); // ... } AddDbContext is an extension method defined by the framework for the IServiceCollection interface. This allows convenient registration of our DbContext class. We do not see into the implementation of AddDbContext , but actually it simply performs a scoped registration of our context type into the container: services . AddScoped < TodoContext , TodoContext >(); As shown in the example, TodoContext is not registered via an abstraction (no ITodoContext interface exists) , but via the TodoContext implementation type itself. DI frameworks / IoC containers support the key part of a mapping to be a specific type, e.g. the implementation type itself . Use this approach only when justified, e.g. when we don't need extensibility for the specific type, and introducing an abstraction (interface) would only complicate the solution. In an Asp.Net Core environment, we don't introduce an interface for our DbContext derived class: instead, we always register it with the type of its class to the IoC container (in our example TodoContext -> TodoContext mapping). DbContext itself can work with many persistent providers (e.g. MSSQL, Oracle, in-memory, etc.), so in many cases it does not make sense to put it behind further abstractions. In those cases when we need to abstract data access, we do not introduce an interface to access DbContext . Instead, we use the Repository design pattern, and we introduce interfaces for each repository implementations classes, and then register their mappings to the IoC container (e.g. ITodoRepository -> TodoRepository ). The repository classes either instantiate the DbContext objects themselves or the DbContext is injected as constructor parameter). Note This document does not intend to make a standpoint over the often disputed question, whether it makes or does not make sense introducing a repository layer in an Entity Framework based application. For illustration purposes, our TodoApi application uses a mixed solution in this sense: controller/service classes use DbContext directly to persist TodoItem objects, and use the Repository pattern to handle Contacts. Don't mix the two approaches in a real-life application. The example above also shows that you can also provide a lambda expression when registering DbContext (in case TodoContext ) using AddDbContext : opt => opt . UseInMemoryDatabase ( \"TodoList\" ) This lambda expression is called by the container later at the resolve step - that is, every time when a TodoContext is instantiated. An option object is provided as a parameter (in this example, the opt argument): this allows us to configurate the instance created by the container. In our example, calling the UseInMemoryDatabase operation creates an in-memory based database called \"TodoList\".","title":"Entity Framework DbContext container registration and resolution"},{"location":"lecture-notes/di/#advanced-dependency-injection-registration-example","text":"Not compulsory material. Let's cover code parts of Startup.ConfigureServices we skipped previously. The registration of EMailSender looks quite tricky: services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); Let's take a look at the constructor of EMailSender to be able to better understand the situation: ```csharp public EMailSender(ILogger logger, string smtpAddress) { _logger = logger; _smtpAddress = smtpAddress; } `EMailSender` will need to be instantiated by the container when resolving `IEMailSender`, and the constructor parameters must be specified appropriately. The logger parameter is completely \"OK\", and the container can resolve it based on the ILogger-> Logger container mapping registration. However, there is no way to find out the value of the `smtpAddress` parameter. To solve this problem, ASP.NET Core proposes an \"options\" mechanism for the framework, which allows us to retrieve the value from some configuration. Covering the \"options\" topic would be a far-reaching thread for us, so for simplification we applied another approach. The `AddSingleton` (and other Add ... operations) have an overload in which we can specify a lambda expression. This lambda is called by the container later at the resolve step (that is, when we ask the container for an `IEMailSender` implementation) for each instance. With the help of this lambda we manually create the `EMailSender` object, so we have the chance to provide the necessary constructor parameters. In fact, the container is really \"helpful\" with us: it provides an `IServiceCollection` object as the lambda parameter for us (in this example it's called `sp`), and based on container registrations we can conveniently resolve types with the help of the already covered `GetRequiredService` and `GetService` calls. ## Further topics ### Dependency Injection/IoC containers in general The particularities of the DI container built in ASP.NET Core: * It provides basic services required by most applications (e.g., does not support property injection). * If you need more DI related functionality, you can use another IoC container Asp.Net Core can work with. * Several Dependecy Injection / IoC container class libraries exist that can be used with .NET Core, with .NET Framework, or with both. A few examples: AutoFac, DryIoc, LightInject, Castle Windsor, Ninject, StructureMap, SimpleInjector, MEF, ... * It's implemented in the __Microsoft.Extensions.DependencyInjection__ NuGet package. * For Asp.Net Core applications, it is automatically installed when the Asp.Net project is created. In fact, as we have seen, Asp.Net Core middleware heavily relies on it, it's a key pillar of runtime configuration and extensibility. * For other .NET Core applications (e.g. a simple .NET Core based console app), you need to add it manually by installing the Microsoft.Extensions.DependencyInjection NuGet package for the project. * Note: the NuGet package can be used with the (full) .NET Framework as well as it supports .NET Standard. ### The Service Locator antipattern Dependency injection is not the only way of using an IoC container. Another technique called __Service Locator__ exists. Dependency Injection is based on the mechanism of passing the dependencies of a class as constructor parameters. Service Locator uses another approach: the classes directly access the IoC container in their methods to resolve their dependencies. Keep in mind that this approach is considered an __anti-pattern__. The reason is simple: every time time a class needs a dependency, it has to turn to a container, so much of our code will depend on the container itself! In contrast, when dependency injection is used, dependency resolution is performed \"once\" at the application entry point for \"root objects\" (e.g. for the controller class in case of a Web API call), the rest of our code is completely independent of the container. Note that in our previous example, in our TodoController, NotificationService, EMailSender, Logger, and ContactRepository classes, we did not refer the container (neither via an IServiceProvider, nor by any other means). ### Asp.Net Core framework services Asp.Net Core has several built in services. E.g. it has support for Web API, and support for Razor Pages or MVC based web applications. These all rely on the DI services of Asp.Net Core. The `Startup.ConfigureServices` method of an Asp.Net Web API application has to have this piece of code: ```csharp services.AddMvc() .SetCompatibilityVersion(CompatibilityVersion.Version_2_1); AddMvc is a built in extension method for the IServiceProvider interface, which registers numerous (far more than 100!) service and configuration classes into the container required by the internals of the Web API middleware/pipeline. Starting from .NET Core 3.0 the situation is different: instead of calling AddMvc() we typically call AddControllers(), which is a more lightweight option, resulting in significantly less container registrations.","title":"Advanced dependency injection registration example"},{"location":"lecture-notes/di/#disposing-service-objects","text":"The container calls Dispose for the objects it creates if the object implements the IDisposable interface.","title":"Disposing service objects"},{"location":"lecture-notes/di/#resources","text":"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection https://stackify.com/net-core-dependency-injection/amp https://medium.com/volosoft/asp-net-core-dependency-injection-best-practices-tips-tricks-c6e9c67f9d96","title":"Resources"},{"location":"lecture-notes/ef/","text":"Defining relationships in Entity Framework \u00b6 We store both entities and the relationships that connect them in relational databases. This allows us to query related entities through joining tables, expressed with the join SQL command. Entity Framework, which is an ORM framework, provides us with built-in support for conveniently managing these relationships. Defining relationships \u00b6 Convention-based mapping Entity Framework has conventions that enable mapping relationships automatically without explicit configuration. We will not rely on this feature here; instead will define the relationships explicitly. Let us look at our example classes and the relationship among them: public class Product { public int ID ; public string Name ; public int Price ; public int VATID ; public VAT VAT { get ; set ; } } public class VAT { public int ID ; public int Percentage ; public ICollection < Product > Product { get ; set ; } } We can set up the configuration in the OnModelCreating function inherited from the DBContext base class. We can use the following functions for configuring an entity: modelBuilder . Entity < Example > . HasOne ()/. HasMany () . WithOne ()/. WithMany () In the example above, we can see a one-to-many connections, which can be described as follows: modelBuilder . Entity < Product >() . HasOne ( d => d . VAT ) . WithMany ( p => p . Product ) . HasForeignKey ( d => d . VatId ); The connection between the two entities is provided by the foreign key of the Product table pointing to VAT table (which foreign key, naturally, also appears in the database as a column). In the C# code, a VAT object reference appears in class Product . Similarly, VAT objects have a list of connected Product instances. These C# properties are called navigation properties . Explicit joining \u00b6 The DBContext offers the tables as DBSets , on which we can perform LINQ operations. One such operation is the join function. Two DBSets can be joined via the appropriate foreign key. Similar to it's SQL equivalent, the following LINQ expression declaratively describes what we want to get. var query = from p in dbContext . Product join v in dbContext . Vat on p . VatId equals v . Id where p . Name . Contains ( \"test\" ) select v . Percentage ; // Displays the generated SQL query Console . WriteLine ( query . ToQueryString ()); The generated SQL query will look similar to this: SELECT [ v ].[ Percentage ] FROM [ Product ] AS [ p ] INNER JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' We rarely need to use explicit joins. As a matter of fact, we should avoid using them when navigation properties are available. Navigation property \u00b6 Since we have configured the relationship between the Product and VAT EF entities in our DbContext , we can use the VAT property in the Product class: this is the navigation property . The joining \"behind\" the navigation property is handled automatically by EF without us having to define it in the query. This simplifies our previous query to: var query = from p in dbContext . Product where p . Name . Contains ( \"test\" ) select p . VAT . Percentage ; // Displays the generated SQL query Console . WriteLine ( query . ToQueryString ()); Below we can see the generated SQL query, which differs from the previous one only in the type of join, but otherwise, we get to the same solution. SELECT [ v ].[ Percentage ] FROM [ Product ] AS [ p ] LEFT JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' Prefer navigation properties In EF, we should always strive to use the navigation properties when possible. We should avoid performing explicit joins. Include \u00b6 In the previous example, only one scalar result was queried. But what happens to the navigation properties when we query an entire entity? For example: var prod = dbContext . Product . Where ( p => p . Name . Contains ( \"test\" )). First (); Console . WriteLine ( prod . Name ); // this works, it will print the name Console . WriteLine ( prod . VAT . Percentage ); // accessing the referenced entity via the navigation property In this example, we would get a runtime error in the last line. Why is that? Despite the navigation property being configured, EF does not load referenced entities by default. We can work with them in queries (as we wrote p.VAT.Percentage in a previous query), but if we query a Product entity, it does not include the referenced VAT entity. The referenced record(s) could be fetched. But it is up to the developer to decide if they really need them. Just consider, if all the referenced entities were fetched automatically (even transitively), the database would have to look up hundreds or thousands or records to get a single entity and all of it's referenced data via navigation properties. This is unnecessary in most cases. If we really need the referenced entities, the we need to specify this in the code using Include as follows: var query = from p in dbContext . Products . Include ( p => p . VAT ) where p . Name . Contains ( \"test\" ) select p ; // or an anternative syntax for the same: // var query = products // .Include(p => p.VAT) // .Where(p => p.Name.Contains(\"test\")); Console . WriteLine ( query . ToQueryString ()); If we look at the generated SQL statement, it shows both the appropriate join and the required data appearing within the select statement. SELECT [ p ].[ Id ], [ p ].[ CategoryId ], [ p ].[ Description ], [ p ].[ Name ], [ p ].[ Price ], [ p ].[ Stock ], [ p ].[ VatId ], [ v ].[ ID ], [ v ].[ Percentage ] FROM [ Product ] AS [ p ] LEFT JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' Automatic lazy loading of referenced entities In Entity Framework, it is possible to turn on lazy loading , which causes entities to be loaded through navigation properties on demand. The loading is performed in a lazy way (that is, only when needed) without an explicit Include . While this solution is convenient for the developer, it comes at a price: loading data when needed (when the code reaches a statement referencing the property) will typically result in several separate database queries. In the Include solution, you can see above that a single query loads both the Product and VAT data. If we used lazy loading , there would be a query for the Product data and another one for the referenced VAT properties at a later time. Thus, lazy loading is usually worse in terms of performance.","title":"Defining relationships in Entity Framework"},{"location":"lecture-notes/ef/#defining-relationships-in-entity-framework","text":"We store both entities and the relationships that connect them in relational databases. This allows us to query related entities through joining tables, expressed with the join SQL command. Entity Framework, which is an ORM framework, provides us with built-in support for conveniently managing these relationships.","title":"Defining relationships in Entity Framework"},{"location":"lecture-notes/ef/#defining-relationships","text":"Convention-based mapping Entity Framework has conventions that enable mapping relationships automatically without explicit configuration. We will not rely on this feature here; instead will define the relationships explicitly. Let us look at our example classes and the relationship among them: public class Product { public int ID ; public string Name ; public int Price ; public int VATID ; public VAT VAT { get ; set ; } } public class VAT { public int ID ; public int Percentage ; public ICollection < Product > Product { get ; set ; } } We can set up the configuration in the OnModelCreating function inherited from the DBContext base class. We can use the following functions for configuring an entity: modelBuilder . Entity < Example > . HasOne ()/. HasMany () . WithOne ()/. WithMany () In the example above, we can see a one-to-many connections, which can be described as follows: modelBuilder . Entity < Product >() . HasOne ( d => d . VAT ) . WithMany ( p => p . Product ) . HasForeignKey ( d => d . VatId ); The connection between the two entities is provided by the foreign key of the Product table pointing to VAT table (which foreign key, naturally, also appears in the database as a column). In the C# code, a VAT object reference appears in class Product . Similarly, VAT objects have a list of connected Product instances. These C# properties are called navigation properties .","title":"Defining relationships"},{"location":"lecture-notes/ef/#explicit-joining","text":"The DBContext offers the tables as DBSets , on which we can perform LINQ operations. One such operation is the join function. Two DBSets can be joined via the appropriate foreign key. Similar to it's SQL equivalent, the following LINQ expression declaratively describes what we want to get. var query = from p in dbContext . Product join v in dbContext . Vat on p . VatId equals v . Id where p . Name . Contains ( \"test\" ) select v . Percentage ; // Displays the generated SQL query Console . WriteLine ( query . ToQueryString ()); The generated SQL query will look similar to this: SELECT [ v ].[ Percentage ] FROM [ Product ] AS [ p ] INNER JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' We rarely need to use explicit joins. As a matter of fact, we should avoid using them when navigation properties are available.","title":"Explicit joining"},{"location":"lecture-notes/ef/#navigation-property","text":"Since we have configured the relationship between the Product and VAT EF entities in our DbContext , we can use the VAT property in the Product class: this is the navigation property . The joining \"behind\" the navigation property is handled automatically by EF without us having to define it in the query. This simplifies our previous query to: var query = from p in dbContext . Product where p . Name . Contains ( \"test\" ) select p . VAT . Percentage ; // Displays the generated SQL query Console . WriteLine ( query . ToQueryString ()); Below we can see the generated SQL query, which differs from the previous one only in the type of join, but otherwise, we get to the same solution. SELECT [ v ].[ Percentage ] FROM [ Product ] AS [ p ] LEFT JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' Prefer navigation properties In EF, we should always strive to use the navigation properties when possible. We should avoid performing explicit joins.","title":"Navigation property"},{"location":"lecture-notes/ef/#include","text":"In the previous example, only one scalar result was queried. But what happens to the navigation properties when we query an entire entity? For example: var prod = dbContext . Product . Where ( p => p . Name . Contains ( \"test\" )). First (); Console . WriteLine ( prod . Name ); // this works, it will print the name Console . WriteLine ( prod . VAT . Percentage ); // accessing the referenced entity via the navigation property In this example, we would get a runtime error in the last line. Why is that? Despite the navigation property being configured, EF does not load referenced entities by default. We can work with them in queries (as we wrote p.VAT.Percentage in a previous query), but if we query a Product entity, it does not include the referenced VAT entity. The referenced record(s) could be fetched. But it is up to the developer to decide if they really need them. Just consider, if all the referenced entities were fetched automatically (even transitively), the database would have to look up hundreds or thousands or records to get a single entity and all of it's referenced data via navigation properties. This is unnecessary in most cases. If we really need the referenced entities, the we need to specify this in the code using Include as follows: var query = from p in dbContext . Products . Include ( p => p . VAT ) where p . Name . Contains ( \"test\" ) select p ; // or an anternative syntax for the same: // var query = products // .Include(p => p.VAT) // .Where(p => p.Name.Contains(\"test\")); Console . WriteLine ( query . ToQueryString ()); If we look at the generated SQL statement, it shows both the appropriate join and the required data appearing within the select statement. SELECT [ p ].[ Id ], [ p ].[ CategoryId ], [ p ].[ Description ], [ p ].[ Name ], [ p ].[ Price ], [ p ].[ Stock ], [ p ].[ VatId ], [ v ].[ ID ], [ v ].[ Percentage ] FROM [ Product ] AS [ p ] LEFT JOIN [ VAT ] AS [ v ] ON [ p ].[ VatId ] = [ v ].[ ID ] WHERE [ p ].[ Name ] LIKE N '%test%' Automatic lazy loading of referenced entities In Entity Framework, it is possible to turn on lazy loading , which causes entities to be loaded through navigation properties on demand. The loading is performed in a lazy way (that is, only when needed) without an explicit Include . While this solution is convenient for the developer, it comes at a price: loading data when needed (when the code reaches a statement referencing the property) will typically result in several separate database queries. In the Include solution, you can see above that a single query loads both the Product and VAT data. If we used lazy loading , there would be a query for the Product data and another one for the referenced VAT properties at a later time. Thus, lazy loading is usually worse in terms of performance.","title":"Include"},{"location":"lecture-notes/linq/","text":"LINQ: Language Integrated Query \u00b6 Consider the following classes and lists of such instances. class Product { public int ID ; public string Name ; public int Price ; public int VATID ; } class VAT { public int ID ; public int Percentage ; } List < Product > products = ... List < VAT > vat = ... System.Linq To access Linq functionality we need the System.Linq namespace: using System.Linq ; LINQ operations \u00b6 The examples below, when available, show both syntaxes. Filtering \u00b6 products . Where ( p => p . Price < 1000 ) from p in products where p . Price < 1000 Projection \u00b6 products . Select ( p => p . Name ) from p in products select p . Name Join \u00b6 from p in products join v in vat on p . VATID equals v . Id select p . Price * v . Percentage products . Join ( vat , p => p . VATID , v => v . Id , ( p , v ) => p . Price * v . Percentage ) Sorting \u00b6 products . OrderBy [ Descending ]( p => p . Name ) . ThenBy [ Descending ]( p => p . Price ) from p in products orderby p . Name , p . Price [ descending ] Set operations \u00b6 products . Select ( p => p . Name ). Distinct () products . Where ( p => p . Price < 1000 ) . Union ( products . Where ( p => p . Price > 100000 ) ) // similarly Except, Intersect Aggregation \u00b6 products . Count () products . Select ( p => p . Price ). Average () // similarly Sum, Min, Max First, last \u00b6 products . First () products . Last () products . Where ( p => p . Id == 12 ). FirstOrDefault () products . Where ( p => p . Id == 12 ). SingleOrDefault () Paging \u00b6 products . Take ( 10 ) products . Skip ( 10 ). Take ( 10 ) Contains (exists) \u00b6 products . Any ( p => p . Price == 1234 ) products . Where ( p => p . Price == 1234 ). Any () Grouping \u00b6 from p in products group p by p . VATID products . GroupBy ( p => p . VATID ) Advanced projections \u00b6 During projection we can transform the results into various formats. Whole entity \u00b6 from p in products ... select p The result set is of type IQueryable<Product> , so we get Product instances. Specified field \u00b6 from p in products ... select p . Name The result set is of type IQueryable<string> , so we only get the names. Named types \u00b6 from p in products ... select new MyType ( p . Name , p . Price ) The result set is of type IQueryable<MyType> , when MyType is a class we have to define and has a matching constructor. Anonym types \u00b6 from p in products where p . Price > 1000 select new { ID = p . ID , Name = p . Name }; Anonym types can be instantiated using the syntax new { } . The compiler will effectively create a class definition with the properties we specified. This is generally used when we only need two or three properties, and we have no need for the entire entity. A similar use case for anonym types is when we calculate a property inside the query, such as the name of the product and the full price: from p in products join v in vat on p . VATID equals v . Id select new { Name = p . Name , FullPrice = p . Price * v . Percentage } LINQ expressions and IEnumerable/IQueryable \u00b6 Depending on the data source we are using Linq on the result of a query, such as products.Where(p => p.Price < 1000) yields a variable of type IEnumerable<T> or IQueryable<T> . Neither of these contain the result sets; they are only descriptors, that is, the operation has not yet been evaluated yet . This is called deferred execution , as the execution will only happen when the result is effectively used: when the result set is iterated (e.g. foreach ), when a specific item is accessed (see later, e.g. .First() ), when we as for a list instead ( .ToList() ). This operation is useful, because this allows us to chain LINQ operations after each other, such as: var l = products . Where ( p => p . Price < 1000 ) . Where ( p => p . Name . Contains ( 's' )) . OrderBy ( p => p . Name ) . Select ( p => p . Name ) ... // variable l does not contain the result foreach ( var x in l ) // this is when the execution will happen { ... } Force evaluation If we want to force the execution at any given moment, we usually use .ToList() . But this has to be considered first and only used when necessary. More information and further examples \u00b6 Lambda expressions: https://www.tutorialsteacher.com/linq/linq-lambda-expression Linq: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/","title":"LINQ: Language Integrated Query"},{"location":"lecture-notes/linq/#linq-language-integrated-query","text":"Consider the following classes and lists of such instances. class Product { public int ID ; public string Name ; public int Price ; public int VATID ; } class VAT { public int ID ; public int Percentage ; } List < Product > products = ... List < VAT > vat = ... System.Linq To access Linq functionality we need the System.Linq namespace: using System.Linq ;","title":"LINQ: Language Integrated Query"},{"location":"lecture-notes/linq/#linq-operations","text":"The examples below, when available, show both syntaxes.","title":"LINQ operations"},{"location":"lecture-notes/linq/#filtering","text":"products . Where ( p => p . Price < 1000 ) from p in products where p . Price < 1000","title":"Filtering"},{"location":"lecture-notes/linq/#projection","text":"products . Select ( p => p . Name ) from p in products select p . Name","title":"Projection"},{"location":"lecture-notes/linq/#join","text":"from p in products join v in vat on p . VATID equals v . Id select p . Price * v . Percentage products . Join ( vat , p => p . VATID , v => v . Id , ( p , v ) => p . Price * v . Percentage )","title":"Join"},{"location":"lecture-notes/linq/#sorting","text":"products . OrderBy [ Descending ]( p => p . Name ) . ThenBy [ Descending ]( p => p . Price ) from p in products orderby p . Name , p . Price [ descending ]","title":"Sorting"},{"location":"lecture-notes/linq/#set-operations","text":"products . Select ( p => p . Name ). Distinct () products . Where ( p => p . Price < 1000 ) . Union ( products . Where ( p => p . Price > 100000 ) ) // similarly Except, Intersect","title":"Set operations"},{"location":"lecture-notes/linq/#aggregation","text":"products . Count () products . Select ( p => p . Price ). Average () // similarly Sum, Min, Max","title":"Aggregation"},{"location":"lecture-notes/linq/#first-last","text":"products . First () products . Last () products . Where ( p => p . Id == 12 ). FirstOrDefault () products . Where ( p => p . Id == 12 ). SingleOrDefault ()","title":"First, last"},{"location":"lecture-notes/linq/#paging","text":"products . Take ( 10 ) products . Skip ( 10 ). Take ( 10 )","title":"Paging"},{"location":"lecture-notes/linq/#contains-exists","text":"products . Any ( p => p . Price == 1234 ) products . Where ( p => p . Price == 1234 ). Any ()","title":"Contains (exists)"},{"location":"lecture-notes/linq/#grouping","text":"from p in products group p by p . VATID products . GroupBy ( p => p . VATID )","title":"Grouping"},{"location":"lecture-notes/linq/#advanced-projections","text":"During projection we can transform the results into various formats.","title":"Advanced projections"},{"location":"lecture-notes/linq/#whole-entity","text":"from p in products ... select p The result set is of type IQueryable<Product> , so we get Product instances.","title":"Whole entity"},{"location":"lecture-notes/linq/#specified-field","text":"from p in products ... select p . Name The result set is of type IQueryable<string> , so we only get the names.","title":"Specified field"},{"location":"lecture-notes/linq/#named-types","text":"from p in products ... select new MyType ( p . Name , p . Price ) The result set is of type IQueryable<MyType> , when MyType is a class we have to define and has a matching constructor.","title":"Named types"},{"location":"lecture-notes/linq/#anonym-types","text":"from p in products where p . Price > 1000 select new { ID = p . ID , Name = p . Name }; Anonym types can be instantiated using the syntax new { } . The compiler will effectively create a class definition with the properties we specified. This is generally used when we only need two or three properties, and we have no need for the entire entity. A similar use case for anonym types is when we calculate a property inside the query, such as the name of the product and the full price: from p in products join v in vat on p . VATID equals v . Id select new { Name = p . Name , FullPrice = p . Price * v . Percentage }","title":"Anonym types"},{"location":"lecture-notes/linq/#linq-expressions-and-ienumerableiqueryable","text":"Depending on the data source we are using Linq on the result of a query, such as products.Where(p => p.Price < 1000) yields a variable of type IEnumerable<T> or IQueryable<T> . Neither of these contain the result sets; they are only descriptors, that is, the operation has not yet been evaluated yet . This is called deferred execution , as the execution will only happen when the result is effectively used: when the result set is iterated (e.g. foreach ), when a specific item is accessed (see later, e.g. .First() ), when we as for a list instead ( .ToList() ). This operation is useful, because this allows us to chain LINQ operations after each other, such as: var l = products . Where ( p => p . Price < 1000 ) . Where ( p => p . Name . Contains ( 's' )) . OrderBy ( p => p . Name ) . Select ( p => p . Name ) ... // variable l does not contain the result foreach ( var x in l ) // this is when the execution will happen { ... } Force evaluation If we want to force the execution at any given moment, we usually use .ToList() . But this has to be considered first and only used when necessary.","title":"LINQ expressions and IEnumerable/IQueryable"},{"location":"lecture-notes/linq/#more-information-and-further-examples","text":"Lambda expressions: https://www.tutorialsteacher.com/linq/linq-lambda-expression Linq: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/","title":"More information and further examples"},{"location":"lecture-notes/mongodb/","text":"MongoDB basics, operations, and the MongoDB .NET Driver \u00b6 NoSQL databases \u00b6 NoSQL databases are data management systems that do not work with the relational data model. The NoSQL name can be a little misleading, as the concept has little to do with the SQL language - the main difference is rather the representation of the data. Why do we need such new databases when we already have the relational model and relational databases? A small database with a simple schema can be easily described in the relational model. But our applications evolve: new functionalities are added, making the schema more complex. More and more data is added to the database making the maintenance inefficient. Relational databases need constant schema changes and updates, which can be cumbersome. Constant data migrations can be a pain. Furthermore, relational databases can be bottlenecks from the scalability perspective - but we will not discuss this aspect in more detail. NoSQL databases offer a solution to these problems. Instead of the rigid schema , NoSQL databases have a flexible schema . In other words, we will require less consistency regarding the data. Basic concepts of MongoDB \u00b6 MongoDB is a client-server database system that has a non-relational schema. The mongod (Mongo daemon) process on the right is the database server. The other side is our application, where a client connects to the database using a network connection. This connection uses the so-called wire protocol , which is a MongoDB proprietary communication protocol. The protocol transmits data and queries in JSON format represented in a binary fashion as BSON. Logical structure \u00b6 The top layer of a MongoDB database system is the so-called cluster . The servers are organized into these clusters. We will not discuss clusters here; these are tools for enabling scalability. The databases are the mongod processes, which host the databases. A MongoDB server/cluster stores multiple databases. And the databases contain collections . If we want to map these concepts to a relational model, then the collections correspond to the tables, and the rows/records in a table correspond to the documents of the collection. Let us investigate these further. Document \u00b6 The document is the unit of storage in MongoDB. A document is a JSON (-like) file: it contains key-value pairs. MongoDB itself stores it as a BSON in binary format. { name : \"sue\" , age : 26 , status : \"A\" , groups : [ \"news\" , \"sports\" ] } The keys can have arbitrary names with a few limitations, such that they have to be unique and cannot begin with the $ character. The names are case sensitive. Values can be string, number, date, binary, embedded document, null , or even as the groups in the example shows, an array - a relational database cannot represent an array in such a simple way. Mapping to the object-oriented world, a document is an object. MongoDB documents have a maximum size of 16MB and this is not a configurable parameter. Collection \u00b6 Collections are analogous to relational database tables, but without a schema. Collections need no definition; the system creates them upon first use. Collections are a place for \"similar\" documents. Although there is no schema, indexes can still be defined on the collections to support fast searching. Since there is no schema, there are no domain integrity requirements enforced either. Database \u00b6 The database has the same purpose as in the relational model. It gathers all data of our application. Access management is also configured on the database level. The name of databases is case sensitive and lowercase by convention. Key \u00b6 The _id field is the unambiguous identifier of each document. Other keys cannot be defined. This field does not need to be specified during insert; the client driver or the server can generate a new value (a 12 byte ObjectId by default). Uniqueness can be guaranteed with the use of indices. We can define an index and mark it as unique to create a key-like field. These unique indices can contain multiple fields too. There are no references to keys in MongoDB. A document can reference another document by copying it's key, but the system has no consistency guarantees for these (e.g., the referenced document can be deleted). MongoDB operations and the MongoDB .NET Driver \u00b6 The following code snippets use the official Nuget package MongoDB.Driver . Establishing a connection \u00b6 To access the MongoDB database, you first need a connection. A MongoClient class represents the connection. We need the server address to establish a connection (see https://docs.mongodb.com/manual/reference/connection-string/ for details on the connection string). var client = new MongoClient ( \"mongodb://localhost:27017\" ); The connection should be treated as a singleton and not disposed of. Connection lifetime The connection is typically stored in a global static variable, or an IoC (Inversion of Control) / DI (Dependency Injection) store. Although the database name may be in the connection string (e.g. mongodb://localhost:27017/datadriven ), it is used only for authentication. Thus, after establishing the connection, we need to specify what database we will use. var db = client . GetDatabase ( \"datadriven\" ); The database does not need to exist in advance. The above call will automatically create if the database does not already exist. Managing collections \u00b6 Unlike a relational database, in MongoDB, our operations are always performed on a single collection , so the selection of a collection is not part of the issued command (as in the from in SQL), but a prerequisite for the operation. You can get a specific collection by calling GetCollection ; its generic parameter is the C# class implementing the document type. var collection = db . GetCollection < BsonDocument >( \"products\" ); The basic concept of the .NET MongoDB driver is to map every document to a .NET object. This is also called ODM (Object Document Mapping) . ODM is the equivalent of ORM in the NoSQL database world. \"Raw\" json In other languages and platforms, MongoDB drivers do not always map to objects. Sample codes found on the Internet often show communication via \"raw\" JSON documents. Let's try to avoid this, as we learned in ORM, that object-oriented mapping is more convenient and secure. In the previous example, a document of the type \"BsonDocument\" is used. BsonDocument is a generic document representation in which we can store key-value pairs. It is uncomfortable and unsafe to use; thus we usually do try to avoid it. See the suggested solution soon. You can run queries on the variable representing the collection, such as inserting a document and then listing the contents of the collection. The collection will be created automatically the first time you use it, so you don't have to define it. collection . InsertOne ( new BsonDocument () { { \"name\" , \"Apple\" }, { \"categoryName\" , \"Apple\" }, { \"price\" , 123 } }); // listing all documents: a search criteria is needed // this is an empty criteria matching all documents var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var l in list ) Console . WriteLine ( l ); Naming convention Field names in the document start with lowercase letters like price or categoryName (this is the so-called camel case spelling). This is a convention of the MongoDB world for historical reasons. Unless there is a good reason, do not deviate from it. Mapping documents to C# objects \u00b6 As with relational databases, we can work with objects and classes in MongoDB. The .NET driver for MongoDB offers this conveniently. The first step is to define the C# class(es) to map the contents of the database. Since there is no schema for the database and table, we cannot generate C# code based on the schema (as we did with the Entity Framework). So in this world, we tend to follow the Code First approach, which is to write C# code and have the system translate it to database collections. Let us define the following classes to represent Products . public class Product { public ObjectId Id { get ; set ; } // this will be the identifier with name _id public string Name { get ; set ; } public float Price { get ; set ; } public int Stock { get ; set ; } public string [] Categories { get ; set ; } // array field public VAT VAT { get ; set ; } // embedded document } public class VAT // this class is only ever embedded, hence needs to id { public string VATCategoryName { get ; set ; } public float Percentage { get ; set ; } } Note that the name of the field was price before, but in C# it starts with a capital letter, according to Pascal Case : Price . The MongoDB .NET driver integrates with the C# language and the .NET environment and respects its conventions so that the names in the class definition and the field names in the MongoDB documents will be mapped automatically: the Price class property will be price in the document. Customizing the mapping \u00b6 The C# class - MongoDB document mapping is automatic, but it can also be customized. There are several ways to deviate from the conventions. The easiest way is to use custom attributes in the class definition: public class Product { // maps to field _id [BsonId] public ObjectId Identifier { get ; set ; } // can specify the name explicitly [BsonElement(\"price\")] public string TotalPrice { get ; set ; } // properties can be ignored [BsonIgnore] public string DoNotSave { get ; set ; } } Our other option is to register so-called convention packs at a higher level. The convention pack describes the rules of mapping. (A set of conventions also defines the default behavior.) For example, you can specify the following to map the field names to camel case and exclude data members with a default value (defined in the C# language) from the document. // define convention pack var pack = new ConventionPack (); pack . Add ( new CamelCaseElementNameConvention ()); pack . Add ( new IgnoreIfDefaultConvention ( true )); // register the convention pack // the first parameter is a name to reference this pack // the last argument is a fitlering criteria when to use this convention ConventionRegistry . Register ( \"datadriven\" , pack , t => true ); We also have more sophisticated customizations, such as defining conversion logic for translation between a C# representation and a MongoDB representation, and specifying how to save inheritance hierarchies. For more details, see the official documentation: https://mongodb.github.io/mongo-csharp-driver/2.8/reference/bson/serialization/ . Queries \u00b6 We will use the collection from now on by mapping it to the Product class. This is the recommended solution; the BsonDocument based solution is used only when necessary. The simplest query we have already seen is to list all the documents: var collection = db . GetCollection < Product >( \"products\" ); var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var p in list ) Console . WriteLine ( $\"Id: {p.Id}, Name: {p.Name}\" ); Listing is done using the Find method. The name illustrates MongoDB's philosophy: listing an entire collection is not practical, so there is no simple syntax for it. Find requires a search criteria, which is an empty condition here to matches everything. There are several ways to describe search criteria. With BsonDocument based filtering, the filtering condition must be written according to the MongoDB syntax. We generally will avoid this because the MongoDB .NET driver provides a more convenient solution for us. In most cases, we can use Lambda expressions to describe the filtering. collection . Find ( x => x . Price < 123 ); In this case, the Lambda expression is a delegate of type Predicate <T> , that is, expects a Product and returns bool . Thus in the example above, the x variable represents a Products instance. Of course, this search also works for more complex cases. collection . Find ( x => x . Price < 123 && x . Name . Contains ( \"red\" )); The filtering described by the Lambda expressions hides what search syntax we actually have in MongoDB. For example, the above Contains search condition will actually mean a search with a regular expression. In MongoDB's own language, the previous filter looks like this: { \"price\" : { \"$lt\" : 123.0 }, \"name\" : \"/red/s\" } Note that this description is itself a document. If we wanted to write the filter condition ourselves, we would have to create this descriptor in a BsonDocument . The document keys describe the fields used for filtering, and the values are the filter criteria. In some cases, the condition is a scalar value such as a regular expression (or if we filter for equality); in other cases, the condition is an embedded document, as with the < condition. Here, the $lt key is a special key that denotes the less than operator and the value to the right of the operator is 123.0. The regular expression should be specified according to JavaScript RegExp Syntax . The conditions listed in this way are automatically evaluated in and and fashion. Instead of the Lambda expression, we can create a similar description without having to compile a filter condition in \"text\" form. The .NET driver for MongoDB gives us the ability to use a so-called builders . collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Lt ( x => x . Price , 123 ), Builders < Product >. Filter . Regex ( x => x . Name , \"/red/s\" ), ) ); The above syntax is a bit more eloquent than the Lambda expression, but it is closer to the MongoDB philosophy, and better describes what we want. We can view this syntax as SQL, a declarative, goal-oriented, but platform-specific description. However, it is also type-safe. The Builders<T> generic class is an auxiliary class that we can use to build filtering and other MongoDB specific definitions. Builders<Product>.Filter can be used to define filtering conditions that match the Product C# class. First, we create an and connection, within which we have two filtering conditions. The operators are the less than and regular expressions seen before. We pass two parameters to these functions: the field to be filtered and the operand. Note that no string-based field names were used here or in the Lambda expressions. We can refer to the class fields with the C# Expression syntax. This is practical because we avoid typing field names. Note that all ways of describing the search criteria are identical. The MongoDB driver maps each syntax to its internal representation. Lambda expression-based requires fewer characters and fits better into C#, while the builder approach is used to express MongoDB features better. You can use either one. Using query results \u00b6 The result of the collection.Find(...) function is not yet the result set, but only a descriptor to execute the query. There are generally three ways to retrieve and process the result. Listing \u00b6 Get the complete result set as a list: collection.Find(...).ToList() . Get first/single item \u00b6 If you only need the first item, or know that there will be only one item, you can use collection.Find(...).First() , .FirstOrDefault() , or .Single() , .SingleOrDefault() functions. Cursor \u00b6 If the result set contains multiple documents, it is advisable to iterate it using a cursor. MongoDB limits the size of the response to a query, so if we query too many records, we may get an error instead of a result. To overcome this, we use the cursors where we always get only a subset of the documents. var cur = collection . Find (...). ToCursor (); while ( cur . MoveNext ()) // cursor stepping { foreach ( var t in cur . Current ) // the value of the cursor is not a single document, but a list in itself { ... } } Operators for filtering \u00b6 The filter criteria apply to the fields in the document, and the filter criteria are always constant. Thus it is not possible, for example, to compare two fields , and we cannot refer to other collections. There is a so-called MongoDB aggregation pipeline, which allows you to formulate more complex queries, but for now, let us focus on simple queries. The filter condition compares a field in the document to a constant we specify. The following options are most commonly used. Comparison operators \u00b6 collection . Find ( x => x . Price == 123 ); collection . Find ( Builders < Product >. Filter . Eq ( x => x . Price , 123 )); //Eq, as in equals collection . Find ( x => x . Price != 123 ); collection . Find ( Builders < Product >. Filter . Ne ( x => x . Price , 123 )); // Ne, as in not equals collection . Find ( x => x . Price >= 123 ); collection . Find ( Builders < Product >. Filter . Gte ( x => x . Price , 123 )); // Gte, as in greater than or equal to collection . Find ( x => x . Price < 123 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . Price , 123 )); // Lt, as in less than Boolean operators \u00b6 collection . Find ( x => x . Price > 500 && x . Price < 1000 ); collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Gt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Price , 1000 ) ) ); collection . Find ( x => x . Price < 500 || x . Stock < 10 ); collection . Find ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ); collection . Find ( x => !( x . Price < 500 || x . Stock < 10 )); collection . Find ( Builders < Product >. Filter . Not ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ) ); Value is one of multiple alternatives \u00b6 collection . Find ( x => x . Id == ... || x . Id = ...); collection . Find ( Builders < Product >. Filter . In ( x => x . Id , new [] { ... })); // similarly Nin, as in not in oper\u00e1tor Value exists (not null) \u00b6 collection . Find ( x => x . VAT != null ); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT )); Exists filtering Does exist, that is, non-null filtering is special because there are two ways to have a null value in MongoDB: if the key exists in the document and it has a value of null; or if the key does not exist at all. Filtering fields of embedded document \u00b6 Embedded documents can be used for filtering in the same way. The following are all valid, and it does not matter if the embedded document ( VAT ) does not exist: collection . Find ( x => x . VAT . Percentage < 27 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . VAT . Percentage , 27 )); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT . Percentage , exists : false )); // does not exists, that is, in C#, equals null Filtering based on an array field \u00b6 Any field in the document can be an array value, as in the example string [] Categories . In MongoDB, we can define filtering based on an array field using the Any* criterion. // products of this category collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); // products that are assigned to at least one category not listed by name collection . Find ( Builders < Product >. Filter . AnyNin ( x => x . Categories , new [] { \"Balls\" , \"Rackets\" })); Any... The Any* conditions look at every element of an array but match only once with respect to the document. So, if multiple elements of an array match a condition, we only get the document once in the result set. Query execution pipeline \u00b6 MongoDB queries are executed through a pipeline. We won't go into details about this, but in addition to simple filtering, we'll see a few examples frequently used in queries. Paging, sorting \u00b6 For paging, we specify the maximum number of matching documents we request: collection . Find (...). Limit ( 100 ); And for the items on the following page, we skip the items already seen on the first page: collection . Find (...). Skip ( 100 ). Limit ( 100 ); Skip and Limit are meaningless in this form because without sorting, the \"first 100 elements\" query is not deterministic. So for these types of queries, it is necessary to provide an appropriate sorting requirement. Sorting is defined using Builders<T> . collection . Find (...) . Sort ( Builders < Product >. Sort . Ascending ( x => x . Name )) . Skip ( 100 ). Limit ( 100 ); Paging issue The above paging mechanism is still not entirely correct. For example, if a product is deleted in between the query of the first and second pages, the products will shift by one, and there may be a product that will be skipped. This is, in fact, not a problem just with MongoDB. Consider how you would solve this problem. Number of documents \u00b6 There are two ways to query the number of documents that match a query: collection . CountDocuments ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )). CountDocuments (); Aggregation pipeline \u00b6 Aggregation operations process multiple documents and return some calculated results from them. MongoDB provides three ways to perform aggregation operations: Aggregation pipelines, Single Purpose Aggregation Operations, and Map-reduce functions. Since MongoDB version 5.0, Map-reduce is an obsolete method because the aggregation pipeline is better in terms of usability and speed. For Single Purpose Aggregation Operations , MongoDB provides us with IMongoCollection<TDocument>.EstimatedDocumentCount() , IMongoCollection<TDocument>.Count() and IMongoCollection<TDocument>.Distinct() functions, which all perform simple aggregation on a single collection. Source https://docs.mongodb.com/manual/images/distinct.bakedsvg.svg General aggregations can be performed by defining a pipeline manually. An aggregation pipeline is built up from stages, each serving a specific action (filter, group, count, calculate, etc.) on its input documents. A pipeline can also return multiple results from a set of documents (e.g., total, average, maximum, or minimum values) . Let's look at this through an example of grouping. // products in the \"Balls\" category grouped by VAT percentage foreach ( var g in collection . Aggregate () . Match ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )) // filtering . Group ( x => x . VAT . Percentage , x => x ) // grouping . ToList ()) { Console . WriteLine ( $\"VAT percentage: {g.Key}\" ); foreach ( var p in g ) Console . WriteLine ( $\"\\tProduct: {p.Name}\" ); } Insert, Modify, Delete \u00b6 After queries, let's get to know data modification constructs. Inserting a new document \u00b6 To insert a new document, you need the object representing the new document. We can add this to the collection. var newProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruits\" } }; collection . InsertOne ( newProduct ); Console . WriteLine ( $\"Inserted record id id: {newProduct.Id}\" ); // after insert the ID of the document will be available in the C# instance Note that the Id field is not assigned. This will be set by the client driver. If we want, we can give it a value, but it is not customary. Remember, there is no schema in MongoDB, so the inserted document may be completely different from the rest of the items in the collection. Note that not all fields are assigned values. Because there are no integrity criteria, any insertion will be successful, but there may be problems with queries (for example, assuming that the Stock field is always set). You can use the InsertMany function to insert multiple documents, but remember that there are no transactions, so adding multiple documents is an independent operation. If, for any reason, an error occurs during the insertion, the successfully inserted documents will remain in the database. However, each document is saved atomically, so no \"half\" document can be added to the database in the event of an error. Delete documents \u00b6 To delete, you need to define a filter condition and execute it with the DeleteOne or DeleteMany functions. The difference is that DeleteOne only deletes the first matching document, while DeleteMany deletes all. If you know that only one document can match this condition (for example, deleting it by ID), you should use DeleteOne as the database does not have to perform an exhaustive search. The deletion condition can be described by the syntax familiar to the search. Deletion is different from Entity Framework. Here, the entity does not have to be loaded; instead, we specify a filtering condition. var deleteResult = collection . DeleteOne ( x => x . Id == new ObjectId ( \"...\" )); Console . WriteLine ( $\"Deleted: {deleteResult.DeletedCount} records\" ); If you want to retrieve the deleted element, you can use FindOneAndDelete , which returns the deleted entity. Updating documents \u00b6 Perhaps the most interesting feature of MongoDB is the update of documents. While the functionalities showed before (queries, inserts, deletions) are similar to most databases (either relational or NoSQL), MongoDB supports a much broader range of modification operations. There are two ways to change a document: replace the entire document with a new one or update its parts. Complete document replacement \u00b6 To replace a document completely, we need a filtering condition to specify which document we want to replace; and we need a new document. var replacementProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruit\" } }; var replaceResult = collection . ReplaceOne ( x => x . Id == new ObjectId ( \"...\" ), replacementProduct ); Console . WriteLine ( $\"Updated: {replaceResult.ModifiedCount}\" ); A single document is matched and replaces it with another document. The operation itself is atomic, that is, if it is interrupted, no half document is saved. You can use the FindOneAndReplace method to get the pre-swap document. Interesting It is also possible to change the document ID during update (the replacement document can have a different ID). Document update operators \u00b6 Document update operators can change the value of a document's fields atomically without replacing the entire document. We use the help of the Builder<T> to describe the modifying operations. Set your stock to a constant value: collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 )); The first parameter of the UpdateOne function is the filter condition. You can use any of the syntax described before. The second parameter is the descriptor of the update operation, which you can build with Builders<T> . In the example code above, the argument names are specified ( filter: and update: ) to make it clear what the parameter represents. This is optional, but it increases readability (at the expense of code length). The operation can update multiple fields at the same time. collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 ) . CurrentDate ( x => x . StockUpdated ) . Unset ( x => x . NeedsUpdate ) ); Typical modifier operators are: Set : Set the value of the field; SetOnInsert : like Set but executed only when a new document is inserted (see upsert below); Unset : delete field (remove key and value from document); CurrentDate : set the current date; Inc : increment value; Min , Max : change the value of a field if the value entered is smaller / larger than the current value of the field; Mul : value multiplication; PopFirst , PopLast : remove first / last element from an array; Pull : remove value from an array; Push : add value to an array at the end (further options in the same operator: array sorting, keeping the first n element of an array); AddToSet : add a value to an array if it does not already exist. The above operations are meaningful even if the specified field does not exist. Depending on the type of operator, the database will make changes to a default value. For example, for Inc and Mul , the field will be set to 0 and then modified. For array operations, an empty array is modified. For other operations, you can look up the behavior in the documentation . Multiple documents can be modified at the same time using this method. The requested update operations are performed on all documents that match the filter criteria. For example: in view of the summer season, put all balls on sale with a 25% discount. collection . UpdateMany ( filter : Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" ), update : Builders < Product >. Update . Mul ( x => x . Price , 0.75 ) . AddToSet ( x => x . Categories , \"On sale\" )); Update operators change the documents atomically. Using them can eliminate some of the problems caused by concurrent data access. Upsert : updating or inserting a document \u00b6 During update operations, we have the option to upsert (update/insert) . This means that either an insertion or an update is made, depending on whether the item was in the database. The default behavior is not to upsert, we must request it explicitly. collection . ReplaceOne ( filter : x => x . Id == new ObjectId ( \"...\" ), replacement : replacementObject , options : new UpdateOptions () { IsUpsert = true }); We can also do upsert with update operators. As we have seen, modifier operators are not concerned about missing fields. Likewise, it does not matter if the document does not exist; this is equivalent to performing a modifying operation on a completely blank document. collection . UpdateOne ( filter : ..., update : ..., options : new UpdateOptions () { IsUpsert = true }); The upsert operation can be a workaround for managing concurrency in the absence of a transaction. Because we do not have a transaction, we cannot verify before insertion that a particular record does not yet exist. Instead, we can use the upsert method, which allows atomic querying and insertion/modification. merge Note: In SQL, the merge command provides a similar solution.","title":"MongoDB basics, operations, and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#mongodb-basics-operations-and-the-mongodb-net-driver","text":"","title":"MongoDB basics, operations, and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#nosql-databases","text":"NoSQL databases are data management systems that do not work with the relational data model. The NoSQL name can be a little misleading, as the concept has little to do with the SQL language - the main difference is rather the representation of the data. Why do we need such new databases when we already have the relational model and relational databases? A small database with a simple schema can be easily described in the relational model. But our applications evolve: new functionalities are added, making the schema more complex. More and more data is added to the database making the maintenance inefficient. Relational databases need constant schema changes and updates, which can be cumbersome. Constant data migrations can be a pain. Furthermore, relational databases can be bottlenecks from the scalability perspective - but we will not discuss this aspect in more detail. NoSQL databases offer a solution to these problems. Instead of the rigid schema , NoSQL databases have a flexible schema . In other words, we will require less consistency regarding the data.","title":"NoSQL databases"},{"location":"lecture-notes/mongodb/#basic-concepts-of-mongodb","text":"MongoDB is a client-server database system that has a non-relational schema. The mongod (Mongo daemon) process on the right is the database server. The other side is our application, where a client connects to the database using a network connection. This connection uses the so-called wire protocol , which is a MongoDB proprietary communication protocol. The protocol transmits data and queries in JSON format represented in a binary fashion as BSON.","title":"Basic concepts of MongoDB"},{"location":"lecture-notes/mongodb/#logical-structure","text":"The top layer of a MongoDB database system is the so-called cluster . The servers are organized into these clusters. We will not discuss clusters here; these are tools for enabling scalability. The databases are the mongod processes, which host the databases. A MongoDB server/cluster stores multiple databases. And the databases contain collections . If we want to map these concepts to a relational model, then the collections correspond to the tables, and the rows/records in a table correspond to the documents of the collection. Let us investigate these further.","title":"Logical structure"},{"location":"lecture-notes/mongodb/#document","text":"The document is the unit of storage in MongoDB. A document is a JSON (-like) file: it contains key-value pairs. MongoDB itself stores it as a BSON in binary format. { name : \"sue\" , age : 26 , status : \"A\" , groups : [ \"news\" , \"sports\" ] } The keys can have arbitrary names with a few limitations, such that they have to be unique and cannot begin with the $ character. The names are case sensitive. Values can be string, number, date, binary, embedded document, null , or even as the groups in the example shows, an array - a relational database cannot represent an array in such a simple way. Mapping to the object-oriented world, a document is an object. MongoDB documents have a maximum size of 16MB and this is not a configurable parameter.","title":"Document"},{"location":"lecture-notes/mongodb/#collection","text":"Collections are analogous to relational database tables, but without a schema. Collections need no definition; the system creates them upon first use. Collections are a place for \"similar\" documents. Although there is no schema, indexes can still be defined on the collections to support fast searching. Since there is no schema, there are no domain integrity requirements enforced either.","title":"Collection"},{"location":"lecture-notes/mongodb/#database","text":"The database has the same purpose as in the relational model. It gathers all data of our application. Access management is also configured on the database level. The name of databases is case sensitive and lowercase by convention.","title":"Database"},{"location":"lecture-notes/mongodb/#key","text":"The _id field is the unambiguous identifier of each document. Other keys cannot be defined. This field does not need to be specified during insert; the client driver or the server can generate a new value (a 12 byte ObjectId by default). Uniqueness can be guaranteed with the use of indices. We can define an index and mark it as unique to create a key-like field. These unique indices can contain multiple fields too. There are no references to keys in MongoDB. A document can reference another document by copying it's key, but the system has no consistency guarantees for these (e.g., the referenced document can be deleted).","title":"Key"},{"location":"lecture-notes/mongodb/#mongodb-operations-and-the-mongodb-net-driver","text":"The following code snippets use the official Nuget package MongoDB.Driver .","title":"MongoDB operations and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#establishing-a-connection","text":"To access the MongoDB database, you first need a connection. A MongoClient class represents the connection. We need the server address to establish a connection (see https://docs.mongodb.com/manual/reference/connection-string/ for details on the connection string). var client = new MongoClient ( \"mongodb://localhost:27017\" ); The connection should be treated as a singleton and not disposed of. Connection lifetime The connection is typically stored in a global static variable, or an IoC (Inversion of Control) / DI (Dependency Injection) store. Although the database name may be in the connection string (e.g. mongodb://localhost:27017/datadriven ), it is used only for authentication. Thus, after establishing the connection, we need to specify what database we will use. var db = client . GetDatabase ( \"datadriven\" ); The database does not need to exist in advance. The above call will automatically create if the database does not already exist.","title":"Establishing a connection"},{"location":"lecture-notes/mongodb/#managing-collections","text":"Unlike a relational database, in MongoDB, our operations are always performed on a single collection , so the selection of a collection is not part of the issued command (as in the from in SQL), but a prerequisite for the operation. You can get a specific collection by calling GetCollection ; its generic parameter is the C# class implementing the document type. var collection = db . GetCollection < BsonDocument >( \"products\" ); The basic concept of the .NET MongoDB driver is to map every document to a .NET object. This is also called ODM (Object Document Mapping) . ODM is the equivalent of ORM in the NoSQL database world. \"Raw\" json In other languages and platforms, MongoDB drivers do not always map to objects. Sample codes found on the Internet often show communication via \"raw\" JSON documents. Let's try to avoid this, as we learned in ORM, that object-oriented mapping is more convenient and secure. In the previous example, a document of the type \"BsonDocument\" is used. BsonDocument is a generic document representation in which we can store key-value pairs. It is uncomfortable and unsafe to use; thus we usually do try to avoid it. See the suggested solution soon. You can run queries on the variable representing the collection, such as inserting a document and then listing the contents of the collection. The collection will be created automatically the first time you use it, so you don't have to define it. collection . InsertOne ( new BsonDocument () { { \"name\" , \"Apple\" }, { \"categoryName\" , \"Apple\" }, { \"price\" , 123 } }); // listing all documents: a search criteria is needed // this is an empty criteria matching all documents var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var l in list ) Console . WriteLine ( l ); Naming convention Field names in the document start with lowercase letters like price or categoryName (this is the so-called camel case spelling). This is a convention of the MongoDB world for historical reasons. Unless there is a good reason, do not deviate from it.","title":"Managing collections"},{"location":"lecture-notes/mongodb/#mapping-documents-to-c-objects","text":"As with relational databases, we can work with objects and classes in MongoDB. The .NET driver for MongoDB offers this conveniently. The first step is to define the C# class(es) to map the contents of the database. Since there is no schema for the database and table, we cannot generate C# code based on the schema (as we did with the Entity Framework). So in this world, we tend to follow the Code First approach, which is to write C# code and have the system translate it to database collections. Let us define the following classes to represent Products . public class Product { public ObjectId Id { get ; set ; } // this will be the identifier with name _id public string Name { get ; set ; } public float Price { get ; set ; } public int Stock { get ; set ; } public string [] Categories { get ; set ; } // array field public VAT VAT { get ; set ; } // embedded document } public class VAT // this class is only ever embedded, hence needs to id { public string VATCategoryName { get ; set ; } public float Percentage { get ; set ; } } Note that the name of the field was price before, but in C# it starts with a capital letter, according to Pascal Case : Price . The MongoDB .NET driver integrates with the C# language and the .NET environment and respects its conventions so that the names in the class definition and the field names in the MongoDB documents will be mapped automatically: the Price class property will be price in the document.","title":"Mapping documents to C# objects"},{"location":"lecture-notes/mongodb/#customizing-the-mapping","text":"The C# class - MongoDB document mapping is automatic, but it can also be customized. There are several ways to deviate from the conventions. The easiest way is to use custom attributes in the class definition: public class Product { // maps to field _id [BsonId] public ObjectId Identifier { get ; set ; } // can specify the name explicitly [BsonElement(\"price\")] public string TotalPrice { get ; set ; } // properties can be ignored [BsonIgnore] public string DoNotSave { get ; set ; } } Our other option is to register so-called convention packs at a higher level. The convention pack describes the rules of mapping. (A set of conventions also defines the default behavior.) For example, you can specify the following to map the field names to camel case and exclude data members with a default value (defined in the C# language) from the document. // define convention pack var pack = new ConventionPack (); pack . Add ( new CamelCaseElementNameConvention ()); pack . Add ( new IgnoreIfDefaultConvention ( true )); // register the convention pack // the first parameter is a name to reference this pack // the last argument is a fitlering criteria when to use this convention ConventionRegistry . Register ( \"datadriven\" , pack , t => true ); We also have more sophisticated customizations, such as defining conversion logic for translation between a C# representation and a MongoDB representation, and specifying how to save inheritance hierarchies. For more details, see the official documentation: https://mongodb.github.io/mongo-csharp-driver/2.8/reference/bson/serialization/ .","title":"Customizing the mapping"},{"location":"lecture-notes/mongodb/#queries","text":"We will use the collection from now on by mapping it to the Product class. This is the recommended solution; the BsonDocument based solution is used only when necessary. The simplest query we have already seen is to list all the documents: var collection = db . GetCollection < Product >( \"products\" ); var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var p in list ) Console . WriteLine ( $\"Id: {p.Id}, Name: {p.Name}\" ); Listing is done using the Find method. The name illustrates MongoDB's philosophy: listing an entire collection is not practical, so there is no simple syntax for it. Find requires a search criteria, which is an empty condition here to matches everything. There are several ways to describe search criteria. With BsonDocument based filtering, the filtering condition must be written according to the MongoDB syntax. We generally will avoid this because the MongoDB .NET driver provides a more convenient solution for us. In most cases, we can use Lambda expressions to describe the filtering. collection . Find ( x => x . Price < 123 ); In this case, the Lambda expression is a delegate of type Predicate <T> , that is, expects a Product and returns bool . Thus in the example above, the x variable represents a Products instance. Of course, this search also works for more complex cases. collection . Find ( x => x . Price < 123 && x . Name . Contains ( \"red\" )); The filtering described by the Lambda expressions hides what search syntax we actually have in MongoDB. For example, the above Contains search condition will actually mean a search with a regular expression. In MongoDB's own language, the previous filter looks like this: { \"price\" : { \"$lt\" : 123.0 }, \"name\" : \"/red/s\" } Note that this description is itself a document. If we wanted to write the filter condition ourselves, we would have to create this descriptor in a BsonDocument . The document keys describe the fields used for filtering, and the values are the filter criteria. In some cases, the condition is a scalar value such as a regular expression (or if we filter for equality); in other cases, the condition is an embedded document, as with the < condition. Here, the $lt key is a special key that denotes the less than operator and the value to the right of the operator is 123.0. The regular expression should be specified according to JavaScript RegExp Syntax . The conditions listed in this way are automatically evaluated in and and fashion. Instead of the Lambda expression, we can create a similar description without having to compile a filter condition in \"text\" form. The .NET driver for MongoDB gives us the ability to use a so-called builders . collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Lt ( x => x . Price , 123 ), Builders < Product >. Filter . Regex ( x => x . Name , \"/red/s\" ), ) ); The above syntax is a bit more eloquent than the Lambda expression, but it is closer to the MongoDB philosophy, and better describes what we want. We can view this syntax as SQL, a declarative, goal-oriented, but platform-specific description. However, it is also type-safe. The Builders<T> generic class is an auxiliary class that we can use to build filtering and other MongoDB specific definitions. Builders<Product>.Filter can be used to define filtering conditions that match the Product C# class. First, we create an and connection, within which we have two filtering conditions. The operators are the less than and regular expressions seen before. We pass two parameters to these functions: the field to be filtered and the operand. Note that no string-based field names were used here or in the Lambda expressions. We can refer to the class fields with the C# Expression syntax. This is practical because we avoid typing field names. Note that all ways of describing the search criteria are identical. The MongoDB driver maps each syntax to its internal representation. Lambda expression-based requires fewer characters and fits better into C#, while the builder approach is used to express MongoDB features better. You can use either one.","title":"Queries"},{"location":"lecture-notes/mongodb/#using-query-results","text":"The result of the collection.Find(...) function is not yet the result set, but only a descriptor to execute the query. There are generally three ways to retrieve and process the result.","title":"Using query results"},{"location":"lecture-notes/mongodb/#listing","text":"Get the complete result set as a list: collection.Find(...).ToList() .","title":"Listing"},{"location":"lecture-notes/mongodb/#get-firstsingle-item","text":"If you only need the first item, or know that there will be only one item, you can use collection.Find(...).First() , .FirstOrDefault() , or .Single() , .SingleOrDefault() functions.","title":"Get first/single item"},{"location":"lecture-notes/mongodb/#cursor","text":"If the result set contains multiple documents, it is advisable to iterate it using a cursor. MongoDB limits the size of the response to a query, so if we query too many records, we may get an error instead of a result. To overcome this, we use the cursors where we always get only a subset of the documents. var cur = collection . Find (...). ToCursor (); while ( cur . MoveNext ()) // cursor stepping { foreach ( var t in cur . Current ) // the value of the cursor is not a single document, but a list in itself { ... } }","title":"Cursor"},{"location":"lecture-notes/mongodb/#operators-for-filtering","text":"The filter criteria apply to the fields in the document, and the filter criteria are always constant. Thus it is not possible, for example, to compare two fields , and we cannot refer to other collections. There is a so-called MongoDB aggregation pipeline, which allows you to formulate more complex queries, but for now, let us focus on simple queries. The filter condition compares a field in the document to a constant we specify. The following options are most commonly used.","title":"Operators for filtering"},{"location":"lecture-notes/mongodb/#comparison-operators","text":"collection . Find ( x => x . Price == 123 ); collection . Find ( Builders < Product >. Filter . Eq ( x => x . Price , 123 )); //Eq, as in equals collection . Find ( x => x . Price != 123 ); collection . Find ( Builders < Product >. Filter . Ne ( x => x . Price , 123 )); // Ne, as in not equals collection . Find ( x => x . Price >= 123 ); collection . Find ( Builders < Product >. Filter . Gte ( x => x . Price , 123 )); // Gte, as in greater than or equal to collection . Find ( x => x . Price < 123 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . Price , 123 )); // Lt, as in less than","title":"Comparison operators"},{"location":"lecture-notes/mongodb/#boolean-operators","text":"collection . Find ( x => x . Price > 500 && x . Price < 1000 ); collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Gt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Price , 1000 ) ) ); collection . Find ( x => x . Price < 500 || x . Stock < 10 ); collection . Find ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ); collection . Find ( x => !( x . Price < 500 || x . Stock < 10 )); collection . Find ( Builders < Product >. Filter . Not ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ) );","title":"Boolean operators"},{"location":"lecture-notes/mongodb/#value-is-one-of-multiple-alternatives","text":"collection . Find ( x => x . Id == ... || x . Id = ...); collection . Find ( Builders < Product >. Filter . In ( x => x . Id , new [] { ... })); // similarly Nin, as in not in oper\u00e1tor","title":"Value is one of multiple alternatives"},{"location":"lecture-notes/mongodb/#value-exists-not-null","text":"collection . Find ( x => x . VAT != null ); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT )); Exists filtering Does exist, that is, non-null filtering is special because there are two ways to have a null value in MongoDB: if the key exists in the document and it has a value of null; or if the key does not exist at all.","title":"Value exists (not null)"},{"location":"lecture-notes/mongodb/#filtering-fields-of-embedded-document","text":"Embedded documents can be used for filtering in the same way. The following are all valid, and it does not matter if the embedded document ( VAT ) does not exist: collection . Find ( x => x . VAT . Percentage < 27 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . VAT . Percentage , 27 )); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT . Percentage , exists : false )); // does not exists, that is, in C#, equals null","title":"Filtering fields of embedded document"},{"location":"lecture-notes/mongodb/#filtering-based-on-an-array-field","text":"Any field in the document can be an array value, as in the example string [] Categories . In MongoDB, we can define filtering based on an array field using the Any* criterion. // products of this category collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); // products that are assigned to at least one category not listed by name collection . Find ( Builders < Product >. Filter . AnyNin ( x => x . Categories , new [] { \"Balls\" , \"Rackets\" })); Any... The Any* conditions look at every element of an array but match only once with respect to the document. So, if multiple elements of an array match a condition, we only get the document once in the result set.","title":"Filtering based on an array field"},{"location":"lecture-notes/mongodb/#query-execution-pipeline","text":"MongoDB queries are executed through a pipeline. We won't go into details about this, but in addition to simple filtering, we'll see a few examples frequently used in queries.","title":"Query execution pipeline"},{"location":"lecture-notes/mongodb/#paging-sorting","text":"For paging, we specify the maximum number of matching documents we request: collection . Find (...). Limit ( 100 ); And for the items on the following page, we skip the items already seen on the first page: collection . Find (...). Skip ( 100 ). Limit ( 100 ); Skip and Limit are meaningless in this form because without sorting, the \"first 100 elements\" query is not deterministic. So for these types of queries, it is necessary to provide an appropriate sorting requirement. Sorting is defined using Builders<T> . collection . Find (...) . Sort ( Builders < Product >. Sort . Ascending ( x => x . Name )) . Skip ( 100 ). Limit ( 100 ); Paging issue The above paging mechanism is still not entirely correct. For example, if a product is deleted in between the query of the first and second pages, the products will shift by one, and there may be a product that will be skipped. This is, in fact, not a problem just with MongoDB. Consider how you would solve this problem.","title":"Paging, sorting"},{"location":"lecture-notes/mongodb/#number-of-documents","text":"There are two ways to query the number of documents that match a query: collection . CountDocuments ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )). CountDocuments ();","title":"Number of documents"},{"location":"lecture-notes/mongodb/#aggregation-pipeline","text":"Aggregation operations process multiple documents and return some calculated results from them. MongoDB provides three ways to perform aggregation operations: Aggregation pipelines, Single Purpose Aggregation Operations, and Map-reduce functions. Since MongoDB version 5.0, Map-reduce is an obsolete method because the aggregation pipeline is better in terms of usability and speed. For Single Purpose Aggregation Operations , MongoDB provides us with IMongoCollection<TDocument>.EstimatedDocumentCount() , IMongoCollection<TDocument>.Count() and IMongoCollection<TDocument>.Distinct() functions, which all perform simple aggregation on a single collection. Source https://docs.mongodb.com/manual/images/distinct.bakedsvg.svg General aggregations can be performed by defining a pipeline manually. An aggregation pipeline is built up from stages, each serving a specific action (filter, group, count, calculate, etc.) on its input documents. A pipeline can also return multiple results from a set of documents (e.g., total, average, maximum, or minimum values) . Let's look at this through an example of grouping. // products in the \"Balls\" category grouped by VAT percentage foreach ( var g in collection . Aggregate () . Match ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )) // filtering . Group ( x => x . VAT . Percentage , x => x ) // grouping . ToList ()) { Console . WriteLine ( $\"VAT percentage: {g.Key}\" ); foreach ( var p in g ) Console . WriteLine ( $\"\\tProduct: {p.Name}\" ); }","title":"Aggregation pipeline"},{"location":"lecture-notes/mongodb/#insert-modify-delete","text":"After queries, let's get to know data modification constructs.","title":"Insert, Modify, Delete"},{"location":"lecture-notes/mongodb/#inserting-a-new-document","text":"To insert a new document, you need the object representing the new document. We can add this to the collection. var newProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruits\" } }; collection . InsertOne ( newProduct ); Console . WriteLine ( $\"Inserted record id id: {newProduct.Id}\" ); // after insert the ID of the document will be available in the C# instance Note that the Id field is not assigned. This will be set by the client driver. If we want, we can give it a value, but it is not customary. Remember, there is no schema in MongoDB, so the inserted document may be completely different from the rest of the items in the collection. Note that not all fields are assigned values. Because there are no integrity criteria, any insertion will be successful, but there may be problems with queries (for example, assuming that the Stock field is always set). You can use the InsertMany function to insert multiple documents, but remember that there are no transactions, so adding multiple documents is an independent operation. If, for any reason, an error occurs during the insertion, the successfully inserted documents will remain in the database. However, each document is saved atomically, so no \"half\" document can be added to the database in the event of an error.","title":"Inserting a new document"},{"location":"lecture-notes/mongodb/#delete-documents","text":"To delete, you need to define a filter condition and execute it with the DeleteOne or DeleteMany functions. The difference is that DeleteOne only deletes the first matching document, while DeleteMany deletes all. If you know that only one document can match this condition (for example, deleting it by ID), you should use DeleteOne as the database does not have to perform an exhaustive search. The deletion condition can be described by the syntax familiar to the search. Deletion is different from Entity Framework. Here, the entity does not have to be loaded; instead, we specify a filtering condition. var deleteResult = collection . DeleteOne ( x => x . Id == new ObjectId ( \"...\" )); Console . WriteLine ( $\"Deleted: {deleteResult.DeletedCount} records\" ); If you want to retrieve the deleted element, you can use FindOneAndDelete , which returns the deleted entity.","title":"Delete documents"},{"location":"lecture-notes/mongodb/#updating-documents","text":"Perhaps the most interesting feature of MongoDB is the update of documents. While the functionalities showed before (queries, inserts, deletions) are similar to most databases (either relational or NoSQL), MongoDB supports a much broader range of modification operations. There are two ways to change a document: replace the entire document with a new one or update its parts.","title":"Updating documents"},{"location":"lecture-notes/mongodb/#complete-document-replacement","text":"To replace a document completely, we need a filtering condition to specify which document we want to replace; and we need a new document. var replacementProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruit\" } }; var replaceResult = collection . ReplaceOne ( x => x . Id == new ObjectId ( \"...\" ), replacementProduct ); Console . WriteLine ( $\"Updated: {replaceResult.ModifiedCount}\" ); A single document is matched and replaces it with another document. The operation itself is atomic, that is, if it is interrupted, no half document is saved. You can use the FindOneAndReplace method to get the pre-swap document. Interesting It is also possible to change the document ID during update (the replacement document can have a different ID).","title":"Complete document replacement"},{"location":"lecture-notes/mongodb/#document-update-operators","text":"Document update operators can change the value of a document's fields atomically without replacing the entire document. We use the help of the Builder<T> to describe the modifying operations. Set your stock to a constant value: collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 )); The first parameter of the UpdateOne function is the filter condition. You can use any of the syntax described before. The second parameter is the descriptor of the update operation, which you can build with Builders<T> . In the example code above, the argument names are specified ( filter: and update: ) to make it clear what the parameter represents. This is optional, but it increases readability (at the expense of code length). The operation can update multiple fields at the same time. collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 ) . CurrentDate ( x => x . StockUpdated ) . Unset ( x => x . NeedsUpdate ) ); Typical modifier operators are: Set : Set the value of the field; SetOnInsert : like Set but executed only when a new document is inserted (see upsert below); Unset : delete field (remove key and value from document); CurrentDate : set the current date; Inc : increment value; Min , Max : change the value of a field if the value entered is smaller / larger than the current value of the field; Mul : value multiplication; PopFirst , PopLast : remove first / last element from an array; Pull : remove value from an array; Push : add value to an array at the end (further options in the same operator: array sorting, keeping the first n element of an array); AddToSet : add a value to an array if it does not already exist. The above operations are meaningful even if the specified field does not exist. Depending on the type of operator, the database will make changes to a default value. For example, for Inc and Mul , the field will be set to 0 and then modified. For array operations, an empty array is modified. For other operations, you can look up the behavior in the documentation . Multiple documents can be modified at the same time using this method. The requested update operations are performed on all documents that match the filter criteria. For example: in view of the summer season, put all balls on sale with a 25% discount. collection . UpdateMany ( filter : Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" ), update : Builders < Product >. Update . Mul ( x => x . Price , 0.75 ) . AddToSet ( x => x . Categories , \"On sale\" )); Update operators change the documents atomically. Using them can eliminate some of the problems caused by concurrent data access.","title":"Document update operators"},{"location":"lecture-notes/mongodb/#upsert-updating-or-inserting-a-document","text":"During update operations, we have the option to upsert (update/insert) . This means that either an insertion or an update is made, depending on whether the item was in the database. The default behavior is not to upsert, we must request it explicitly. collection . ReplaceOne ( filter : x => x . Id == new ObjectId ( \"...\" ), replacement : replacementObject , options : new UpdateOptions () { IsUpsert = true }); We can also do upsert with update operators. As we have seen, modifier operators are not concerned about missing fields. Likewise, it does not matter if the document does not exist; this is equivalent to performing a modifying operation on a completely blank document. collection . UpdateOne ( filter : ..., update : ..., options : new UpdateOptions () { IsUpsert = true }); The upsert operation can be a workaround for managing concurrency in the absence of a transaction. Because we do not have a transaction, we cannot verify before insertion that a particular record does not yet exist. Instead, we can use the upsert method, which allows atomic querying and insertion/modification. merge Note: In SQL, the merge command provides a similar solution.","title":"Upsert: updating or inserting a document"},{"location":"lecture-notes/mssql/server-side-programming/","text":"Microsoft SQL Server programming \u00b6 The language of the Microsoft SQL Server platform is T-SQL . The T-SQL language is platform-specific, meaning the language can only be used in MSSQL server - although other platforms have similar languages. The T-SQL language and the database server-side programming tools it supports extend the originally declarative SQL language with imperative tools such as variables, branches, procedures, and additional tools such as triggers and cursors. Server-side programming \u00b6 Server-side programming By server-side or database server-side programming, we mean that we execute not only commands to query and modify data in the database, but also carry out business logic inside the database. To understand when it is worthwhile to use server-side programming tools, it is first important to understand why we would consider writing business logic in the database at all. Why would we want to implement business logic tasks in the database? In a layered architecture, the lower layer provides services to the layer above it. So the upper layer \"can't get around\" the layer below; the operations have to go through the lower layer. But when you consider C#/Java/C++/ etc. code, we may not be able to guarantee such rules in the codebase. If we implement a complex set of rules and logic in a C# class, for example, it is difficult to guarantee that this class cannot be \"bypassed.\" However, if the logic is in the database, it cannot be bypassed or circumvented. This will also be due to the fact that server-side programming gives us tools that ensure the execution of certain logic under all circumstances (see triggers later). There are advantages and disadvantages to server-side programming. When considering the implementation of a functionality, in addition to knowing the layered architecture, we also need to look at what the technologies allow and which of the possible alternatives has the most benefits. If we implement business functionality in the database, we ensure the following benefits . The responsibility of the database for managing consistency becomes even more evident. The relational model places great emphasis on consistency, but not all business consistency rules can be described directly in the relational model. Just think of the example of the Neptune system, where courses have an enrollment limit. This is a business rule, and if we break it, our data is inconsistent in the business sense. If the database is responsible for complying with this rule, we can ensure that the data is always consistent. We can reduce data traffic going out of the database. We often query data to display it to the user, which we cannot reduce. But if we query data only to make a decision based on it in the business logic layer, it is possible to avoid transferring the data between the database and the business logic if we bring the logic into the database instead. This is also more secure because no data is sent over the network unnecessarily (where sensitive data may be intercepted or outputted into error messages and log files by accident). The logic written in the database server can also be thought of as an interface that hides the details of data access and modification from the user (here: data access layer or business logic layer). On the one hand, this provides us with a level of abstraction, and on the other hand, it can aid parallel, faster development. While one development team builds the logic in the database, another team can write the application on top of it because the interface is defined earlier. Fixing errors is also more straightforward when the error is in the database. In this case, it is enough to fix the code in the database. Any system built on top of it will work correctly right away (unlike fixing a bug in Java code, because then a new version of the Java application has to be released and installed too). Of course, there are disadvantages to server-side programming. The language we use is platform-dependent. We cannot transfer solutions from one database system to another. Moreover, programming knowledge itself is not easily transferable. A C++ programmer can code in C# more quickly than if he did not have such knowledge. But this is not true for server-side programming. One platform does not support the same tools as the other. The syntax of the languages also differs significantly. Database server-side programming requires an entirely new approach and different techniques. The load of the database server is increased. If a server performs more tasks, it will require more resources. Databases are critical points of data-driven systems, primarily since classical relational databases do not support horizontal scaling too well (load balancing between multiple servers). If the database server is responsible for more tasks, it can quickly become the bottleneck. These techniques are no longer evolving. We might even call them outdated used only in legacy applications. This server-side world is less common nowadays in software development projects. Basics of the T-SQL language \u00b6 The T-SQL language is the language of Microsoft SQL Server, which, in addition to the standard SQL statements, allows you to: use variables, write branches and cycles, create stored procedures (\"methods\"), use cursors (iterators), define triggers (event-handling procedures), and much more. Let\u2019s look at the syntax of the language through examples. See the official documentation for the detailed syntax. The following examples can be executed on the sample database . Variables \u00b6 Variables must be declared before use. By convention, variable names begin with @ . Uninitialized variables are all NULL . DECLARE @ num int SELECT @ num -- NULL Value assignment is possible with the SET statement or directly in the declaration: DECLARE @ num int = 5 SELECT @ num -- 5 SET @ num = 3 SELECT @ num -- 3 The scope of the variable is not bound to the instruction block (between BEGIN-END ). The variable is available within the so-called batch or stored procedure: BEGIN DECLARE @ num int SET @ num = 3 END SELECT @ num -- This works, the variable is also available outside the instruction block. -- 3 GO -- starts a new batch SELECT @ num -- Error: Must declare the scalar variable \"@num\". You can also assign value to a variable via a query: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 1 If the query returns more than one row, the last value remains in the variable: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer -- there are multiple matching rows -- the last result of SELECT is stored in the variable If the query does not yield any result, the value of the variable does not change: DECLARE @ name nvarchar ( max ) SET @ name = 'aaa' SELECT @ name = Name FROM Customer WHERE ID = 99999999 -- no matching row SELECT @ name -- aaa Instruction blocks and control structures \u00b6 An instruction block is written between BEGIN-END commands: BEGIN DECLARE @ num int SET @ num = 3 END Branching is possible by using the IF-ELSE structure: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 123 IF @ name IS NOT NULL -- If the user exists BEGIN PRINT 'Updating email' UPDATE Customer SET Email = 'agh*******@gmail.com' WHERE ID = 123 END ELSE BEGIN PRINT 'No such customer' END We use the WHILE condition and a BEGIN-END statement block for looping: -- Generate at least 1000 products (e.g., for testing) WHILE ( SELECT COUNT ( * ) FROM Product ) < 1000 BEGIN INSERT INTO Product ( Name , Price , Stock , VATID , CategoryID ) VALUES ( 'Abc' , 1 , 1 , 3 , 13 ) END Built-in functions \u00b6 There are numerous built-in functions are available in T-SQL. Below are a few examples. In the following examples, the results of the functions are queried with select . This is for the sole purpose of seeing the result. A function can be used anywhere in the language where a scalar value can be used. String functions: -- Concatenation SELECT CONCAT ( 'Happy ' , 'Birthday!' ) -- Happy Birthday! -- N characters from the left SELECT LEFT ( 'ABCDEF' , 2 ) -- AB -- Text length SELECT LEN ( 'ABCDEF' ) -- 6 -- Substring replacement SELECT REPLACE ( 'Happy Birthday!' , 'day' , 'month' ) -- Happy Birthmonth! -- Lowercase conversion SELECT LOWER ( 'ABCDEF' ) -- abcdef Manage dates: -- Current date and time SELECT GETDATE () -- 2021-09-28 10: 43: 59.120 -- Date's year component SELECT YEAR ( GETDATE ()) -- 2021 -- Specific component of the date SELECT DATEPART ( day , '12 / 20/2021 ' ) SELECT DATEPART ( month , '12 / 20/2021 ' ) -- 20 -- 12 -- Difference between dates measured in a given unit (here: day) SELECT DATEDIFF ( day , '2021-09-28 12:10:09' , '2021-11-04 13:45:09' ) -- 37 Data type conversion: SELECT CAST ( '12 ' as int ) -- 12 SELECT CONVERT ( int , '12' ) -- 12 SELECT CONVERT ( int , 'aa' ) -- Error: Conversion failed when converting the varchar value 'aa' to data type int. SELECT TRY_CONVERT ( int , 'aa' ) -- NULL ISNULL : result is the first argument if it is not null, otherwise the second argument (which can be null). DECLARE @ a int DECLARE @ b int = 5 SELECT ISNULL ( @ a , @ b ) -- 5 Not to be confused with the is null condition, e.g., UPDATE Product SET Price = 111 WHERE Price is null Cursors \u00b6 A cursor is an iterator used to scroll through a set of records item by item. We use it when a query returns multiple items, and we want to process them individually. Using a cursor consists of the following steps: The cursor must be declared and then opened. The iteration takes place in a cycle. The cursor is closed and released. Declaration and opening \u00b6 A cursor is created with the DECLARE statement. We also provide the query yielding the results in the declaration. The full syntax is: DECLARE cursor name CURSOR [ FORWARD_ONLY | SCROLL ] [ STATIC | KEYSET DYNAMIC FAST_FORWARD ] [ READ_ONLY | SCROLL_LOCKS | OPTIMISTIC ] FOR query [ FOR UPDATE [ OF column name [, ... n ]]] The meaning of optional flags in the declaration are (for more details, see the documentation ): FORWARD_ONLY : only FETCH NEXT is possible SCROLL : you are free to move forward and backward in the cursor STATIC : works from a copy: the results are snapshotted when opening the cursor KEYSET : the database state at opening the cursor yields the row ids and their order, but the contents of the records are queried when fetching them DYNAMIC : each fetch gets up to date data; allows access to changes of competing transactions READ_ONLY : the contents of the cursor cannot be updated SCROLL_LOCKS : fetching locks the rows, thus guaranteeing that any subsequent update or delete statement is successful OPTIMISTIC : does not lock, uses optimistic concurrency management (to check for any changes between the time of FETCH and subsequent update ) FOR UPDATE : list of columns that can be updated The declaration is not enough to use use the cursor; it must be opened with the OPEN command. The pair of OPEN is the CLOSE command ending the use of the cursor. After closing, the cursor can be reopened, so we need to indicate when we no longer use the cursor; this is the DEALLOCATE command. (Typically, CLOSE and DEALLOCATE follow each other because we only use the cursor once.) Advancing the cursor \u00b6 The current element of the cursor is accessed by \"copying\" the values \u200b\u200binto local variable(s) using the FETCH command. The variables used here must be declared in advance. The FETCH statement typically get the following element ( FETCH NEXT ), but if the cursor is not FORWARD_ONLY , you can move back and forward too: FETCH [ NEXT | PRIORITY FIRST | LAST | ABSOLUTE { n | @ nvar } | RELATIVE { n | @ nvar } ] FROM cursor_name INTO @ variable_name [, ... n ] We can determine whether the FETCH statement was successful by querying the implicit variable @@FETCH_STATUS . The value of the variable @@FETCH_STATUS is: 0 for a successful FETCH, -1 for a failed FETCH, -2 if the requested row is missing (when using KEYSET ). The complete iteration thus requires two FETCH statements and one WHILE loop: -- declare, open ... FETCH NEXT FROM cur INTO @ var1 , @ var2 WHILE @@ FETCH_STATUS = 0 BEGIN -- ... custom logic FETCH NEXT FROM cur INTO @ var1 , @ var2 END Note that the FETCH statement appears twice here. This is because the first one outside of the loop is used to query the very first record, and the second one inside the loop retrieves each additional record one at a time. Example \u00b6 Let us see a complete example. Let us query products that have few items in stock left, and if the last sale was more than a year ago, discount the product price: -- Extract the data from the cursor into these variables DECLARE @ ProductName nvarchar ( max ) DECLARE @ ProductID int DECLARE @ LastOrder datetime DECLARE products_cur CURSOR SCROLL SCROLL_LOCKS -- Lock for guaranteed update FOR SELECT Id , Name FROM Product WHERE Stock < 3 -- Cursor query FOR UPDATE OF Price -- We also want to update the records -- Typical opening, fetch, loop OPEN products_cur FETCH FROM products_cur INTO @ ProductID , @ ProductName WHILE @@ FETCH_STATUS = 0 BEGIN -- We can perform any operation in the cycle -- Find the time of the last purchase SELECT @ LastOrder = MAX ([ Order ]. Date ) FROM [ Order ] JOIN OrderItem ON [ Order ]. Id = OrderItem . OrderId WHERE OrderItem . ProductID = @ ProductId -- Diagnostic display PRINT CONCAT ( 'ProductID:' , convert ( nvarchar , @ ProductID ), 'Last order:' , ISNULL ( convert ( nvarchar , @ LastOrder ), 'No last order' )) IF @ LastOrder IS NULL OR @ LastOrder < DATEADD ( year , - 1 , GETDATE ()) BEGIN UPDATE Product SET Price = Price * 0 . 75 WHERE CURRENT OF products_cur -- Update current cursor record -- Alternative: WHERE Id = @ProductID END -- Query next record and then go to the WHILE loop to verify if it was successful FETCH FROM products_cur INTO @ ProductID , @ ProductName END -- Stop using the cursor CLOSE products_cur DEALLOCATE products_cur Stored procedures and functions \u00b6 The codes written in the previous examples were sent to the server and executed immediately. We can also write code that is stored by the server and can be called at any later time. In a modular programming environment, we usually call these functions, and in an object-oriented world, we call them methods. In Microsoft SQL Server, these are called stored procedures and stored functions. Stored in the name indicates that the procedure code is stored in the database along with the data (and will be included in backups, for example). The difference between a procedure and a function is that procedures typically have no return value, while functions do. An additional restriction in the MSSQL platform is that functions can only read the database but not make changes. Procedures \u00b6 You can create a stored procedure with the following syntax: CREATE [ OR ALTER ] PROC [ EDURE ] procedure_name [ { @ parameter data_type } ] [, ... n ] AS [ BEGIN ] sql_instructions [... n ] [ END ] The result of the CREATE OR ALTER statement is the creation of the stored procedure, if it does not exist, or else its update with the new contents. Prior to MSSQL Server 2016, there was no CREATE OR ALTER , only CREATE PROC and ALTER PROC . We can delete a stored procedure with the DROP PROCECURE statement, which removes the procedure from the server. For example, Let us create a new tax percentage record in the VAT table, guaranteeing that only unique percentages can be added: create or alter procedure InsertNewVAT -- create a stored procedure @ Percentage int -- stored procedure parameters as begin -- this is where the code begins, which the system executes when the procedure is called begin tran -- to avoid non-repeatable reading set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values \u200b\u200b ( @ Percentage ) else print 'error' ; commit end The stored procedure is created by executing the former command, and then it can be called as follows: exec InsertNewVAT 27 Stored procedures are part of our database. For example, in Microsoft SQL Server Management Studio, it is visible here: Scalar functions \u00b6 The declaration of a function is similar to a procedure, but we must also specify the return type: CREATE [ OR ALTER ] FUNCTION name ([ { @ parameter data_type } ] [, ... n ]) RETURNS data type [ AS ] BEGIN instructions RETURN scalar_value END Let us see a function with return value int that has no input parameters: CREATE OR ALTER FUNCTION LargestVATPercentage () RETURNS int BEGIN RETURN ( SELECT MAX ( Percentage ) FROM VAT ) END Here's how to use this function: select dbo . LargestVATPercentage () -- The dbo prefix is \u200b\u200bthe name of the schema, indicating that this is not a built-in function -- Without this, the function is not found -- or for example DECLARE @ maxvat int = dbo . LargestVATPercentage () select @ maxvat Table functions \u00b6 A function can also yield a table as the result. In this case, the declaration looks like this: CREATE [ OR ALTER ] FUNCTION name ([ { @ parameter data type } ] [, ... n ]) RETURNS TABLE [ AS ] RETURN select statement For example, consider retrieving VAT rates above a certain percentage: CREATE FUNCTION VATPercentages ( @ min int ) RETURNS TABLE AS RETURN ( SELECT ID , Percentage FROM VAT WHERE Percentage > @ min ) This function returns a table, so you can use the function anywhere a table can appear, for example: SELECT * FROM VATPercentages ( 20 ) Since the function returns a table, we can even join it: SELECT VAT . Percentage , count ( * ) FROM VAT JOIN VATPercentages ( 20 ) p on VAT . ID = p . Id GROUP BY VAT . Percentage Error handling \u00b6 In the stored procedure example, we wanted to prevent duplicate records from being inserted into a table. This was accomplished above by not executing the instruction. However, it would be more appropriate to report the error to the caller. This is what structured error handling is about. In case of an error, you can use the throw command to raise an error. This command interrupts code execution and returns control to the caller (where the error can be handled or passed on). The error has a number (between 50000 and 2147483647), a text, and an error status identifier between 0-255. The updated procedure for recording the VAT key looks like this: create or alter procedure InsertNewVAT @ Percentage int as begin begin tran set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values \u200b\u200b ( @ Percentage ) else throw 51000 , 'error' , 1 ; commit end To handle (catch) an error, you can use the following syntax: begin try exec InsertNewVAT 27 end try begin catch -- access the error details with the following functions (similar to stack trace in other languages) SELECT ERROR_NUMBER () AS ErrorNumber , ERROR_SEVERITY () AS ErrorSeverity , ERROR_STATE () AS ErrorState , ERROR_PROCEDURE () AS ErrorProcedure , ERROR_LINE () AS ErrorLine , ERROR_MESSAGE () AS ErrorMessage ; end catch Of course, it's not just user code that can throw errors. The system also signals errors identically, and we can handle them using the same tools. Triggers \u00b6 The tools and language elements described so far have similar counterparts in other platforms. However, triggers are unique to databases. Triggers are event-handling stored procedures. We can subscribe to various events in the database, and when the event occurs, the system will execute our code defined in the trigger. We will only discuss DML triggers. These are triggers that run due to data modification ( insert , update , delete ) operations. There are other triggers as well; e.g., you can create triggers for system events. Check the official documentation for more details. DML triggers \u00b6 Using triggers, we can solve several tasks that would be difficult otherwise. Consider, for example, an audit logging requirement: when a change is made to a particular table, let us record a log entry. We could solve this task in C#/Java/Python by creating a class or methods for accessing the database table in question. However, nothing prevents the programmer from \"bypassing\" this logic and accessing the database directly. We cannot prevent this with triggers, but we can create a trigger that performs the required logging instead of the C#/Java/Python code. Let us look at this example: logging the deletion of any products in a dedicated table: -- Create the auditing table create table AuditLog ([ Description ] [ nvarchar ]( max ) NULL ) go -- Logging trigger create or alter trigger ProductDeleteLog on Product for delete as insert into AuditLog ( Description ) select 'Product deleted: ' + convert ( nvarchar , d . Name ) from deleted d Executing the commands above creates a trigger in the database (just as a stored procedure is created). This trigger is then executed automatically. So the trigger is not called by us but by the system. Nevertheless, we give the trigger a name to reference it (e.g., if we want to delete it with the DROP TRIGGER statement). The trigger is linked to the table in the database: The syntax for defining a DML trigger is as follows: CREATE TRIGGER trigger_name ON { table | view } FOR { [ DELETE ] [,] [ INSERT ] [,] [ UPDATE ] } AS sql_instruction [... n ] Note that in the trigger definition, we specify the table or view. So a trigger listens for events of a single table. The events are set by listing the requested modifying operations (e.g., for update, insert ). Note that three possible options cover all types of changes; also note, that there is no select event \u2014 since it is not a change. The instructions defined in the trigger code are executed after the specified events occur. This means that the changes are already performed (for example, new rows are already inserted into the table), but the transaction of the operation is not yet finished. Thus, we can make further changes as part of the same transaction (and consequently, seeing the result of the \"original\" command and the trigger as an atomic change) or even aborting the transaction. A particular use case for triggers is to check the consistency of data (that cannot be verified otherwise) and to abort the modification in the event of a violation. We will see an example of this soon. Triggers are executed per instruction , which means they are called once per DML operation. In other words, the trigger does not handle the changes per row; instead, all changes caused by a single operation are handled at once. So, for example, if an update statement changes 15 rows, the trigger is called once, and we will see all 15 changes. Of course, this is also true for inserting and deleting - a deletion operation can delete multiple rows, and we can insert multiple records with a single insert command. There is no row-level trigger Other database platforms have row-level triggers, where the trigger is called individually for all the modified rows. Microsoft SQL Server platform does not have such a trigger! How do we know what changes are handled in the trigger? Inside the trigger, we have access to two log tables through the implicit variables inserted and deleted . The structure of these tables is identical to the table on which the trigger is defined. These tables exist only during the trigger execution and can only be accessed from within the trigger. Their content depends on the type of operation that invoked the trigger: insert delete update inserted new records empty new values of records deleted empty deleted records old values \u200b\u200bof records When inserting, the inserted records can be found in the database table (but there, we do not \"see\" that they have been newly inserted), and they are also available in the inserted table. In the case of deletion, deleted contains the rows already deleted from the table. Finally, in the case of update , we see the states before and after the change in the two log tables. We need to work with these log tables as tables; we should always expect to have more than one record in them. The inserted and deleted are tables The inserted and deleted tables can only be treated as tables! For example, it does not make sense to use select @id=inserted.ID ; instead, we can use a cursor on these tables or join them. We have already seen an example of audit logging implemented with a trigger. Let us look at other use-cases. Let us have a table with an email address column. When inserting and modifying, we need to check the email address value, and we must not accept text that does not look like an email address. Here we validate a rule of consistency with the trigger. -- Create a function to check the email address CREATE FUNCTION [ IsEmailValid ]( @ email nvarchar ( 1000 )) RETURNS bit -- true / false return value AS BEGIN IF @ email is null RETURN 0 -- Cannot be null IF @ email = '' RETURN 0 -- Cannot be an empty string IF @ email LIKE '%_@%_._%' RETURN 1 -- Looks like an email RETURN 0 -- The same in one line: -- RETURN CASE WHEN ISNULL(@email, '') <> '' AND @email LIKE '%_@%_._%' THEN 1 ELSE 0 END END -- The trigger create or alter trigger CustomerEmailSyntaxCheck on Customer for insert , update -- Check both inserting and modifying as -- For both insertion and modification, the new data is in the inserted table -- Is there an item there for which the new email address is not valid? if exists ( select 1 from inserted i where dbo . IsEmailValid ( i . Email ) = 0 ) throw 51234 , 'invalid email address' , 1 -- abort the transaction by raising the error The above trigger runs after insertion or modification in the same transaction. So if we throw an error, the transaction will be aborted (unless handled by the caller). By running the trigger at the instruction level, a single faulty record interrupts the entire operation. Of course, this is what we expect due to atomicity: the indivisibility of the transaction is satisfied for the instruction as a whole, i.e., for inserting/modifying several records at once. Another common use of triggers is maintenance of denormalized data . Although we try to avoid denormalization in a relational database, in practice, it may be necessary to store computed data for performance reasons. Let us look at an example of this as well. Suppose customers have two email addresses: one to sign in with, an optional second one to use for notifications. To avoid always having to query both email addresses and choosing between the two, let us make sure the effective email address is available in the database \"calculated\" from the previous two: -- Additional email address columns for customers alter table Customer add [ NotificationEmail ] nvarchar ( max ), [ EffectiveEmail ] nvarchar ( max ) go -- Trigger to update the effective email address create or alter trigger CustomerEmailUpdate on Customer for insert , update as update Customer -- We modify the Customer table, not the inserted implicit table set EffectiveEmail = ISNULL ( i . NotificationEmail , i . Email ) -- Copy one or the other value to the EffectiveEmail column from Customer c join inserted i on c . ID = i . ID -- Records must be retrieved from the Customer table based on the inserted rows Trigger recursion Note that in this trigger, an update is executed in response to an update event. This is a recursion. Recursion of DML triggers is disabled by default, so the above example does not invoke trigger recursion. However, if trigger recursion were enabled in the database, we would need to handle it. Let us look at another example of denormalized data maintenance. In the order table, let us add a grand total column, which is the total net price of the order. We need a trigger to keep the value updated automatically: create or alter trigger OrderTotalUpdateTrigger on OrderItem for insert , update , delete as update Order set Total = isnull ( Total , 0 ) + TotalChange from Order inner join ( select i . OrderID , sum ( Amount * Price ) as TotalChange from inserted i group by i . OrderID ) OrderChange on Order . ID = OrderChange . OrderID update Order set Total = isnull ( Total , 0 ) \u2013 TotalChange from Order inner join ( select d . OrderID , sum ( Amount * Price ) as TotalChange from deleted d group by d . OrderID ) OrderChange on Order . ID = OrderChange . OrderID In this trigger, it is worth noting that while the event occurs in the OrderItem table, the content to be updated is in the Order table. This is fine, a trigger can read and write any part of the database, and all changes are executed in the same transaction. Furthermore, we do not recalculate the total amount in the trigger but alter it in response to the changes. Although this makes the trigger code more complex, it is more effective this way. Sequence of triggers We can define multiple triggers for an event. But the order of their execution cannot be specified. We can set the first and last triggers, but we cannot make assumptions regarding their sequence otherwise - it is considered ill-advised to design functionality where triggers need to build on each other. Instead of triggers \u00b6 A special type of trigger is the so-called instead of trigger . Such triggers can be defined for both tables and views. Let us look at using them on tables first. An instead of trigger defined on a table, as its name suggests, runs the instruction we define in the trigger instead of the actual operation's insert / update / delete . E.g., when inserting, the new rows are not added to the table, and when deleting, rows are not deleted. Instead, we can define in the trigger how to perform these operations. In the overridden process, we can access the table itself and execute the necessary actions in this table. These operations do not cause recursion in the trigger. These triggers can be considered as before triggers, i.e., we can perform checks before making the changes and abort the operation in case of an error. A typical use case for an instead of trigger is, for example, when we do not want to perform a deletion. This is also called soft delete :,instead of deleting, we only mark the records as deleted: -- Soft delete flag column in the table with a default value of 0 (i.e., false) alter table Product add [ IsDeleted ] bit NOT NULL CONSTRAINT DF_Product_IsDeleted DEFAULT 0 go -- Instead of trigger, the delete command does not perform the deletion -- the following code runs instead create or alter trigger ProductSoftDelete on Product instead of delete as update Product set IsDeleted = 1 where ID in ( select ID from deleted ) Another typical use case for instead of triggers is views. A view is the result of a query, so inserting new data into the view does not make sense. However, you can use an instead of trigger to define what to do instead of \"inserting into view.\" Let us look at an example. In the view below, we combine data from the product and VAT tables so that the VAT percentage is displayed in the view instead of the ID of the referenced VAT record. We can insert into this view by inserting the data into the product table instead: -- Define the view create view ProductWithVatPercentage as select p . Id , p . Name , p . Price , p . Stock , v . Percentage from Product p join Vat v is p . VATID = v . Id -- Instead of trigger for the view create or alter trigger ProductWithVatPercentageInsert on ProductWithVatPercentage instead of insert as -- The insertion goes into the Product table: a new row is created for each inserted record -- And we find the VAT record corresponding to the provided percentage -- The solution is not complete because it does not handle if there is no matching VAT record insert into Product ( Name , Price , Stock , VATID , CategoryID ) select i . Name , i . Price , i . Stock , v . ID , 1 from inserted i join VAT v on v . Percentage = i . Percentage -- The trigger can be tested by inserting data into the view insert into ProductWithVatPercentage ( Name , Price , Stock , Percentage ) values ( 'Red ball' , 1234 , 22 , 27 )","title":"Microsoft SQL Server programming"},{"location":"lecture-notes/mssql/server-side-programming/#microsoft-sql-server-programming","text":"The language of the Microsoft SQL Server platform is T-SQL . The T-SQL language is platform-specific, meaning the language can only be used in MSSQL server - although other platforms have similar languages. The T-SQL language and the database server-side programming tools it supports extend the originally declarative SQL language with imperative tools such as variables, branches, procedures, and additional tools such as triggers and cursors.","title":"Microsoft SQL Server programming"},{"location":"lecture-notes/mssql/server-side-programming/#server-side-programming","text":"Server-side programming By server-side or database server-side programming, we mean that we execute not only commands to query and modify data in the database, but also carry out business logic inside the database. To understand when it is worthwhile to use server-side programming tools, it is first important to understand why we would consider writing business logic in the database at all. Why would we want to implement business logic tasks in the database? In a layered architecture, the lower layer provides services to the layer above it. So the upper layer \"can't get around\" the layer below; the operations have to go through the lower layer. But when you consider C#/Java/C++/ etc. code, we may not be able to guarantee such rules in the codebase. If we implement a complex set of rules and logic in a C# class, for example, it is difficult to guarantee that this class cannot be \"bypassed.\" However, if the logic is in the database, it cannot be bypassed or circumvented. This will also be due to the fact that server-side programming gives us tools that ensure the execution of certain logic under all circumstances (see triggers later). There are advantages and disadvantages to server-side programming. When considering the implementation of a functionality, in addition to knowing the layered architecture, we also need to look at what the technologies allow and which of the possible alternatives has the most benefits. If we implement business functionality in the database, we ensure the following benefits . The responsibility of the database for managing consistency becomes even more evident. The relational model places great emphasis on consistency, but not all business consistency rules can be described directly in the relational model. Just think of the example of the Neptune system, where courses have an enrollment limit. This is a business rule, and if we break it, our data is inconsistent in the business sense. If the database is responsible for complying with this rule, we can ensure that the data is always consistent. We can reduce data traffic going out of the database. We often query data to display it to the user, which we cannot reduce. But if we query data only to make a decision based on it in the business logic layer, it is possible to avoid transferring the data between the database and the business logic if we bring the logic into the database instead. This is also more secure because no data is sent over the network unnecessarily (where sensitive data may be intercepted or outputted into error messages and log files by accident). The logic written in the database server can also be thought of as an interface that hides the details of data access and modification from the user (here: data access layer or business logic layer). On the one hand, this provides us with a level of abstraction, and on the other hand, it can aid parallel, faster development. While one development team builds the logic in the database, another team can write the application on top of it because the interface is defined earlier. Fixing errors is also more straightforward when the error is in the database. In this case, it is enough to fix the code in the database. Any system built on top of it will work correctly right away (unlike fixing a bug in Java code, because then a new version of the Java application has to be released and installed too). Of course, there are disadvantages to server-side programming. The language we use is platform-dependent. We cannot transfer solutions from one database system to another. Moreover, programming knowledge itself is not easily transferable. A C++ programmer can code in C# more quickly than if he did not have such knowledge. But this is not true for server-side programming. One platform does not support the same tools as the other. The syntax of the languages also differs significantly. Database server-side programming requires an entirely new approach and different techniques. The load of the database server is increased. If a server performs more tasks, it will require more resources. Databases are critical points of data-driven systems, primarily since classical relational databases do not support horizontal scaling too well (load balancing between multiple servers). If the database server is responsible for more tasks, it can quickly become the bottleneck. These techniques are no longer evolving. We might even call them outdated used only in legacy applications. This server-side world is less common nowadays in software development projects.","title":"Server-side programming"},{"location":"lecture-notes/mssql/server-side-programming/#basics-of-the-t-sql-language","text":"The T-SQL language is the language of Microsoft SQL Server, which, in addition to the standard SQL statements, allows you to: use variables, write branches and cycles, create stored procedures (\"methods\"), use cursors (iterators), define triggers (event-handling procedures), and much more. Let\u2019s look at the syntax of the language through examples. See the official documentation for the detailed syntax. The following examples can be executed on the sample database .","title":"Basics of the T-SQL language"},{"location":"lecture-notes/mssql/server-side-programming/#variables","text":"Variables must be declared before use. By convention, variable names begin with @ . Uninitialized variables are all NULL . DECLARE @ num int SELECT @ num -- NULL Value assignment is possible with the SET statement or directly in the declaration: DECLARE @ num int = 5 SELECT @ num -- 5 SET @ num = 3 SELECT @ num -- 3 The scope of the variable is not bound to the instruction block (between BEGIN-END ). The variable is available within the so-called batch or stored procedure: BEGIN DECLARE @ num int SET @ num = 3 END SELECT @ num -- This works, the variable is also available outside the instruction block. -- 3 GO -- starts a new batch SELECT @ num -- Error: Must declare the scalar variable \"@num\". You can also assign value to a variable via a query: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 1 If the query returns more than one row, the last value remains in the variable: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer -- there are multiple matching rows -- the last result of SELECT is stored in the variable If the query does not yield any result, the value of the variable does not change: DECLARE @ name nvarchar ( max ) SET @ name = 'aaa' SELECT @ name = Name FROM Customer WHERE ID = 99999999 -- no matching row SELECT @ name -- aaa","title":"Variables"},{"location":"lecture-notes/mssql/server-side-programming/#instruction-blocks-and-control-structures","text":"An instruction block is written between BEGIN-END commands: BEGIN DECLARE @ num int SET @ num = 3 END Branching is possible by using the IF-ELSE structure: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 123 IF @ name IS NOT NULL -- If the user exists BEGIN PRINT 'Updating email' UPDATE Customer SET Email = 'agh*******@gmail.com' WHERE ID = 123 END ELSE BEGIN PRINT 'No such customer' END We use the WHILE condition and a BEGIN-END statement block for looping: -- Generate at least 1000 products (e.g., for testing) WHILE ( SELECT COUNT ( * ) FROM Product ) < 1000 BEGIN INSERT INTO Product ( Name , Price , Stock , VATID , CategoryID ) VALUES ( 'Abc' , 1 , 1 , 3 , 13 ) END","title":"Instruction blocks and control structures"},{"location":"lecture-notes/mssql/server-side-programming/#built-in-functions","text":"There are numerous built-in functions are available in T-SQL. Below are a few examples. In the following examples, the results of the functions are queried with select . This is for the sole purpose of seeing the result. A function can be used anywhere in the language where a scalar value can be used. String functions: -- Concatenation SELECT CONCAT ( 'Happy ' , 'Birthday!' ) -- Happy Birthday! -- N characters from the left SELECT LEFT ( 'ABCDEF' , 2 ) -- AB -- Text length SELECT LEN ( 'ABCDEF' ) -- 6 -- Substring replacement SELECT REPLACE ( 'Happy Birthday!' , 'day' , 'month' ) -- Happy Birthmonth! -- Lowercase conversion SELECT LOWER ( 'ABCDEF' ) -- abcdef Manage dates: -- Current date and time SELECT GETDATE () -- 2021-09-28 10: 43: 59.120 -- Date's year component SELECT YEAR ( GETDATE ()) -- 2021 -- Specific component of the date SELECT DATEPART ( day , '12 / 20/2021 ' ) SELECT DATEPART ( month , '12 / 20/2021 ' ) -- 20 -- 12 -- Difference between dates measured in a given unit (here: day) SELECT DATEDIFF ( day , '2021-09-28 12:10:09' , '2021-11-04 13:45:09' ) -- 37 Data type conversion: SELECT CAST ( '12 ' as int ) -- 12 SELECT CONVERT ( int , '12' ) -- 12 SELECT CONVERT ( int , 'aa' ) -- Error: Conversion failed when converting the varchar value 'aa' to data type int. SELECT TRY_CONVERT ( int , 'aa' ) -- NULL ISNULL : result is the first argument if it is not null, otherwise the second argument (which can be null). DECLARE @ a int DECLARE @ b int = 5 SELECT ISNULL ( @ a , @ b ) -- 5 Not to be confused with the is null condition, e.g., UPDATE Product SET Price = 111 WHERE Price is null","title":"Built-in functions"},{"location":"lecture-notes/mssql/server-side-programming/#cursors","text":"A cursor is an iterator used to scroll through a set of records item by item. We use it when a query returns multiple items, and we want to process them individually. Using a cursor consists of the following steps: The cursor must be declared and then opened. The iteration takes place in a cycle. The cursor is closed and released.","title":"Cursors"},{"location":"lecture-notes/mssql/server-side-programming/#declaration-and-opening","text":"A cursor is created with the DECLARE statement. We also provide the query yielding the results in the declaration. The full syntax is: DECLARE cursor name CURSOR [ FORWARD_ONLY | SCROLL ] [ STATIC | KEYSET DYNAMIC FAST_FORWARD ] [ READ_ONLY | SCROLL_LOCKS | OPTIMISTIC ] FOR query [ FOR UPDATE [ OF column name [, ... n ]]] The meaning of optional flags in the declaration are (for more details, see the documentation ): FORWARD_ONLY : only FETCH NEXT is possible SCROLL : you are free to move forward and backward in the cursor STATIC : works from a copy: the results are snapshotted when opening the cursor KEYSET : the database state at opening the cursor yields the row ids and their order, but the contents of the records are queried when fetching them DYNAMIC : each fetch gets up to date data; allows access to changes of competing transactions READ_ONLY : the contents of the cursor cannot be updated SCROLL_LOCKS : fetching locks the rows, thus guaranteeing that any subsequent update or delete statement is successful OPTIMISTIC : does not lock, uses optimistic concurrency management (to check for any changes between the time of FETCH and subsequent update ) FOR UPDATE : list of columns that can be updated The declaration is not enough to use use the cursor; it must be opened with the OPEN command. The pair of OPEN is the CLOSE command ending the use of the cursor. After closing, the cursor can be reopened, so we need to indicate when we no longer use the cursor; this is the DEALLOCATE command. (Typically, CLOSE and DEALLOCATE follow each other because we only use the cursor once.)","title":"Declaration and opening"},{"location":"lecture-notes/mssql/server-side-programming/#advancing-the-cursor","text":"The current element of the cursor is accessed by \"copying\" the values \u200b\u200binto local variable(s) using the FETCH command. The variables used here must be declared in advance. The FETCH statement typically get the following element ( FETCH NEXT ), but if the cursor is not FORWARD_ONLY , you can move back and forward too: FETCH [ NEXT | PRIORITY FIRST | LAST | ABSOLUTE { n | @ nvar } | RELATIVE { n | @ nvar } ] FROM cursor_name INTO @ variable_name [, ... n ] We can determine whether the FETCH statement was successful by querying the implicit variable @@FETCH_STATUS . The value of the variable @@FETCH_STATUS is: 0 for a successful FETCH, -1 for a failed FETCH, -2 if the requested row is missing (when using KEYSET ). The complete iteration thus requires two FETCH statements and one WHILE loop: -- declare, open ... FETCH NEXT FROM cur INTO @ var1 , @ var2 WHILE @@ FETCH_STATUS = 0 BEGIN -- ... custom logic FETCH NEXT FROM cur INTO @ var1 , @ var2 END Note that the FETCH statement appears twice here. This is because the first one outside of the loop is used to query the very first record, and the second one inside the loop retrieves each additional record one at a time.","title":"Advancing the cursor"},{"location":"lecture-notes/mssql/server-side-programming/#example","text":"Let us see a complete example. Let us query products that have few items in stock left, and if the last sale was more than a year ago, discount the product price: -- Extract the data from the cursor into these variables DECLARE @ ProductName nvarchar ( max ) DECLARE @ ProductID int DECLARE @ LastOrder datetime DECLARE products_cur CURSOR SCROLL SCROLL_LOCKS -- Lock for guaranteed update FOR SELECT Id , Name FROM Product WHERE Stock < 3 -- Cursor query FOR UPDATE OF Price -- We also want to update the records -- Typical opening, fetch, loop OPEN products_cur FETCH FROM products_cur INTO @ ProductID , @ ProductName WHILE @@ FETCH_STATUS = 0 BEGIN -- We can perform any operation in the cycle -- Find the time of the last purchase SELECT @ LastOrder = MAX ([ Order ]. Date ) FROM [ Order ] JOIN OrderItem ON [ Order ]. Id = OrderItem . OrderId WHERE OrderItem . ProductID = @ ProductId -- Diagnostic display PRINT CONCAT ( 'ProductID:' , convert ( nvarchar , @ ProductID ), 'Last order:' , ISNULL ( convert ( nvarchar , @ LastOrder ), 'No last order' )) IF @ LastOrder IS NULL OR @ LastOrder < DATEADD ( year , - 1 , GETDATE ()) BEGIN UPDATE Product SET Price = Price * 0 . 75 WHERE CURRENT OF products_cur -- Update current cursor record -- Alternative: WHERE Id = @ProductID END -- Query next record and then go to the WHILE loop to verify if it was successful FETCH FROM products_cur INTO @ ProductID , @ ProductName END -- Stop using the cursor CLOSE products_cur DEALLOCATE products_cur","title":"Example"},{"location":"lecture-notes/mssql/server-side-programming/#stored-procedures-and-functions","text":"The codes written in the previous examples were sent to the server and executed immediately. We can also write code that is stored by the server and can be called at any later time. In a modular programming environment, we usually call these functions, and in an object-oriented world, we call them methods. In Microsoft SQL Server, these are called stored procedures and stored functions. Stored in the name indicates that the procedure code is stored in the database along with the data (and will be included in backups, for example). The difference between a procedure and a function is that procedures typically have no return value, while functions do. An additional restriction in the MSSQL platform is that functions can only read the database but not make changes.","title":"Stored procedures and functions"},{"location":"lecture-notes/mssql/server-side-programming/#procedures","text":"You can create a stored procedure with the following syntax: CREATE [ OR ALTER ] PROC [ EDURE ] procedure_name [ { @ parameter data_type } ] [, ... n ] AS [ BEGIN ] sql_instructions [... n ] [ END ] The result of the CREATE OR ALTER statement is the creation of the stored procedure, if it does not exist, or else its update with the new contents. Prior to MSSQL Server 2016, there was no CREATE OR ALTER , only CREATE PROC and ALTER PROC . We can delete a stored procedure with the DROP PROCECURE statement, which removes the procedure from the server. For example, Let us create a new tax percentage record in the VAT table, guaranteeing that only unique percentages can be added: create or alter procedure InsertNewVAT -- create a stored procedure @ Percentage int -- stored procedure parameters as begin -- this is where the code begins, which the system executes when the procedure is called begin tran -- to avoid non-repeatable reading set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values \u200b\u200b ( @ Percentage ) else print 'error' ; commit end The stored procedure is created by executing the former command, and then it can be called as follows: exec InsertNewVAT 27 Stored procedures are part of our database. For example, in Microsoft SQL Server Management Studio, it is visible here:","title":"Procedures"},{"location":"lecture-notes/mssql/server-side-programming/#scalar-functions","text":"The declaration of a function is similar to a procedure, but we must also specify the return type: CREATE [ OR ALTER ] FUNCTION name ([ { @ parameter data_type } ] [, ... n ]) RETURNS data type [ AS ] BEGIN instructions RETURN scalar_value END Let us see a function with return value int that has no input parameters: CREATE OR ALTER FUNCTION LargestVATPercentage () RETURNS int BEGIN RETURN ( SELECT MAX ( Percentage ) FROM VAT ) END Here's how to use this function: select dbo . LargestVATPercentage () -- The dbo prefix is \u200b\u200bthe name of the schema, indicating that this is not a built-in function -- Without this, the function is not found -- or for example DECLARE @ maxvat int = dbo . LargestVATPercentage () select @ maxvat","title":"Scalar functions"},{"location":"lecture-notes/mssql/server-side-programming/#table-functions","text":"A function can also yield a table as the result. In this case, the declaration looks like this: CREATE [ OR ALTER ] FUNCTION name ([ { @ parameter data type } ] [, ... n ]) RETURNS TABLE [ AS ] RETURN select statement For example, consider retrieving VAT rates above a certain percentage: CREATE FUNCTION VATPercentages ( @ min int ) RETURNS TABLE AS RETURN ( SELECT ID , Percentage FROM VAT WHERE Percentage > @ min ) This function returns a table, so you can use the function anywhere a table can appear, for example: SELECT * FROM VATPercentages ( 20 ) Since the function returns a table, we can even join it: SELECT VAT . Percentage , count ( * ) FROM VAT JOIN VATPercentages ( 20 ) p on VAT . ID = p . Id GROUP BY VAT . Percentage","title":"Table functions"},{"location":"lecture-notes/mssql/server-side-programming/#error-handling","text":"In the stored procedure example, we wanted to prevent duplicate records from being inserted into a table. This was accomplished above by not executing the instruction. However, it would be more appropriate to report the error to the caller. This is what structured error handling is about. In case of an error, you can use the throw command to raise an error. This command interrupts code execution and returns control to the caller (where the error can be handled or passed on). The error has a number (between 50000 and 2147483647), a text, and an error status identifier between 0-255. The updated procedure for recording the VAT key looks like this: create or alter procedure InsertNewVAT @ Percentage int as begin begin tran set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values \u200b\u200b ( @ Percentage ) else throw 51000 , 'error' , 1 ; commit end To handle (catch) an error, you can use the following syntax: begin try exec InsertNewVAT 27 end try begin catch -- access the error details with the following functions (similar to stack trace in other languages) SELECT ERROR_NUMBER () AS ErrorNumber , ERROR_SEVERITY () AS ErrorSeverity , ERROR_STATE () AS ErrorState , ERROR_PROCEDURE () AS ErrorProcedure , ERROR_LINE () AS ErrorLine , ERROR_MESSAGE () AS ErrorMessage ; end catch Of course, it's not just user code that can throw errors. The system also signals errors identically, and we can handle them using the same tools.","title":"Error handling"},{"location":"lecture-notes/mssql/server-side-programming/#triggers","text":"The tools and language elements described so far have similar counterparts in other platforms. However, triggers are unique to databases. Triggers are event-handling stored procedures. We can subscribe to various events in the database, and when the event occurs, the system will execute our code defined in the trigger. We will only discuss DML triggers. These are triggers that run due to data modification ( insert , update , delete ) operations. There are other triggers as well; e.g., you can create triggers for system events. Check the official documentation for more details.","title":"Triggers"},{"location":"lecture-notes/mssql/server-side-programming/#dml-triggers","text":"Using triggers, we can solve several tasks that would be difficult otherwise. Consider, for example, an audit logging requirement: when a change is made to a particular table, let us record a log entry. We could solve this task in C#/Java/Python by creating a class or methods for accessing the database table in question. However, nothing prevents the programmer from \"bypassing\" this logic and accessing the database directly. We cannot prevent this with triggers, but we can create a trigger that performs the required logging instead of the C#/Java/Python code. Let us look at this example: logging the deletion of any products in a dedicated table: -- Create the auditing table create table AuditLog ([ Description ] [ nvarchar ]( max ) NULL ) go -- Logging trigger create or alter trigger ProductDeleteLog on Product for delete as insert into AuditLog ( Description ) select 'Product deleted: ' + convert ( nvarchar , d . Name ) from deleted d Executing the commands above creates a trigger in the database (just as a stored procedure is created). This trigger is then executed automatically. So the trigger is not called by us but by the system. Nevertheless, we give the trigger a name to reference it (e.g., if we want to delete it with the DROP TRIGGER statement). The trigger is linked to the table in the database: The syntax for defining a DML trigger is as follows: CREATE TRIGGER trigger_name ON { table | view } FOR { [ DELETE ] [,] [ INSERT ] [,] [ UPDATE ] } AS sql_instruction [... n ] Note that in the trigger definition, we specify the table or view. So a trigger listens for events of a single table. The events are set by listing the requested modifying operations (e.g., for update, insert ). Note that three possible options cover all types of changes; also note, that there is no select event \u2014 since it is not a change. The instructions defined in the trigger code are executed after the specified events occur. This means that the changes are already performed (for example, new rows are already inserted into the table), but the transaction of the operation is not yet finished. Thus, we can make further changes as part of the same transaction (and consequently, seeing the result of the \"original\" command and the trigger as an atomic change) or even aborting the transaction. A particular use case for triggers is to check the consistency of data (that cannot be verified otherwise) and to abort the modification in the event of a violation. We will see an example of this soon. Triggers are executed per instruction , which means they are called once per DML operation. In other words, the trigger does not handle the changes per row; instead, all changes caused by a single operation are handled at once. So, for example, if an update statement changes 15 rows, the trigger is called once, and we will see all 15 changes. Of course, this is also true for inserting and deleting - a deletion operation can delete multiple rows, and we can insert multiple records with a single insert command. There is no row-level trigger Other database platforms have row-level triggers, where the trigger is called individually for all the modified rows. Microsoft SQL Server platform does not have such a trigger! How do we know what changes are handled in the trigger? Inside the trigger, we have access to two log tables through the implicit variables inserted and deleted . The structure of these tables is identical to the table on which the trigger is defined. These tables exist only during the trigger execution and can only be accessed from within the trigger. Their content depends on the type of operation that invoked the trigger: insert delete update inserted new records empty new values of records deleted empty deleted records old values \u200b\u200bof records When inserting, the inserted records can be found in the database table (but there, we do not \"see\" that they have been newly inserted), and they are also available in the inserted table. In the case of deletion, deleted contains the rows already deleted from the table. Finally, in the case of update , we see the states before and after the change in the two log tables. We need to work with these log tables as tables; we should always expect to have more than one record in them. The inserted and deleted are tables The inserted and deleted tables can only be treated as tables! For example, it does not make sense to use select @id=inserted.ID ; instead, we can use a cursor on these tables or join them. We have already seen an example of audit logging implemented with a trigger. Let us look at other use-cases. Let us have a table with an email address column. When inserting and modifying, we need to check the email address value, and we must not accept text that does not look like an email address. Here we validate a rule of consistency with the trigger. -- Create a function to check the email address CREATE FUNCTION [ IsEmailValid ]( @ email nvarchar ( 1000 )) RETURNS bit -- true / false return value AS BEGIN IF @ email is null RETURN 0 -- Cannot be null IF @ email = '' RETURN 0 -- Cannot be an empty string IF @ email LIKE '%_@%_._%' RETURN 1 -- Looks like an email RETURN 0 -- The same in one line: -- RETURN CASE WHEN ISNULL(@email, '') <> '' AND @email LIKE '%_@%_._%' THEN 1 ELSE 0 END END -- The trigger create or alter trigger CustomerEmailSyntaxCheck on Customer for insert , update -- Check both inserting and modifying as -- For both insertion and modification, the new data is in the inserted table -- Is there an item there for which the new email address is not valid? if exists ( select 1 from inserted i where dbo . IsEmailValid ( i . Email ) = 0 ) throw 51234 , 'invalid email address' , 1 -- abort the transaction by raising the error The above trigger runs after insertion or modification in the same transaction. So if we throw an error, the transaction will be aborted (unless handled by the caller). By running the trigger at the instruction level, a single faulty record interrupts the entire operation. Of course, this is what we expect due to atomicity: the indivisibility of the transaction is satisfied for the instruction as a whole, i.e., for inserting/modifying several records at once. Another common use of triggers is maintenance of denormalized data . Although we try to avoid denormalization in a relational database, in practice, it may be necessary to store computed data for performance reasons. Let us look at an example of this as well. Suppose customers have two email addresses: one to sign in with, an optional second one to use for notifications. To avoid always having to query both email addresses and choosing between the two, let us make sure the effective email address is available in the database \"calculated\" from the previous two: -- Additional email address columns for customers alter table Customer add [ NotificationEmail ] nvarchar ( max ), [ EffectiveEmail ] nvarchar ( max ) go -- Trigger to update the effective email address create or alter trigger CustomerEmailUpdate on Customer for insert , update as update Customer -- We modify the Customer table, not the inserted implicit table set EffectiveEmail = ISNULL ( i . NotificationEmail , i . Email ) -- Copy one or the other value to the EffectiveEmail column from Customer c join inserted i on c . ID = i . ID -- Records must be retrieved from the Customer table based on the inserted rows Trigger recursion Note that in this trigger, an update is executed in response to an update event. This is a recursion. Recursion of DML triggers is disabled by default, so the above example does not invoke trigger recursion. However, if trigger recursion were enabled in the database, we would need to handle it. Let us look at another example of denormalized data maintenance. In the order table, let us add a grand total column, which is the total net price of the order. We need a trigger to keep the value updated automatically: create or alter trigger OrderTotalUpdateTrigger on OrderItem for insert , update , delete as update Order set Total = isnull ( Total , 0 ) + TotalChange from Order inner join ( select i . OrderID , sum ( Amount * Price ) as TotalChange from inserted i group by i . OrderID ) OrderChange on Order . ID = OrderChange . OrderID update Order set Total = isnull ( Total , 0 ) \u2013 TotalChange from Order inner join ( select d . OrderID , sum ( Amount * Price ) as TotalChange from deleted d group by d . OrderID ) OrderChange on Order . ID = OrderChange . OrderID In this trigger, it is worth noting that while the event occurs in the OrderItem table, the content to be updated is in the Order table. This is fine, a trigger can read and write any part of the database, and all changes are executed in the same transaction. Furthermore, we do not recalculate the total amount in the trigger but alter it in response to the changes. Although this makes the trigger code more complex, it is more effective this way. Sequence of triggers We can define multiple triggers for an event. But the order of their execution cannot be specified. We can set the first and last triggers, but we cannot make assumptions regarding their sequence otherwise - it is considered ill-advised to design functionality where triggers need to build on each other.","title":"DML triggers"},{"location":"lecture-notes/mssql/server-side-programming/#instead-of-triggers","text":"A special type of trigger is the so-called instead of trigger . Such triggers can be defined for both tables and views. Let us look at using them on tables first. An instead of trigger defined on a table, as its name suggests, runs the instruction we define in the trigger instead of the actual operation's insert / update / delete . E.g., when inserting, the new rows are not added to the table, and when deleting, rows are not deleted. Instead, we can define in the trigger how to perform these operations. In the overridden process, we can access the table itself and execute the necessary actions in this table. These operations do not cause recursion in the trigger. These triggers can be considered as before triggers, i.e., we can perform checks before making the changes and abort the operation in case of an error. A typical use case for an instead of trigger is, for example, when we do not want to perform a deletion. This is also called soft delete :,instead of deleting, we only mark the records as deleted: -- Soft delete flag column in the table with a default value of 0 (i.e., false) alter table Product add [ IsDeleted ] bit NOT NULL CONSTRAINT DF_Product_IsDeleted DEFAULT 0 go -- Instead of trigger, the delete command does not perform the deletion -- the following code runs instead create or alter trigger ProductSoftDelete on Product instead of delete as update Product set IsDeleted = 1 where ID in ( select ID from deleted ) Another typical use case for instead of triggers is views. A view is the result of a query, so inserting new data into the view does not make sense. However, you can use an instead of trigger to define what to do instead of \"inserting into view.\" Let us look at an example. In the view below, we combine data from the product and VAT tables so that the VAT percentage is displayed in the view instead of the ID of the referenced VAT record. We can insert into this view by inserting the data into the product table instead: -- Define the view create view ProductWithVatPercentage as select p . Id , p . Name , p . Price , p . Stock , v . Percentage from Product p join Vat v is p . VATID = v . Id -- Instead of trigger for the view create or alter trigger ProductWithVatPercentageInsert on ProductWithVatPercentage instead of insert as -- The insertion goes into the Product table: a new row is created for each inserted record -- And we find the VAT record corresponding to the provided percentage -- The solution is not complete because it does not handle if there is no matching VAT record insert into Product ( Name , Price , Stock , VATID , CategoryID ) select i . Name , i . Price , i . Stock , v . ID , 1 from inserted i join VAT v on v . Percentage = i . Percentage -- The trigger can be tested by inserting data into the view insert into ProductWithVatPercentage ( Name , Price , Stock , Percentage ) values ( 'Red ball' , 1234 , 22 , 27 )","title":"Instead of triggers"},{"location":"lecture-notes/mssql/sql/","text":"SQL language, MSSQL platform-specific SQL \u00b6 You can run these queries on the sample database . Simple queries \u00b6 Which product costs less than 2000 and have less than 50 in stock? select Name , Price , Stock from Product where Price < 2000 and Stock < 50 Which product has no description? select * from Product where Description is null Joining tables \u00b6 Customers with a main site in Budapest (the two alternatives are equivalent). select * from Customer c , CustomerSite s where c . MainCustomerSiteID = s . ID and City = 'Budapest' select * from Customer c inner join CustomerSite s on c . MainCustomerSiteID = s . ID where City = 'Budapest' List the products that start with letter M, the ordered amounts and deadlines. Include the products that have not been ordered yet. select p . Name , sum ( oi . Amount ) from Product p left outer join OrderItem oi on p . id = oi . ProductID where p . Name like 'M%' group by p . Name [Order] [Order] is in brackets, because this signals that this is a table name and not the beginning of the order by SQL language element. Sorting \u00b6 select * from Product order by Name Microsoft SQL Server specific: collation specifies the rules for sorting select * from Product order by Name collate SQL_Latin1_General_Cp1_CI_AI Sort by multiple fields select * from Product order by Stock desc , Price Subqueries \u00b6 List the order statuses, deadlines and dates select o . Date , o . Deadline , s . Name from [ Order ] o inner join Status s on o . StatusId = s . ID An alternative, but the two are not equivalent: the subquery is the equivalent of the left outer join and not the innter join! select o . Date , o . Deadline , ( select s . Name from Status s where o . StatusId = s . ID ) from [ Order ] o Filter duplicates \u00b6 Which products have been ordered in batches of more than 3? One product may have been ordered multiple times, but we want the name only once. select distinct p . Name from Product p inner join OrderItem oi on oi . ProductID = p . ID where oi . Amount > 3 Aggregate functions \u00b6 How much is the most expensive product? select max ( Price ) from Product Which are the most expensive products? select * from Product where Price = ( select max ( Price ) from Product ) What was the min, max and average selling price of each product with name containing Lego having an average selling price more than 10000 select p . Id , p . Name , min ( oi . Price ), max ( oi . Price ), avg ( oi . Price ) from Product p inner join OrderItem oi on p . ID = oi . ProductID Where p . Name like '%Lego%' group by p . Id , p . Name having avg ( oi . Price ) > 10000 order by 2 Inserting records \u00b6 Inserting a single record by assigning value to all columns (except identity ) insert into Product values ( 'aa' , 100 , 0 , 3 , 2 , null ) Set values of selected columns only insert into Product ( Name , Price ) values ( 'aa' , 100 ) Insert the result of a query insert into Product ( Name , Price ) select Name , Price from InvoiceItem where Amount > 2 MSSQL specific: identity column create table VAT ( ID int identity primary key , Percentage int ) insert into VAT ( Percentage ) values ( 27 ) select @@ identity MSSQL specific: setting the value of identity column set identity_insert VAT on insert into VAT ( ID , Percentage ) values ( 123 , 27 ) set identity_insert VAT off Updating records \u00b6 Raise the price of LEGOs by 10% and add 5 to stock update Product set Price = 1 . 1 * Price , Stock = Stock + 5 where Name like '%Lego%' Update based on filtering by referenced table content: raise the price by 10% for those products that are subject to 20% VAT, and have more then 10 pcs in stock update Product set Price = 1 . 1 * Price where Stock > 10 and VATID in ( select ID from VAT where Percentage = 20 ) MSSQL Server specific solution to the same task update Product set Price = 1 . 1 * Price from Product p inner join VAT v on p . VATID = v . ID where Stock > 10 and Percentage = 20 Deleting records \u00b6 delete from Product where ID > 10 Assigning ranks \u00b6 Assigning ranks by ordering select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p Ranking within groups select p . * , rank () over ( partition by CategoryID order by Name ) as r , dense_rank () over ( partition by CategoryID order by Name ) as dr from Product p CTE (Common Table Expression) \u00b6 Motivation: subqueries often make queries complex First three products sorted by name alphabetically select * from ( select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p ) a where a . dr <= 3 Same solution using CTE with q1 as ( select * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product ) select * from q1 where q1 . dr <= 3 How many pieces have been sold from the second most expensive product? with q as ( select * , dense_rank () over ( order by Price desc ) dr from Product ) select q . ID , q . Name , sum ( Amount ) from q inner join OrderItem oi on oi . ProductID = q . ID where q . dr = 2 group by q . ID , q . Name Paging: list products alphabetically from 3. to 8. record with q as ( select * , rank () over ( order by Name ) r from Product ) select * from q where q . r between 3 and 8 Paging using MSSQL Server (2012+) specific syntax select * from Product order by Name offset 2 rows fetch next 6 rows only select top 3 * from Product order by Name Querying XML documents \u00b6 In a relational database, in addition to relational data, semi-structured data (e.g., XML) can also be stored - but relational is the main content. For example, in the sample database , the Description field of the Product table is XML. XPath \u00b6 An XML document has a tree structure. The XPath language allows navigating this tree and selecting specific content. The following table illustrates the capabilities of the XPath language. XPath expression Meaning tagname Node with specified name / Search starts from the root // In a descendend at any level . Current node .. Parent node @name Specific attribute /library/book[k] The book at index k within the library node (indexes start at 1) /library/book[last()] Last child /library/book[position()<k] The first k-1 child nodes //title[@lang=\"hu\"] Title elements that have lang attribute with value \"hu\" //title[text()] The text content of the title nodes /library/book[price>5000] Books within the library node that have a price more than 5000 XQuery and XPath XPath has many other capabilities in addition to the ones above, including expressing more complex queries. In the following examples, we will specify the data to be queried using XQuery . XQuery builds on XPath and adds additional functionality. Both XPath and XQuery are platform-independent languages \u200b\u200bbased on W3C standards. Queries \u00b6 Let us have a table with an XML column. In addition to querying the entire XML value, we can query content from within the XML document. In order to do this, we need to use T-SQL functions capable of working on the XML content: query(XQuery) , value(XQuery, SQLType) and exist(XQuery) . Let's look at a few examples of these. Let us query how many packages the products consist of. select Description . query ( '/product/package_parameters/number_of_packages' ) from Product For example, this could yield: <number_of_packages> 1 </number_of_packages> The function query() returns XML; if it is only the value that is needed, we can use the value() function. The value() function must also specify the type of data queried as a string literal. select Description . value ( '(/product/package_parameters/number_of_packages)[1]' , 'int' ) from Product The result will be 1. SQLType The type passed as a parameter cannot be xml. Conversion to the specified type is performed with the T-SQL CONVERT function. Let us query the names of the recommended products for ages 0-18 months. select Name from Product where Description . exist ( '(/product)[(./recommended_age)[1] eq \"0-18 m\"]' ) = 1 Function exist() returns 1 if the XQuery expression evaluation yields a non-empty result; or 0 if the query result is empty. We can also use the value() method instead of exist() here. select Name from Product where Description . value ( '(/product/recommended_age)[1]' , 'varchar(max)' ) = '0-18 m' Manipulating queries \u00b6 We can not only query XML data, but also modify it in place. The modification in the database is performed in an atomic way, i.e., there is no need to fetch the XML into a client application, modify it and then write it back. Instead, following the philosophy of server-side programming, we bring the logic (here: modification) into the database. Data modification queries can be performed with the modify(XML_DML) function, where we use the so-called XML DML language to describe the desired change. Let's look at a few examples. In the product called Lego City harbor, let us change the recommended age to 6-99 years. update Product set Description . modify ( 'replace value of (/product/recommended_age/text())[1] with \"6-99 y\"' ) where Name = 'Lego City harbour' The XML DML expression consists of two parts: in the first part ( replace value of ) the element to be modified is selected; in the second part ( with ) the new value is specified. Only one element can be modified within an XML, so the path must be specified to match only one element - thus the [1] at the end of the example. Let us insert a weigth tag into the XML description of product Lego City harbor after the package_size tag. update Product set Description . modify ( 'insert <weight>2.28</weight> after (/product/package_parameters/package_size)[1]' ) where Name = 'Lego City harbour' The expression has of two parts here too: the first one ( insert ) specifies the new element, and the second one describes where to insert the new element. The new item can be added as a sibling or child of the specified item. Let us remove the description tag(s) from the description of every product. update Product set Description . modify ( 'delete /product/description' ) where Description is not null When deleting, we specify the path of the items to be deleted after delete .","title":"SQL language, MSSQL platform-specific SQL"},{"location":"lecture-notes/mssql/sql/#sql-language-mssql-platform-specific-sql","text":"You can run these queries on the sample database .","title":"SQL language, MSSQL platform-specific SQL"},{"location":"lecture-notes/mssql/sql/#simple-queries","text":"Which product costs less than 2000 and have less than 50 in stock? select Name , Price , Stock from Product where Price < 2000 and Stock < 50 Which product has no description? select * from Product where Description is null","title":"Simple queries"},{"location":"lecture-notes/mssql/sql/#joining-tables","text":"Customers with a main site in Budapest (the two alternatives are equivalent). select * from Customer c , CustomerSite s where c . MainCustomerSiteID = s . ID and City = 'Budapest' select * from Customer c inner join CustomerSite s on c . MainCustomerSiteID = s . ID where City = 'Budapest' List the products that start with letter M, the ordered amounts and deadlines. Include the products that have not been ordered yet. select p . Name , sum ( oi . Amount ) from Product p left outer join OrderItem oi on p . id = oi . ProductID where p . Name like 'M%' group by p . Name [Order] [Order] is in brackets, because this signals that this is a table name and not the beginning of the order by SQL language element.","title":"Joining tables"},{"location":"lecture-notes/mssql/sql/#sorting","text":"select * from Product order by Name Microsoft SQL Server specific: collation specifies the rules for sorting select * from Product order by Name collate SQL_Latin1_General_Cp1_CI_AI Sort by multiple fields select * from Product order by Stock desc , Price","title":"Sorting"},{"location":"lecture-notes/mssql/sql/#subqueries","text":"List the order statuses, deadlines and dates select o . Date , o . Deadline , s . Name from [ Order ] o inner join Status s on o . StatusId = s . ID An alternative, but the two are not equivalent: the subquery is the equivalent of the left outer join and not the innter join! select o . Date , o . Deadline , ( select s . Name from Status s where o . StatusId = s . ID ) from [ Order ] o","title":"Subqueries"},{"location":"lecture-notes/mssql/sql/#filter-duplicates","text":"Which products have been ordered in batches of more than 3? One product may have been ordered multiple times, but we want the name only once. select distinct p . Name from Product p inner join OrderItem oi on oi . ProductID = p . ID where oi . Amount > 3","title":"Filter duplicates"},{"location":"lecture-notes/mssql/sql/#aggregate-functions","text":"How much is the most expensive product? select max ( Price ) from Product Which are the most expensive products? select * from Product where Price = ( select max ( Price ) from Product ) What was the min, max and average selling price of each product with name containing Lego having an average selling price more than 10000 select p . Id , p . Name , min ( oi . Price ), max ( oi . Price ), avg ( oi . Price ) from Product p inner join OrderItem oi on p . ID = oi . ProductID Where p . Name like '%Lego%' group by p . Id , p . Name having avg ( oi . Price ) > 10000 order by 2","title":"Aggregate functions"},{"location":"lecture-notes/mssql/sql/#inserting-records","text":"Inserting a single record by assigning value to all columns (except identity ) insert into Product values ( 'aa' , 100 , 0 , 3 , 2 , null ) Set values of selected columns only insert into Product ( Name , Price ) values ( 'aa' , 100 ) Insert the result of a query insert into Product ( Name , Price ) select Name , Price from InvoiceItem where Amount > 2 MSSQL specific: identity column create table VAT ( ID int identity primary key , Percentage int ) insert into VAT ( Percentage ) values ( 27 ) select @@ identity MSSQL specific: setting the value of identity column set identity_insert VAT on insert into VAT ( ID , Percentage ) values ( 123 , 27 ) set identity_insert VAT off","title":"Inserting records"},{"location":"lecture-notes/mssql/sql/#updating-records","text":"Raise the price of LEGOs by 10% and add 5 to stock update Product set Price = 1 . 1 * Price , Stock = Stock + 5 where Name like '%Lego%' Update based on filtering by referenced table content: raise the price by 10% for those products that are subject to 20% VAT, and have more then 10 pcs in stock update Product set Price = 1 . 1 * Price where Stock > 10 and VATID in ( select ID from VAT where Percentage = 20 ) MSSQL Server specific solution to the same task update Product set Price = 1 . 1 * Price from Product p inner join VAT v on p . VATID = v . ID where Stock > 10 and Percentage = 20","title":"Updating records"},{"location":"lecture-notes/mssql/sql/#deleting-records","text":"delete from Product where ID > 10","title":"Deleting records"},{"location":"lecture-notes/mssql/sql/#assigning-ranks","text":"Assigning ranks by ordering select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p Ranking within groups select p . * , rank () over ( partition by CategoryID order by Name ) as r , dense_rank () over ( partition by CategoryID order by Name ) as dr from Product p","title":"Assigning ranks"},{"location":"lecture-notes/mssql/sql/#cte-common-table-expression","text":"Motivation: subqueries often make queries complex First three products sorted by name alphabetically select * from ( select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p ) a where a . dr <= 3 Same solution using CTE with q1 as ( select * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product ) select * from q1 where q1 . dr <= 3 How many pieces have been sold from the second most expensive product? with q as ( select * , dense_rank () over ( order by Price desc ) dr from Product ) select q . ID , q . Name , sum ( Amount ) from q inner join OrderItem oi on oi . ProductID = q . ID where q . dr = 2 group by q . ID , q . Name Paging: list products alphabetically from 3. to 8. record with q as ( select * , rank () over ( order by Name ) r from Product ) select * from q where q . r between 3 and 8 Paging using MSSQL Server (2012+) specific syntax select * from Product order by Name offset 2 rows fetch next 6 rows only select top 3 * from Product order by Name","title":"CTE (Common Table Expression)"},{"location":"lecture-notes/mssql/sql/#querying-xml-documents","text":"In a relational database, in addition to relational data, semi-structured data (e.g., XML) can also be stored - but relational is the main content. For example, in the sample database , the Description field of the Product table is XML.","title":"Querying XML documents"},{"location":"lecture-notes/mssql/sql/#xpath","text":"An XML document has a tree structure. The XPath language allows navigating this tree and selecting specific content. The following table illustrates the capabilities of the XPath language. XPath expression Meaning tagname Node with specified name / Search starts from the root // In a descendend at any level . Current node .. Parent node @name Specific attribute /library/book[k] The book at index k within the library node (indexes start at 1) /library/book[last()] Last child /library/book[position()<k] The first k-1 child nodes //title[@lang=\"hu\"] Title elements that have lang attribute with value \"hu\" //title[text()] The text content of the title nodes /library/book[price>5000] Books within the library node that have a price more than 5000 XQuery and XPath XPath has many other capabilities in addition to the ones above, including expressing more complex queries. In the following examples, we will specify the data to be queried using XQuery . XQuery builds on XPath and adds additional functionality. Both XPath and XQuery are platform-independent languages \u200b\u200bbased on W3C standards.","title":"XPath"},{"location":"lecture-notes/mssql/sql/#queries","text":"Let us have a table with an XML column. In addition to querying the entire XML value, we can query content from within the XML document. In order to do this, we need to use T-SQL functions capable of working on the XML content: query(XQuery) , value(XQuery, SQLType) and exist(XQuery) . Let's look at a few examples of these. Let us query how many packages the products consist of. select Description . query ( '/product/package_parameters/number_of_packages' ) from Product For example, this could yield: <number_of_packages> 1 </number_of_packages> The function query() returns XML; if it is only the value that is needed, we can use the value() function. The value() function must also specify the type of data queried as a string literal. select Description . value ( '(/product/package_parameters/number_of_packages)[1]' , 'int' ) from Product The result will be 1. SQLType The type passed as a parameter cannot be xml. Conversion to the specified type is performed with the T-SQL CONVERT function. Let us query the names of the recommended products for ages 0-18 months. select Name from Product where Description . exist ( '(/product)[(./recommended_age)[1] eq \"0-18 m\"]' ) = 1 Function exist() returns 1 if the XQuery expression evaluation yields a non-empty result; or 0 if the query result is empty. We can also use the value() method instead of exist() here. select Name from Product where Description . value ( '(/product/recommended_age)[1]' , 'varchar(max)' ) = '0-18 m'","title":"Queries"},{"location":"lecture-notes/mssql/sql/#manipulating-queries","text":"We can not only query XML data, but also modify it in place. The modification in the database is performed in an atomic way, i.e., there is no need to fetch the XML into a client application, modify it and then write it back. Instead, following the philosophy of server-side programming, we bring the logic (here: modification) into the database. Data modification queries can be performed with the modify(XML_DML) function, where we use the so-called XML DML language to describe the desired change. Let's look at a few examples. In the product called Lego City harbor, let us change the recommended age to 6-99 years. update Product set Description . modify ( 'replace value of (/product/recommended_age/text())[1] with \"6-99 y\"' ) where Name = 'Lego City harbour' The XML DML expression consists of two parts: in the first part ( replace value of ) the element to be modified is selected; in the second part ( with ) the new value is specified. Only one element can be modified within an XML, so the path must be specified to match only one element - thus the [1] at the end of the example. Let us insert a weigth tag into the XML description of product Lego City harbor after the package_size tag. update Product set Description . modify ( 'insert <weight>2.28</weight> after (/product/package_parameters/package_size)[1]' ) where Name = 'Lego City harbour' The expression has of two parts here too: the first one ( insert ) specifies the new element, and the second one describes where to insert the new element. The new item can be added as a sibling or child of the specified item. Let us remove the description tag(s) from the description of every product. update Product set Description . modify ( 'delete /product/description' ) where Description is not null When deleting, we specify the path of the items to be deleted after delete .","title":"Manipulating queries"},{"location":"lecture-notes/transactions/","text":"Transactions in databases \u00b6 Context When we talk about transactions, we mean relational databases . The problem and the solutions, however, are generic and are not specific to relational databases. Concurrent data access \u00b6 Database management systems are based on a client-server architecture. The client (the software we write) connects to the database and executes queries. We should always remember that there is a single database, but multiple clients involved here. The purpose of the database system is to serve as many requests as possible; consequently, it executes the queries concurrently . In such a concurrent system, data access can overlap in the following ways. If the concurrent data access (either read or write) concerns independent data, there is no problem, and the operations may proceed concurrently. If all operations only read data, there is no issue either; multiple readers can access the same data. However, if the same data is accessed simultaneously and there is at least one writer , a concurrency problem may manifest itself. This concurrency issue is analogous to the mutual exclusion problem known in operating systems and the various programming languages and frameworks. Concurrent data access in these scenarios usually involve mutual access to shared memory space, and the solution is ensuring mutual exclusion using some kind of guard. In database management systems, concurrency is related to the records (rows) of database tables, and the guards are transactions. Transactions \u00b6 Definition A transaction is a logical unit of a process, a series of operations that only make sense together. A transaction combines operations into one unit, and the system guarantees the following properties: atomic execution, consistency, isolation from each other, and durability. Let us examine these basic properties to understand how concurrent data access issues are resolved with their help. A transaction is just a tool A transaction, similarly to mutexes provided by an operating system or programming framework, is just a tool provided to the software developer. The proper usage is the responsibility of the developer. Transactions basic properties \u00b6 Atomicity \u00b6 Atomic execution means that we have a sequence of operations, and this sequence is meaningful only when all of it is executed. In other words, partial execution must be prohibited. In database systems, we often need multiple statements to achieve our goal, hence the sequence of steps. Let us imaging the checkout process in a webshop: The order is recorded in the database with the provided data The amount of stock is decreased by one since one piece was sold These steps only make sense together. Given that an order has been recorded, the amount of stock must be compensated; otherwise, the data becomes invalid, and we sell more products than we have. Thus, we must not abort the sequence of steps in the middle. This is what atomicity guarantees: if executing a sequence of steps has begun, all steps have to complete successfully or the initial state before the modification must be restored . Consistency \u00b6 The database's consistency rules are described by the integrity requirements, such as the record referenced by a foreign key must exist. There are other types of consistency requirements; e.g., there cannot be more students registered for an exam than the limit in the Neptun system. Transactions ensure that our database is always in a consistent state. While a transaction is in progress, temporary inconsistencies may arise, similarly to the interim state between the two steps of the sequence of the operation above. However, at the end of the transaction, consistency must be restored. In other words: transactions enforce transition between consistent states . Durability \u00b6 Durability prescribes that the effect of a transaction is durable , that is, the results are not lost. Practically it means that the modifications performed by a transaction must be flushed to persistent storage (i.e., disk). There are two types of errors in database systems that can lead to data corruption: soft crash and hard crash. Soft crash means the database process terminates, and the content of memory is lost. Transactions offer protection from these kinds of crashes. A hard crash means that the disk is also affected. Only a backup can provide protection here. Isolation \u00b6 By isolation, we mean to isolate the effect of transactions from each other. That is, when writing our query, we do not need to concern ourselves with other concurrent transactions; the system will handle this aspect. The developer can write queries as if they were executed in the system alone, and the system will guarantee that it will prohibit those concurrency issues that we do not want to deal with . The system will still run transactions concurrently. However, it guarantees to schedule the transactions to not violate the rules of the isolation level requested by the transaction. Therefore, all transactions need to specify the requested isolation level . Isolation problems and isolation levels \u00b6 Before we can discuss the isolation levels, we need to first understand the types of problems that concurrency can cause. Problems \u00b6 Dirty read \u00b6 A dirty read means that a transaction accesses the uncommitted data of another transaction: A transaction modifies a record in the database but does not commit yet. Another transaction reads the same record (in its changed state). The first transaction is aborted, and the system restores the record to the state it was in before the change. The transaction that read the record in the second step is now working with invalid, non-existent data. It should not have read it. Source Source of images: https://vladmihalcea.com/2014/01/05/a-beginners-guide-to-acid-and-database-transactions/ Dirty read should almost always be avoided. Lost update \u00b6 During a lost update, two writes conflict: A transaction changes a record. Another transaction overwrites the same record. The database has the result of the second write as if the first did not even happen. Non-repeatable read \u00b6 A non-repeatable read means that the result of the query depends on the time it was issued: A transaction queries a record. A different transaction changes the same record. If the first transaction re-executes the same query as before, it gets a different result. Phantom records / phantom read \u00b6 We face the problem of phantom records when we work with recordsets: A transaction executes a query that yields multiple records as a result. Meanwhile, a different transaction deletes a record that is included in the previous result set. The first transaction starts processing its result set (e.g., iterates over the records one by one). Should the deleted record be processed now? We can imagine a similar scenario when a record is altered in the second step. Which state should the reader transaction in step three see? The one before, or the one after the modification? Isolation levels \u00b6 The problems discussed before can be avoided by using the right isolation level. We should consider, though, that the \"higher\" level of isolation we prescribe, the lower the throughput of the database system will be. Also, we might face deadlocks (see below). Our goal, thus, is a compromise between a suitable isolation level and performance. The ANSI/ISO SQL standard defines the following isolation levels: Read uncommitted: offers no protection. Read committed: no dirty read. Repeatable read: no dirty read and no non-repeatable read. Serializable: prohibits all issues. Read uncommitted is seldom used. Serializable , similarly, is avoided if possible. The default, usually, is read committed . Scheduling enforced with locks \u00b6 The database enforces isolation through locks: when a record is accessed (read or write), it is locked by the system. The lock is placed on the record when it is first accessed and is removed at the end of the transaction. The type of lock (e.g., shared lock or mutually exclusive) depends on the isolation level and the implementation of the database management system. These locks, in effect, enforce the scheduling of the transactions. When a lock is not available, because the record it used by another translation and concurrent access is not allowed by the isolation level, the transaction will wait. We know that when we use locks, deadlock can occur. This is no different in databases. A deadlock may occur when two transactions are competing for the same locks. See the figure below; a continuous line represents an owned lock, while the dashed ones represent a lock the transaction would like to acquire. Neither of these requests can be fulfilled, resulting in both transactions being unable to move forward. Deadlocks cannot be prevented in database management systems, but they can be recognized and dealt with. The system monitors locks, and when a deadlock is detected one of the transactions is aborted and all its modifications are rolled back. All applications using a database must be prepared to handle this. When a deadlock happens, there is usually no other resolution to retry the operation later (e.g., automatically, or manually requested by the end-user). Transaction boundaries \u00b6 A transaction combines a sequence of steps. It is, therefore, necessary to mark the beginning and the end of the transaction. The way transaction boundaries are signaled may depend on the platform, but generally: All operations are executed within the scope of a transaction. If the transaction boundary is not marked explicitly, each statement is a transaction in itself. Since all SQL statements run within a transaction scope, the transaction properties are automatically guaranteed for all statements. For example, a delete statement affecting multiple records cannot abort and delete only half of the records. The developer executes a begin transaction SQL statement to start a transaction, and completes it either with commit or rollback . Commit completes the translation and saves its changes, while rollback aborts the transaction and undoes its changes. Some database management systems enable nested transactions too. Completing transactions follow the nesting: each level needs to be committed. Transaction logging \u00b6 So far, we have covered what transactions are used for. Let us understand how they work internally. Transactional logging is the process used by the database management system to track the pending modifications of running transactions allowing rolling back these changes in case of abort or soft crash. To understand transactional logging, let us consider the following system model. This conceptual model includes the following operations: Begin T(x): Start of transaction Input(A): Read data from the durable database store (disk) Output(A): Write data to durable database store (disk) Read(A): Transaction reads the data from the memory buffer Write(A): Transaction writes the data to the memory buffer FLUSH_LOG: Write the transaction log to disk The process of transactional logging is demonstrated in the following example. In this example, a transaction modifies two data elements: A is decreases by 2, and B is increased by 2. Undo transaction log \u00b6 We begin with an empty memory buffer. Every data is on disk. The process starts by reading the data from disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Begin(T1) 10 20 - - Begin T1 Input(A) 10 20 10 - Input(B) 10 20 10 20 The transaction has all the necessary data in the memory buffer. The modification is performed, and the data is written back to the buffer. At the same time, the original values and written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20 The transaction completes, and it saves the changes. The transaction commits, which first flushes the transaction log to disk, then the changes are persisted to disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Output(B) 8 22 8 22 Commit T1 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. There is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, some data could already be written to disk. These need to be reverted. The transaction log is processed starting from the end, and for all transactions that have no commit mark in the log, the values must be restored to their original state. To summarize, when using undo logging: the database cannot be modified until the transaction log is flushed, and the commit mark must be placed into the log once the database writes are finished. The key is to flush the transaction log before the changes are persisted. The drawback of this method is that the transaction log is flushed twice, which is a performance issue due to the cost of disk access. Redo transaction log \u00b6 The process starts with reading the data from disk, followed by performing the modifications, but this time the final values are written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 22 To finalize the transaction, the log is flushed first to register the modified values - but no modification is made to the database files yet. Thus, the transaction log needs to be written to disk only once (compared to the undo logging scheme). Operation A (database) B (database) A (buffer) B (buffer) Transactional log Commit T1 Flush_LOG 10 20 8 22 After the transaction log is persisted, the changes are committed to the database files. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Output(A) 8 20 8 22 Output(B) 8 22 8 22 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. In that case, there is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, the commit mark is flushed to the log, but no changes were made to the database yet. Restoring from an aborted state at this stage is performed by processing the transaction log from the beginning and redoing all committed transactions. To summarize, when using redo logging: the database cannot be modified until the transaction log is flushed, commit mark must be placed into the transaction log before writing the database files. There are fewer transaction log flushes in this scheme compared to undo logging; however, the restore procedure is longer. Undo/redo logging \u00b6 As the name suggests, this is the combination of the two schemes. The process starts just like in the previous cases. The difference is in writing the transaction log: both the original and the modified values are written to the log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20, 22 The commit procedure is simpler. The order of writing the database files and writing the commit mark into the transaction log is no longer fixed - however, flushing the transaction log must still be performed first. The simplification, therefore, is that the place of the commit mark is not fixed. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Commit T1 Output(B) 8 22 8 22 Restore needs to combine the procedures discussed before: committed transactions are replayed (just like in redo logging), while aborted transactions are reverted (just like in undo logging). This solution has the following advantages: there is less synchronization during the commit procedure (with regards to writing the transaction log and the database files), the changes can be persisted in the database files sooner (no need to wait for writing the commit mark). Reducing the transaction log \u00b6 The transaction log needs to be emptied periodically. Transactions that are committed and persisted into the database files can be purged from the log. Similarly, aborted transactions that were reverted can also be removed. This is performed automatically by the system, but can also be triggered manually. Long-running transactions can significantly increase the size of the log. The larger the log is, the longer the purging process will take. Extracting deadlock information from MSSQL database \u00b6 Deadlock A deadlock in a system can occur if there are locks. A deadlock can occur if at least two transactions want to obtain the same locks simultaneously. Let there be transactions A and B and resources a and b . Transaction A already locks resource a , while transaction B locks resource b . Then, let us assume that transaction A wants to lock resource b and transaction B also wants to lock resource a . In this case, a deadlock will occur. Let us take the previous example and see how we can diagnose the deadlock once it occurs in MSSQL. To do this, first, we need to cause the deadlock artificially. Let us create two tables on which to generate the deadlock artificially. Create the first table called Lefty , which will have an attribute called Numbers : CREATE TABLE dbo . Lefty ( Numbers INT PRIMARY KEY CLUSTERED ); INSERT INTO dbo . Lefty VALUES ( 1 ), ( 2 ), ( 3 ); Create a second table called Righty , which will also have an attribute, Numbers : CREATE TABLE dbo . Righty ( Numbers INT PRIMARY KEY CLUSTERED ); INSERT INTO dbo . Righty VALUES ( 1 ), ( 2 ), ( 3 ); The two transactions must run simultaneously for a deadlock to occur. If we test manually, this is difficult to achieve, so the order of execution is: Execute the first UPDATE statement from the first transaction From the second transaction, executed both UPDATE statements Execute the second UPDATE statement from the first transaction First transaction: BEGIN TRAN UPDATE dbo . Lefty SET Numbers = Numbers * 2 ; GO UPDATE dbo . Righty SET Numbers = Numbers * 2 ; GO Second transaction: BEGIN TRAN UPDATE dbo . Righty SET Numbers = Numbers + 1 ; GO UPDATE dbo . Lefty SET Numbers = Numbers + 1 ; GO Now we have a deadlock. The system will automatically resolve this soon. Before that happens, we can check what we see in the system. The locks placed by the transactions can be queried in the database with the following query: SELECT OBJECT_NAME ( P . object_id ) AS TableName , Resource_type , request_status , request_session_id FROM sys . dm_tran_locks dtl join sys . partitions P ON dtl . resource_associated_entity_id = p . hobt_id In our example, the result of this query is: TableName Resource_type request_status request_session_id 1 Righty KEY GRANT 54 2 Lefty KEY GRANT 53 So the first transaction placed a lock on the Lefty table, while the second transaction placed it on table Righty . The database also provides information data about blocked transactions that we can query with the following SQL statement: SELECT blocking_session_id AS BlockingSessionID , session_id AS VictimSessionID , wait_time / 1000 AS WaitDurationSecond FROM sys . dm_exec_requests CROSS APPLY sys . dm_exec_sql_text ([ sql_handle ]) WHERE blocking_session_id > 0 In our example, the result of this query is: BlockingSessionID VictimSessionID WaitDurationSecond 1 54 53 0 2 53 54 72 This means that the transaction with ID 53 waits for the transaction with ID 54, and the transaction with ID 54 waits for the transaction with ID 53. The deadlock is soon eliminated automatically by the database. If we want to intervene manually, we can do so with the kill command, selecting the transaction to stop (e.g., kill 53 ). Questions to test your knowledge \u00b6 What type of concurrent data access problems do you know? List the isolation levels. Which problems does each of the levels prohibit? What are the basic properties of transactions? Decide whether the following statements are true or false: The serializable isolation level executes the transactions one after the other. Deadlock can be prevented by using the right isolation level. The default isolation level is usually read committed . If we are not using explicit transactions, then we are protected from the issue of dirty read. The transaction log offers protection against all kinds of data losses. In the redo transaction logging scheme, the transaction log starts with the commit mark.","title":"Transactions in databases"},{"location":"lecture-notes/transactions/#transactions-in-databases","text":"Context When we talk about transactions, we mean relational databases . The problem and the solutions, however, are generic and are not specific to relational databases.","title":"Transactions in databases"},{"location":"lecture-notes/transactions/#concurrent-data-access","text":"Database management systems are based on a client-server architecture. The client (the software we write) connects to the database and executes queries. We should always remember that there is a single database, but multiple clients involved here. The purpose of the database system is to serve as many requests as possible; consequently, it executes the queries concurrently . In such a concurrent system, data access can overlap in the following ways. If the concurrent data access (either read or write) concerns independent data, there is no problem, and the operations may proceed concurrently. If all operations only read data, there is no issue either; multiple readers can access the same data. However, if the same data is accessed simultaneously and there is at least one writer , a concurrency problem may manifest itself. This concurrency issue is analogous to the mutual exclusion problem known in operating systems and the various programming languages and frameworks. Concurrent data access in these scenarios usually involve mutual access to shared memory space, and the solution is ensuring mutual exclusion using some kind of guard. In database management systems, concurrency is related to the records (rows) of database tables, and the guards are transactions.","title":"Concurrent data access"},{"location":"lecture-notes/transactions/#transactions","text":"Definition A transaction is a logical unit of a process, a series of operations that only make sense together. A transaction combines operations into one unit, and the system guarantees the following properties: atomic execution, consistency, isolation from each other, and durability. Let us examine these basic properties to understand how concurrent data access issues are resolved with their help. A transaction is just a tool A transaction, similarly to mutexes provided by an operating system or programming framework, is just a tool provided to the software developer. The proper usage is the responsibility of the developer.","title":"Transactions"},{"location":"lecture-notes/transactions/#transactions-basic-properties","text":"","title":"Transactions basic properties"},{"location":"lecture-notes/transactions/#atomicity","text":"Atomic execution means that we have a sequence of operations, and this sequence is meaningful only when all of it is executed. In other words, partial execution must be prohibited. In database systems, we often need multiple statements to achieve our goal, hence the sequence of steps. Let us imaging the checkout process in a webshop: The order is recorded in the database with the provided data The amount of stock is decreased by one since one piece was sold These steps only make sense together. Given that an order has been recorded, the amount of stock must be compensated; otherwise, the data becomes invalid, and we sell more products than we have. Thus, we must not abort the sequence of steps in the middle. This is what atomicity guarantees: if executing a sequence of steps has begun, all steps have to complete successfully or the initial state before the modification must be restored .","title":"Atomicity"},{"location":"lecture-notes/transactions/#consistency","text":"The database's consistency rules are described by the integrity requirements, such as the record referenced by a foreign key must exist. There are other types of consistency requirements; e.g., there cannot be more students registered for an exam than the limit in the Neptun system. Transactions ensure that our database is always in a consistent state. While a transaction is in progress, temporary inconsistencies may arise, similarly to the interim state between the two steps of the sequence of the operation above. However, at the end of the transaction, consistency must be restored. In other words: transactions enforce transition between consistent states .","title":"Consistency"},{"location":"lecture-notes/transactions/#durability","text":"Durability prescribes that the effect of a transaction is durable , that is, the results are not lost. Practically it means that the modifications performed by a transaction must be flushed to persistent storage (i.e., disk). There are two types of errors in database systems that can lead to data corruption: soft crash and hard crash. Soft crash means the database process terminates, and the content of memory is lost. Transactions offer protection from these kinds of crashes. A hard crash means that the disk is also affected. Only a backup can provide protection here.","title":"Durability"},{"location":"lecture-notes/transactions/#isolation","text":"By isolation, we mean to isolate the effect of transactions from each other. That is, when writing our query, we do not need to concern ourselves with other concurrent transactions; the system will handle this aspect. The developer can write queries as if they were executed in the system alone, and the system will guarantee that it will prohibit those concurrency issues that we do not want to deal with . The system will still run transactions concurrently. However, it guarantees to schedule the transactions to not violate the rules of the isolation level requested by the transaction. Therefore, all transactions need to specify the requested isolation level .","title":"Isolation"},{"location":"lecture-notes/transactions/#isolation-problems-and-isolation-levels","text":"Before we can discuss the isolation levels, we need to first understand the types of problems that concurrency can cause.","title":"Isolation problems and isolation levels"},{"location":"lecture-notes/transactions/#problems","text":"","title":"Problems"},{"location":"lecture-notes/transactions/#dirty-read","text":"A dirty read means that a transaction accesses the uncommitted data of another transaction: A transaction modifies a record in the database but does not commit yet. Another transaction reads the same record (in its changed state). The first transaction is aborted, and the system restores the record to the state it was in before the change. The transaction that read the record in the second step is now working with invalid, non-existent data. It should not have read it. Source Source of images: https://vladmihalcea.com/2014/01/05/a-beginners-guide-to-acid-and-database-transactions/ Dirty read should almost always be avoided.","title":"Dirty read"},{"location":"lecture-notes/transactions/#lost-update","text":"During a lost update, two writes conflict: A transaction changes a record. Another transaction overwrites the same record. The database has the result of the second write as if the first did not even happen.","title":"Lost update"},{"location":"lecture-notes/transactions/#non-repeatable-read","text":"A non-repeatable read means that the result of the query depends on the time it was issued: A transaction queries a record. A different transaction changes the same record. If the first transaction re-executes the same query as before, it gets a different result.","title":"Non-repeatable read"},{"location":"lecture-notes/transactions/#phantom-records-phantom-read","text":"We face the problem of phantom records when we work with recordsets: A transaction executes a query that yields multiple records as a result. Meanwhile, a different transaction deletes a record that is included in the previous result set. The first transaction starts processing its result set (e.g., iterates over the records one by one). Should the deleted record be processed now? We can imagine a similar scenario when a record is altered in the second step. Which state should the reader transaction in step three see? The one before, or the one after the modification?","title":"Phantom records / phantom read"},{"location":"lecture-notes/transactions/#isolation-levels","text":"The problems discussed before can be avoided by using the right isolation level. We should consider, though, that the \"higher\" level of isolation we prescribe, the lower the throughput of the database system will be. Also, we might face deadlocks (see below). Our goal, thus, is a compromise between a suitable isolation level and performance. The ANSI/ISO SQL standard defines the following isolation levels: Read uncommitted: offers no protection. Read committed: no dirty read. Repeatable read: no dirty read and no non-repeatable read. Serializable: prohibits all issues. Read uncommitted is seldom used. Serializable , similarly, is avoided if possible. The default, usually, is read committed .","title":"Isolation levels"},{"location":"lecture-notes/transactions/#scheduling-enforced-with-locks","text":"The database enforces isolation through locks: when a record is accessed (read or write), it is locked by the system. The lock is placed on the record when it is first accessed and is removed at the end of the transaction. The type of lock (e.g., shared lock or mutually exclusive) depends on the isolation level and the implementation of the database management system. These locks, in effect, enforce the scheduling of the transactions. When a lock is not available, because the record it used by another translation and concurrent access is not allowed by the isolation level, the transaction will wait. We know that when we use locks, deadlock can occur. This is no different in databases. A deadlock may occur when two transactions are competing for the same locks. See the figure below; a continuous line represents an owned lock, while the dashed ones represent a lock the transaction would like to acquire. Neither of these requests can be fulfilled, resulting in both transactions being unable to move forward. Deadlocks cannot be prevented in database management systems, but they can be recognized and dealt with. The system monitors locks, and when a deadlock is detected one of the transactions is aborted and all its modifications are rolled back. All applications using a database must be prepared to handle this. When a deadlock happens, there is usually no other resolution to retry the operation later (e.g., automatically, or manually requested by the end-user).","title":"Scheduling enforced with locks"},{"location":"lecture-notes/transactions/#transaction-boundaries","text":"A transaction combines a sequence of steps. It is, therefore, necessary to mark the beginning and the end of the transaction. The way transaction boundaries are signaled may depend on the platform, but generally: All operations are executed within the scope of a transaction. If the transaction boundary is not marked explicitly, each statement is a transaction in itself. Since all SQL statements run within a transaction scope, the transaction properties are automatically guaranteed for all statements. For example, a delete statement affecting multiple records cannot abort and delete only half of the records. The developer executes a begin transaction SQL statement to start a transaction, and completes it either with commit or rollback . Commit completes the translation and saves its changes, while rollback aborts the transaction and undoes its changes. Some database management systems enable nested transactions too. Completing transactions follow the nesting: each level needs to be committed.","title":"Transaction boundaries"},{"location":"lecture-notes/transactions/#transaction-logging","text":"So far, we have covered what transactions are used for. Let us understand how they work internally. Transactional logging is the process used by the database management system to track the pending modifications of running transactions allowing rolling back these changes in case of abort or soft crash. To understand transactional logging, let us consider the following system model. This conceptual model includes the following operations: Begin T(x): Start of transaction Input(A): Read data from the durable database store (disk) Output(A): Write data to durable database store (disk) Read(A): Transaction reads the data from the memory buffer Write(A): Transaction writes the data to the memory buffer FLUSH_LOG: Write the transaction log to disk The process of transactional logging is demonstrated in the following example. In this example, a transaction modifies two data elements: A is decreases by 2, and B is increased by 2.","title":"Transaction logging"},{"location":"lecture-notes/transactions/#undo-transaction-log","text":"We begin with an empty memory buffer. Every data is on disk. The process starts by reading the data from disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Begin(T1) 10 20 - - Begin T1 Input(A) 10 20 10 - Input(B) 10 20 10 20 The transaction has all the necessary data in the memory buffer. The modification is performed, and the data is written back to the buffer. At the same time, the original values and written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20 The transaction completes, and it saves the changes. The transaction commits, which first flushes the transaction log to disk, then the changes are persisted to disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Output(B) 8 22 8 22 Commit T1 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. There is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, some data could already be written to disk. These need to be reverted. The transaction log is processed starting from the end, and for all transactions that have no commit mark in the log, the values must be restored to their original state. To summarize, when using undo logging: the database cannot be modified until the transaction log is flushed, and the commit mark must be placed into the log once the database writes are finished. The key is to flush the transaction log before the changes are persisted. The drawback of this method is that the transaction log is flushed twice, which is a performance issue due to the cost of disk access.","title":"Undo transaction log"},{"location":"lecture-notes/transactions/#redo-transaction-log","text":"The process starts with reading the data from disk, followed by performing the modifications, but this time the final values are written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 22 To finalize the transaction, the log is flushed first to register the modified values - but no modification is made to the database files yet. Thus, the transaction log needs to be written to disk only once (compared to the undo logging scheme). Operation A (database) B (database) A (buffer) B (buffer) Transactional log Commit T1 Flush_LOG 10 20 8 22 After the transaction log is persisted, the changes are committed to the database files. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Output(A) 8 20 8 22 Output(B) 8 22 8 22 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. In that case, there is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, the commit mark is flushed to the log, but no changes were made to the database yet. Restoring from an aborted state at this stage is performed by processing the transaction log from the beginning and redoing all committed transactions. To summarize, when using redo logging: the database cannot be modified until the transaction log is flushed, commit mark must be placed into the transaction log before writing the database files. There are fewer transaction log flushes in this scheme compared to undo logging; however, the restore procedure is longer.","title":"Redo transaction log"},{"location":"lecture-notes/transactions/#undoredo-logging","text":"As the name suggests, this is the combination of the two schemes. The process starts just like in the previous cases. The difference is in writing the transaction log: both the original and the modified values are written to the log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20, 22 The commit procedure is simpler. The order of writing the database files and writing the commit mark into the transaction log is no longer fixed - however, flushing the transaction log must still be performed first. The simplification, therefore, is that the place of the commit mark is not fixed. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Commit T1 Output(B) 8 22 8 22 Restore needs to combine the procedures discussed before: committed transactions are replayed (just like in redo logging), while aborted transactions are reverted (just like in undo logging). This solution has the following advantages: there is less synchronization during the commit procedure (with regards to writing the transaction log and the database files), the changes can be persisted in the database files sooner (no need to wait for writing the commit mark).","title":"Undo/redo logging"},{"location":"lecture-notes/transactions/#reducing-the-transaction-log","text":"The transaction log needs to be emptied periodically. Transactions that are committed and persisted into the database files can be purged from the log. Similarly, aborted transactions that were reverted can also be removed. This is performed automatically by the system, but can also be triggered manually. Long-running transactions can significantly increase the size of the log. The larger the log is, the longer the purging process will take.","title":"Reducing the transaction log"},{"location":"lecture-notes/transactions/#extracting-deadlock-information-from-mssql-database","text":"Deadlock A deadlock in a system can occur if there are locks. A deadlock can occur if at least two transactions want to obtain the same locks simultaneously. Let there be transactions A and B and resources a and b . Transaction A already locks resource a , while transaction B locks resource b . Then, let us assume that transaction A wants to lock resource b and transaction B also wants to lock resource a . In this case, a deadlock will occur. Let us take the previous example and see how we can diagnose the deadlock once it occurs in MSSQL. To do this, first, we need to cause the deadlock artificially. Let us create two tables on which to generate the deadlock artificially. Create the first table called Lefty , which will have an attribute called Numbers : CREATE TABLE dbo . Lefty ( Numbers INT PRIMARY KEY CLUSTERED ); INSERT INTO dbo . Lefty VALUES ( 1 ), ( 2 ), ( 3 ); Create a second table called Righty , which will also have an attribute, Numbers : CREATE TABLE dbo . Righty ( Numbers INT PRIMARY KEY CLUSTERED ); INSERT INTO dbo . Righty VALUES ( 1 ), ( 2 ), ( 3 ); The two transactions must run simultaneously for a deadlock to occur. If we test manually, this is difficult to achieve, so the order of execution is: Execute the first UPDATE statement from the first transaction From the second transaction, executed both UPDATE statements Execute the second UPDATE statement from the first transaction First transaction: BEGIN TRAN UPDATE dbo . Lefty SET Numbers = Numbers * 2 ; GO UPDATE dbo . Righty SET Numbers = Numbers * 2 ; GO Second transaction: BEGIN TRAN UPDATE dbo . Righty SET Numbers = Numbers + 1 ; GO UPDATE dbo . Lefty SET Numbers = Numbers + 1 ; GO Now we have a deadlock. The system will automatically resolve this soon. Before that happens, we can check what we see in the system. The locks placed by the transactions can be queried in the database with the following query: SELECT OBJECT_NAME ( P . object_id ) AS TableName , Resource_type , request_status , request_session_id FROM sys . dm_tran_locks dtl join sys . partitions P ON dtl . resource_associated_entity_id = p . hobt_id In our example, the result of this query is: TableName Resource_type request_status request_session_id 1 Righty KEY GRANT 54 2 Lefty KEY GRANT 53 So the first transaction placed a lock on the Lefty table, while the second transaction placed it on table Righty . The database also provides information data about blocked transactions that we can query with the following SQL statement: SELECT blocking_session_id AS BlockingSessionID , session_id AS VictimSessionID , wait_time / 1000 AS WaitDurationSecond FROM sys . dm_exec_requests CROSS APPLY sys . dm_exec_sql_text ([ sql_handle ]) WHERE blocking_session_id > 0 In our example, the result of this query is: BlockingSessionID VictimSessionID WaitDurationSecond 1 54 53 0 2 53 54 72 This means that the transaction with ID 53 waits for the transaction with ID 54, and the transaction with ID 54 waits for the transaction with ID 53. The deadlock is soon eliminated automatically by the database. If we want to intervene manually, we can do so with the kill command, selecting the transaction to stop (e.g., kill 53 ).","title":"Extracting deadlock information from MSSQL database"},{"location":"lecture-notes/transactions/#questions-to-test-your-knowledge","text":"What type of concurrent data access problems do you know? List the isolation levels. Which problems does each of the levels prohibit? What are the basic properties of transactions? Decide whether the following statements are true or false: The serializable isolation level executes the transactions one after the other. Deadlock can be prevented by using the right isolation level. The default isolation level is usually read committed . If we are not using explicit transactions, then we are protected from the issue of dirty read. The transaction log offers protection against all kinds of data losses. In the redo transaction logging scheme, the transaction log starts with the commit mark.","title":"Questions to test your knowledge"},{"location":"seminar/ef/","text":"Entity Framework \u00b6 The goal of the seminar is to practice writing Linq queries and working with Entity Framework. Entity Framework Core In this seminar, we are using .NET 6 (former .NET Core) available as a cross-platform .NET version for Windows, Linux and Mac. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2022 Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: C# language Entity Framework Core and LINQ How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: Create a project and map the database \u00b6 Let us create a new C# .NET console application in Visual Studio. (NOT the \".NET Framework\" version!) Create a new project; you may work in directory c:\\work . Create the initial EF Core Code First model. We will do Reverse Engineering Code First as we already have a database and we generate C# Code-First model. Install the EF Core NuGet package from the UI or copy these lines in the project file: <ItemGroup> <PackageReference Include= \"Microsoft.EntityFrameworkCore.SqlServer\" Version= \"6.0.8\" /> <PackageReference Include= \"Microsoft.EntityFrameworkCore.Design\" Version= \"6.0.8\" > <PrivateAssets> all </PrivateAssets> <IncludeAssets> runtime; build; native; contentfiles; analyzers; buildtransitive </IncludeAssets> </PackageReference> <PackageReference Include= \"Microsoft.EntityFrameworkCore.Tools\" Version= \"6.0.8\" > <PrivateAssets> all </PrivateAssets> <IncludeAssets> runtime; build; native; contentfiles; analyzers; buildtransitive </IncludeAssets> </PackageReference> </ItemGroup> Run this EF Core PowerShell script in VS in the Package Manager Console which generates the database context and entity model: Scaffold-DbContext 'Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=[neptun]' Microsoft . EntityFrameworkCore . SqlServer -Context AdatvezDbContext -OutputDir Entities EF Core .NET CLI In the future, we will continue to use the commands available from the Package Manager Console , which is installed with the Microsoft.EntityFrameworkCore.Tools package. If anyone wants to use the conventional CLI outside of VS, the documentation can be found at link below. Let's examine the generated code-first model. The database is accessed through the ``AdatvezDbContext'' class Database tables are accessible via DbSet properties. The connection is configured in the `OnConfiguring ' method. In a live application, this typically comes from a configuration file, which is why the AdatvezDbContext(DbContextOptions<AdatvezDbContext> options) constructor was generated The database model was configured in the `OnModelCreating ' method. Make changes to the model Rename the Customer navigation property of the CustomerSite entity to MainCustomer both in the entity and in OnModelCreating . This modification to the code-first model does not change the database schema. CustomerSite.cs public virtual Customer ? MainCustomer { get ; set ; } AdatvezDbContext.cs protected override void OnModelCreating ( ModelBuilder modelBuilder ) { // ... modelBuilder . Entity < CustomerSite >( entity => { // ... entity . HasOne ( d => d . MainCustomer ) . WithMany ( p => p . CustomerSites ) . HasForeignKey ( d => d . CustomerId ) . HasConstraintName ( \"FK__CustomerS__Custo__32E0915F\" ); }); // ... } Change the database schema - Migrations Currently, we have scaffolded our code-first model from the existing database, but we no longer want to maintain the schema with a database-first approach. Instead, use code-first migrations to change the database schema. Let's create an initial migration called `Init', which will contain our initial schema. In the Package Manager Console , issue the following command. Add-Migration Init Let's try to run this migration on the database with the following command. Update-Database This fails by definition, because the commands in the migration want to migrate the schema compared to an empty database, but we already have this schema in our database. EF keeps track of which migrations are already applied to the database in a special table called __EFMigrationHistory . Let's manually add the ``Init'' migration to this table, with which we indicate to EF that it has essentially already run. Pay attention to the name of the migration, which must also include the date. Let's change the database schema in our code-first model. Let the Price' property of our Product' entity be decimal' instead of double', which is more useful for storing amounts of money. It should also be mandatory (cannot be null). Product.cs public decimal Price { get ; set ; } Set the constraint and precision of the SQL field with modelBuilder . DatavezDbContext.cs protected override void OnModelCreating ( ModelBuilder modelBuilder ) { // ... modelBuilder . Entity < Product >( entity => { // ... entity . Property ( e => e . Price ). HasPrecision ( 18 , 2 ). IsRequired (); // ... } // ... } Create a migration of our change and check the generated migration Add-Migration ProductPriceDecimal Run the migration on the database and check its effect in the database Update-Database Task 2: Queries \u00b6 Formulate the following queries using LINQ on the mapped data model. Print the results to the console. Use the debugger to see what kind of SQL statement is generated: by dragging the mouse over the variable of type `IQueryable', you can see the generated SQL as soon as the iteration of the result set begins. List the names and stock of products of which there are more than 30 in stock! Write a query that lists the products that have been ordered at least twice! Create a query that lists orders with a total value of more than HUF 30,000! When listing the result set, the individual items (Product name, amount, net price) should be listed line by line after the customer's name. List the data of the most expensive product! List the buyer pairs that have locations in the same city. A pair should be listed only once. Solution using ConsoleApp3.Entities; using Microsoft.EntityFrameworkCore; Console.WriteLine(\"***** M\u00e1sodik feladat *****\"); using (var db = new AdatvezDbContext()) { // 2.1 Console.WriteLine(\"\\t2.1:\"); // Query szintaktika var productStockQuery = from p in db.Products where p.Stock > 30 select p; // Fluent / Method Chaining szintaktika // var productStockQuery = db.Products.Where(p => p.Stock > 30); foreach (var p in productStockQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\\tStock={p.Stock}\"); } // 2.2 Console.WriteLine(\"\\t2.2:\"); var productOrderQuery = db.Products.Where(p => p.OrderItems.Count >= 2); // query szintaktika //var productOrderQuery = from p in db.Products // where p.OrderItems.Count >= 2 // select p; foreach (var p in productOrderQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\"); } // 2.3 Console.WriteLine(\"\\t2.3 helytelen megold\u00e1s\"); var orderTotalQuery = db.Orders.Where(o => o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000); // query szintaktika //var orderTotalQuery = from o in db.Orders // where o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000 // select o; //foreach (var o in orderTotalQuery) //{ // // Ez az\u00e9rt fog elsz\u00e1llni, mert EF Core-ban nincs alap\u00e9rtelmezetten Lazy Loading, // // \u00edgy a navig\u00e1ci\u00f3s propertyk nem lesznek felt\u00f6ltve // Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name); // foreach (var oi in o.OrderItems) // { // Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\"); // } //} // 2.3 m\u00e1sodik megold\u00e1s // Include-oljuk a hi\u00e1nyz\u00f3 navig\u00e1ci\u00f3s tulajdons\u00e1gokat. // Expression alap\u00fa Include-hoz sz\u00fcks\u00e9g van a k\u00f6vetkez\u0151 n\u00e9vt\u00e9r import\u00e1l\u00e1s\u00e1ra: (CTRL + . is felaj\u00e1nlja a haszn\u00e1lat sor\u00e1n) // using Microsoft.EntityFrameworkCore; // Csak egy lek\u00e9rdez\u00e9st fog gener\u00e1lni, a Navigation Propertyket is felt\u00f6lti r\u00f6gt\u00f6n Console.WriteLine(\"\\tc 2.3 helyes megold\u00e1s:\"); var orderTotalQuery2 = db.Orders .Include(o => o.OrderItems) .ThenInclude(oi => oi.Product) .Include(o => o.CustomerSite) .Include(o => o.CustomerSite.MainCustomer) .Where(o => o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000); // query szintaktika //var orderTotalQuery2 = from o in db.Orders // .Include(o => o.OrderItems) // .ThenInclude(oi => oi.Product) // .Include(o => o.CustomerSite) // .Include(o => o.CustomerSite.MainCustomer) // where o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000 // select o; foreach (var o in orderTotalQuery2) { Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name); foreach (var oi in o.OrderItems) { Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\"); } } // 2.4 Console.WriteLine(\"\\t2.4:\"); var maxPriceQuery = db.Products.Where(p => p.Price == db.Products.Max(a => a.Price)); // query szintaktika //var maxPriceQuery = from p in db.Products // where p.Price == db.Products.Max(a => a.Price) // select p; foreach (var t in maxPriceQuery) { Console.WriteLine($\"\\t\\tName={t.Name}\\tPrice={t.Price}\"); } // 2.5 Console.WriteLine(\"\\t2.5:\"); var cityJoinQuery = db.CustomerSites .Join(db.CustomerSites, s1 => s1.City, s2 => s2.City, (s1, s2) => new { s1, s2 }) .Where(x => x.s1.CustomerId > x.s2.CustomerId) .Select(x => new { c1 = x.s1.MainCustomer, c2 = x.s2.MainCustomer }); // query szintaktika //var cityJoinQuery = from s1 in db.CustomerSites // join s2 in db.CustomerSites on s1.City equals s2.City // where s1.CustomerId > s2.CustomerId // select new { c1 = s1.MainCustomer, c2 = s2.MainCustomer }; foreach (var v in cityJoinQuery) { Console.WriteLine($\"\\t\\tCustomer 1={v.c1.Name}\\tCustomer 2={v.c2.Name}\"); } } Task 3: Data changes \u00b6 The DbContext can be used not only for queries, but also for insertions, modifications and deletions. Write a LINQ-based C# code that increases the price of \"LEGO\" products by 10 percent! Create a new category called Expensive toys and reclassify here all the products whose price is greater than HUF 8,000! Solution using Microsoft.EntityFrameworkCore; using [project name].Entities; Console.WriteLine(\"***** Third Task *****\"); using (var db = new DatavezDbContext()) { // 3.1 Console.WriteLine(\"\\t3.1:\"); var legoProductsQiery = db.Products.Where(p => p.Category.Name == \"LEGO\"); Console.WriteLine(\"\\tBefore change:\"); foreach (var p in legoProductsQiery.ToList()) { Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); p.Price = 1.1m * p.Price; } db.SaveChanges(); Console.WriteLine(\"\\tAfter modification:\"); // ToList induces a database request foreach (var p in legoProductsQiery.ToList()) { Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); } // 3.2 Console.WriteLine(\"\\t3.2:\"); var expensiveToysCategory = db.Categories .Where(c => c.Name == \"Expensive Toys\") .SingleOrDefault(); if (expensiveToysCategory == null) { expensiveToysCategory = new Category { Name = \"Expensive toys\" }; // This is not necessary: if there is an unordered product, we add the category entity to it // and it is automatically included in the category table. However, if we take it explicitly, (1) it better // expresses our intention; and (2) we insert the category even if there are no reclassified product. db.Categories.Add(expensiveToysCategory); } var expensiveProductsQuery = db.Products.Where(p => p.Price > 8000); foreach (var p in expensiveProductsQuery.ToList()) { p.Category = expensiveToysCategory; } db.SaveChanges(); expensiveProductsQuery = db.Products .Include(p => p.Category) .Where(p => p.Category.Name == \"Expensive toys\"); foreach (var p in expensiveProductsQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\\tPrice={p.Price}\\tCategory={p.Category.Name}\"); } } Task 4: Using stored procedures \u00b6 Create a stored procedure using a new code first migration that lists the products of which at least a specified number of units have been sold. Call the stored procedure from C# code! Create a new empty migration named PopularProducts_SP . Add-Migration PopularProducts_SP Create the stored procedure with the code below. Let's ignore the writing of the backward migration for now, where the stored procedure should to be deleted. public partial class PopularProducts_SP : Migration { protected override void Up(MigrationBuilder migrationBuilder) { migrationBuilder.Sql( @\"CREATE OR ALTER PROCEDURE dbo.PopularProducts (@MinAmount int = 10) AS SELECT Product.* FROM Product INNER JOIN ( SELECT OrderItem.ProductID FROM OrderItem GROUP BY OrderItem.ProductID HAVING SUM(OrderItem.Amount) > @MinAmount ) a ON Product.ID = a.ProductID\"); } protected override void Down(MigrationBuilder migrationBuilder) { } } Update the database and check the result! Update-Database Call the stored procedure starting from the Product DbSet of the context using the FromSqlInterpolated or FromSqlRaw methods Solution using Microsoft.EntityFrameworkCore; using [project name].Entities; Console.WriteLine(\"***** Fourth Task *****\"); using (var db = new DatavezDbContext()) { var popularProducts = db.Products.FromSqlInterpolated($\"EXECUTE dbo.PopularProducts @MinAmount={5}\"); foreach (var p in popularProducts) { Console.WriteLine($\"\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); } } FromSqlInterpolated vs. FromSqlRaw In the above solution, the call is defined with the FromSqlInterpolated function, where, due to its name, the string to be interpolated is still processed by EF and the interpolation is not performed traditionally as a string, but replaces SqlParameters in order to protect against SQL injection. On the other hand, when using the FromSqlraw function it is prohibited to use string interpolation, instead we have to manually create the `SqlParameters' and define placeholders in the instruction","title":"Entity Framework"},{"location":"seminar/ef/#entity-framework","text":"The goal of the seminar is to practice writing Linq queries and working with Entity Framework. Entity Framework Core In this seminar, we are using .NET 6 (former .NET Core) available as a cross-platform .NET version for Windows, Linux and Mac.","title":"Entity Framework"},{"location":"seminar/ef/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2022 Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: C# language Entity Framework Core and LINQ","title":"Pre-requisites"},{"location":"seminar/ef/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/ef/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/ef/#exercise-1-create-a-project-and-map-the-database","text":"Let us create a new C# .NET console application in Visual Studio. (NOT the \".NET Framework\" version!) Create a new project; you may work in directory c:\\work . Create the initial EF Core Code First model. We will do Reverse Engineering Code First as we already have a database and we generate C# Code-First model. Install the EF Core NuGet package from the UI or copy these lines in the project file: <ItemGroup> <PackageReference Include= \"Microsoft.EntityFrameworkCore.SqlServer\" Version= \"6.0.8\" /> <PackageReference Include= \"Microsoft.EntityFrameworkCore.Design\" Version= \"6.0.8\" > <PrivateAssets> all </PrivateAssets> <IncludeAssets> runtime; build; native; contentfiles; analyzers; buildtransitive </IncludeAssets> </PackageReference> <PackageReference Include= \"Microsoft.EntityFrameworkCore.Tools\" Version= \"6.0.8\" > <PrivateAssets> all </PrivateAssets> <IncludeAssets> runtime; build; native; contentfiles; analyzers; buildtransitive </IncludeAssets> </PackageReference> </ItemGroup> Run this EF Core PowerShell script in VS in the Package Manager Console which generates the database context and entity model: Scaffold-DbContext 'Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=[neptun]' Microsoft . EntityFrameworkCore . SqlServer -Context AdatvezDbContext -OutputDir Entities EF Core .NET CLI In the future, we will continue to use the commands available from the Package Manager Console , which is installed with the Microsoft.EntityFrameworkCore.Tools package. If anyone wants to use the conventional CLI outside of VS, the documentation can be found at link below. Let's examine the generated code-first model. The database is accessed through the ``AdatvezDbContext'' class Database tables are accessible via DbSet properties. The connection is configured in the `OnConfiguring ' method. In a live application, this typically comes from a configuration file, which is why the AdatvezDbContext(DbContextOptions<AdatvezDbContext> options) constructor was generated The database model was configured in the `OnModelCreating ' method. Make changes to the model Rename the Customer navigation property of the CustomerSite entity to MainCustomer both in the entity and in OnModelCreating . This modification to the code-first model does not change the database schema. CustomerSite.cs public virtual Customer ? MainCustomer { get ; set ; } AdatvezDbContext.cs protected override void OnModelCreating ( ModelBuilder modelBuilder ) { // ... modelBuilder . Entity < CustomerSite >( entity => { // ... entity . HasOne ( d => d . MainCustomer ) . WithMany ( p => p . CustomerSites ) . HasForeignKey ( d => d . CustomerId ) . HasConstraintName ( \"FK__CustomerS__Custo__32E0915F\" ); }); // ... } Change the database schema - Migrations Currently, we have scaffolded our code-first model from the existing database, but we no longer want to maintain the schema with a database-first approach. Instead, use code-first migrations to change the database schema. Let's create an initial migration called `Init', which will contain our initial schema. In the Package Manager Console , issue the following command. Add-Migration Init Let's try to run this migration on the database with the following command. Update-Database This fails by definition, because the commands in the migration want to migrate the schema compared to an empty database, but we already have this schema in our database. EF keeps track of which migrations are already applied to the database in a special table called __EFMigrationHistory . Let's manually add the ``Init'' migration to this table, with which we indicate to EF that it has essentially already run. Pay attention to the name of the migration, which must also include the date. Let's change the database schema in our code-first model. Let the Price' property of our Product' entity be decimal' instead of double', which is more useful for storing amounts of money. It should also be mandatory (cannot be null). Product.cs public decimal Price { get ; set ; } Set the constraint and precision of the SQL field with modelBuilder . DatavezDbContext.cs protected override void OnModelCreating ( ModelBuilder modelBuilder ) { // ... modelBuilder . Entity < Product >( entity => { // ... entity . Property ( e => e . Price ). HasPrecision ( 18 , 2 ). IsRequired (); // ... } // ... } Create a migration of our change and check the generated migration Add-Migration ProductPriceDecimal Run the migration on the database and check its effect in the database Update-Database","title":"Exercise 1: Create a project and map the database"},{"location":"seminar/ef/#task-2-queries","text":"Formulate the following queries using LINQ on the mapped data model. Print the results to the console. Use the debugger to see what kind of SQL statement is generated: by dragging the mouse over the variable of type `IQueryable', you can see the generated SQL as soon as the iteration of the result set begins. List the names and stock of products of which there are more than 30 in stock! Write a query that lists the products that have been ordered at least twice! Create a query that lists orders with a total value of more than HUF 30,000! When listing the result set, the individual items (Product name, amount, net price) should be listed line by line after the customer's name. List the data of the most expensive product! List the buyer pairs that have locations in the same city. A pair should be listed only once. Solution using ConsoleApp3.Entities; using Microsoft.EntityFrameworkCore; Console.WriteLine(\"***** M\u00e1sodik feladat *****\"); using (var db = new AdatvezDbContext()) { // 2.1 Console.WriteLine(\"\\t2.1:\"); // Query szintaktika var productStockQuery = from p in db.Products where p.Stock > 30 select p; // Fluent / Method Chaining szintaktika // var productStockQuery = db.Products.Where(p => p.Stock > 30); foreach (var p in productStockQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\\tStock={p.Stock}\"); } // 2.2 Console.WriteLine(\"\\t2.2:\"); var productOrderQuery = db.Products.Where(p => p.OrderItems.Count >= 2); // query szintaktika //var productOrderQuery = from p in db.Products // where p.OrderItems.Count >= 2 // select p; foreach (var p in productOrderQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\"); } // 2.3 Console.WriteLine(\"\\t2.3 helytelen megold\u00e1s\"); var orderTotalQuery = db.Orders.Where(o => o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000); // query szintaktika //var orderTotalQuery = from o in db.Orders // where o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000 // select o; //foreach (var o in orderTotalQuery) //{ // // Ez az\u00e9rt fog elsz\u00e1llni, mert EF Core-ban nincs alap\u00e9rtelmezetten Lazy Loading, // // \u00edgy a navig\u00e1ci\u00f3s propertyk nem lesznek felt\u00f6ltve // Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name); // foreach (var oi in o.OrderItems) // { // Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\"); // } //} // 2.3 m\u00e1sodik megold\u00e1s // Include-oljuk a hi\u00e1nyz\u00f3 navig\u00e1ci\u00f3s tulajdons\u00e1gokat. // Expression alap\u00fa Include-hoz sz\u00fcks\u00e9g van a k\u00f6vetkez\u0151 n\u00e9vt\u00e9r import\u00e1l\u00e1s\u00e1ra: (CTRL + . is felaj\u00e1nlja a haszn\u00e1lat sor\u00e1n) // using Microsoft.EntityFrameworkCore; // Csak egy lek\u00e9rdez\u00e9st fog gener\u00e1lni, a Navigation Propertyket is felt\u00f6lti r\u00f6gt\u00f6n Console.WriteLine(\"\\tc 2.3 helyes megold\u00e1s:\"); var orderTotalQuery2 = db.Orders .Include(o => o.OrderItems) .ThenInclude(oi => oi.Product) .Include(o => o.CustomerSite) .Include(o => o.CustomerSite.MainCustomer) .Where(o => o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000); // query szintaktika //var orderTotalQuery2 = from o in db.Orders // .Include(o => o.OrderItems) // .ThenInclude(oi => oi.Product) // .Include(o => o.CustomerSite) // .Include(o => o.CustomerSite.MainCustomer) // where o.OrderItems.Sum(oi => oi.Amount * oi.Price) > 30000 // select o; foreach (var o in orderTotalQuery2) { Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name); foreach (var oi in o.OrderItems) { Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\"); } } // 2.4 Console.WriteLine(\"\\t2.4:\"); var maxPriceQuery = db.Products.Where(p => p.Price == db.Products.Max(a => a.Price)); // query szintaktika //var maxPriceQuery = from p in db.Products // where p.Price == db.Products.Max(a => a.Price) // select p; foreach (var t in maxPriceQuery) { Console.WriteLine($\"\\t\\tName={t.Name}\\tPrice={t.Price}\"); } // 2.5 Console.WriteLine(\"\\t2.5:\"); var cityJoinQuery = db.CustomerSites .Join(db.CustomerSites, s1 => s1.City, s2 => s2.City, (s1, s2) => new { s1, s2 }) .Where(x => x.s1.CustomerId > x.s2.CustomerId) .Select(x => new { c1 = x.s1.MainCustomer, c2 = x.s2.MainCustomer }); // query szintaktika //var cityJoinQuery = from s1 in db.CustomerSites // join s2 in db.CustomerSites on s1.City equals s2.City // where s1.CustomerId > s2.CustomerId // select new { c1 = s1.MainCustomer, c2 = s2.MainCustomer }; foreach (var v in cityJoinQuery) { Console.WriteLine($\"\\t\\tCustomer 1={v.c1.Name}\\tCustomer 2={v.c2.Name}\"); } }","title":"Task 2: Queries"},{"location":"seminar/ef/#task-3-data-changes","text":"The DbContext can be used not only for queries, but also for insertions, modifications and deletions. Write a LINQ-based C# code that increases the price of \"LEGO\" products by 10 percent! Create a new category called Expensive toys and reclassify here all the products whose price is greater than HUF 8,000! Solution using Microsoft.EntityFrameworkCore; using [project name].Entities; Console.WriteLine(\"***** Third Task *****\"); using (var db = new DatavezDbContext()) { // 3.1 Console.WriteLine(\"\\t3.1:\"); var legoProductsQiery = db.Products.Where(p => p.Category.Name == \"LEGO\"); Console.WriteLine(\"\\tBefore change:\"); foreach (var p in legoProductsQiery.ToList()) { Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); p.Price = 1.1m * p.Price; } db.SaveChanges(); Console.WriteLine(\"\\tAfter modification:\"); // ToList induces a database request foreach (var p in legoProductsQiery.ToList()) { Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); } // 3.2 Console.WriteLine(\"\\t3.2:\"); var expensiveToysCategory = db.Categories .Where(c => c.Name == \"Expensive Toys\") .SingleOrDefault(); if (expensiveToysCategory == null) { expensiveToysCategory = new Category { Name = \"Expensive toys\" }; // This is not necessary: if there is an unordered product, we add the category entity to it // and it is automatically included in the category table. However, if we take it explicitly, (1) it better // expresses our intention; and (2) we insert the category even if there are no reclassified product. db.Categories.Add(expensiveToysCategory); } var expensiveProductsQuery = db.Products.Where(p => p.Price > 8000); foreach (var p in expensiveProductsQuery.ToList()) { p.Category = expensiveToysCategory; } db.SaveChanges(); expensiveProductsQuery = db.Products .Include(p => p.Category) .Where(p => p.Category.Name == \"Expensive toys\"); foreach (var p in expensiveProductsQuery) { Console.WriteLine($\"\\t\\tName={p.Name}\\tPrice={p.Price}\\tCategory={p.Category.Name}\"); } }","title":"Task 3: Data changes"},{"location":"seminar/ef/#task-4-using-stored-procedures","text":"Create a stored procedure using a new code first migration that lists the products of which at least a specified number of units have been sold. Call the stored procedure from C# code! Create a new empty migration named PopularProducts_SP . Add-Migration PopularProducts_SP Create the stored procedure with the code below. Let's ignore the writing of the backward migration for now, where the stored procedure should to be deleted. public partial class PopularProducts_SP : Migration { protected override void Up(MigrationBuilder migrationBuilder) { migrationBuilder.Sql( @\"CREATE OR ALTER PROCEDURE dbo.PopularProducts (@MinAmount int = 10) AS SELECT Product.* FROM Product INNER JOIN ( SELECT OrderItem.ProductID FROM OrderItem GROUP BY OrderItem.ProductID HAVING SUM(OrderItem.Amount) > @MinAmount ) a ON Product.ID = a.ProductID\"); } protected override void Down(MigrationBuilder migrationBuilder) { } } Update the database and check the result! Update-Database Call the stored procedure starting from the Product DbSet of the context using the FromSqlInterpolated or FromSqlRaw methods Solution using Microsoft.EntityFrameworkCore; using [project name].Entities; Console.WriteLine(\"***** Fourth Task *****\"); using (var db = new DatavezDbContext()) { var popularProducts = db.Products.FromSqlInterpolated($\"EXECUTE dbo.PopularProducts @MinAmount={5}\"); foreach (var p in popularProducts) { Console.WriteLine($\"\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\"); } } FromSqlInterpolated vs. FromSqlRaw In the above solution, the call is defined with the FromSqlInterpolated function, where, due to its name, the string to be interpolated is still processed by EF and the interpolation is not performed traditionally as a string, but replaces SqlParameters in order to protect against SQL injection. On the other hand, when using the FromSqlraw function it is prohibited to use string interpolation, instead we have to manually create the `SqlParameters' and define placeholders in the instruction","title":"Task 4: Using stored procedures"},{"location":"seminar/jpa/","text":"JPA & Spring Data \u00b6 To goal of this seminar is to practice working with JPA and Spring Data. Main topics of focus: working with entities, querying the database with various techniques, updating the database. The code is integrated into a skeleton web application with a UI for testing. Pre-requisites \u00b6 Required tools to complete the tasks: Spring Tool Suite (an IDE based on Eclipse) Microsoft SQL Server Express edition (localdb does not work here) SQL Server Management Studio Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo Recommended to review: JPA lecture EJB, Spring lecture How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Tips for using the IDE \u00b6 Search Type(class, interface, enum): Ctrl+Shift+T (instead of opening folders in Project explorer) Search file: Ctrl+Shift+R Fix missing imports:Ctrl+Shift+O Format code: Ctrl+Shift+F In Java Resources right-click a package / New Class/Interfaces will create the source in this package Restore default layout of views: Window > Reset perspective Increase font size: Window menu / Preferences, start typing font to locate Fonts and Colors Select it and under Basic choose Text Font and increase the size Exercise 0: Create a database \u00b6 Use Microsoft SQL Server Management Studio to connect to the database. We are not using localdb here; the address is: localhost\\sqlexpress and use SQL Server Authentication with username and password sa . Create a new database with the name adatvez . You should use this exact name or will have to update the Java project . To create a new database see the instructions in the first seminar material . If a database with this name already exists, no need to re-create it. Run the database initialization script on this database. If the database exists on this machine, run the script anyway to reset any changes made in the schema. Exercise 1: Start the IDE \u00b6 Start Spring Tool Suite from here: c:\\Work\\hatteralkalmazasok\\sts-4.5.1.RELEASE\\SpringToolSuite4.exe . It will ask for a workspace, select: c:\\Work\\hatteralkalmazasok\\workspaces\\adatvez If there is a webshop project in the Project Explorer already, delete it: right-click the project / Delete , and check Delete project contents on disk Exercise 2: Import project \u00b6 Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo.git Import the downloaded project into the workspace: Open File / Import... Start typing Existing Maven Projects and choose it Locate the downloaded webshop project (the webshop folder in the checked-out repository), OK, check the webshop project in the dialog Finish Overview of the projects It is a maven based project. Maven is a command-line build tool that can be integrated with IDEs as well. It can download the libraries our projects depend on from public repositories. After opening the pom.xml file, the maven project's config file, you can see some dependency tags that will transitively download Hibernate, our JPA implementation, Spring Boot, Spring Data, Spring MVC and Thymeleaf. The application.properties file contains some basic settings. Let us verify the database name (spring.datasource.url), the user name (spring.datasource.username) and password (spring.datasource.password) for the DB access here. In classic Java EE web applications, this JNDI name of the database should be defined in the persistence.xml , but Spring Boot supports XML-less configuration. WebshopApplication is the entry point and configuration of the Spring Boot application. A traditional web application should be deployed to a web container (e.g., Tomcat, Jetty) running in a separate process. In the case of Spring Boot, however, Spring Boot itself will start an embedded web container (Tomcat, by default). The web interface is one page: src\\main\\resources\\templates\\testPage.html . We will not modify it. It contains standard HTML and some Thymeleaf attributes. WebshopController : the controller class implementing the web layer (its methods handle the HTTP requests). These methods typically call a query implemented in a repository or a service method and put the result into the model with a name that we can reference via Thymeleaf. You should call the methods implementing the tasks at the //TODO comments. Exercise 3: Overview of the entities \u00b6 The entities can be found in the hu.bme.aut.adatvez.webshop.model package. We could have written them by hand, but in this case, they were generated from the DB tables via the JPA plugin of Eclipse. Open an entity class, e.g., Vat, and check the JPA-related code. You can see the @Entity , @Id annotations, and @OneToMany or @ManyToOne for defining relationships. Exercise 4: Queries \u00b6 Implement the following queries on the data model. In JPA and Spring Data, you can write queries by different means. In the following tasks, we specify how to write the query so that multiple ways can be demonstrated. It is important to note that each task could be written using any technology and style. The requirements are provided for demonstrating all technologies. The methods implementing the queries should always be called in the WebshopController class, at the corresponding //TODO comment, run the application and test the query from a browser at address http://localhost:9080 . a) List the names and stock of those products of which we have more than 30 pieces in stock! Method: Spring Data repository interface with method name-derived query. b) Write a query that lists those products that were ordered at least twice! Method: JPQL query created with an injected EntityManager in a Spring Data custom repository implementation. c) List the data of the most expensive product! Method: Named query, called from Spring Data repository or with an injected EntityManager. When running the application, the SQL statements generated by Hibernate can be observed in the Console view of Eclipse, due to this config line in application.properties: spring.jpa.show-sql=true Running the application \u00b6 Right-click on the webshop project in the Project Explorer > Debug As > Spring Boot App. This starts the application in debug mode, which starts an embedded web container, and the application is available at http://localhost:9080 from a browser. Having done this once, we can do it more easily: Click on the Debug icon on the toolbar, and you will see the webshop run there. If under the Debug icon you find webshop run , the method above is unnecessary. The running application can be stopped with the red Terminate icon in the Console view. If we run the application twice, without terminating the first run, the second run will report a port collision on the port 9080 and stop. This second execution will be visible in the Console view, and the Terminate command will be inactive, as this copy has been terminated already. Click on the gray double C icon next to Terminate to close this view, and only the active running process will be visible. If we close the Console view by mistake, use shortcut Alt+Shift+Q, C or menu Window / Show View / Console to reopen. After shutdown, we can re-run using F11. When running the application in debug mode, the modifications in HTML files and some Java ode modifications are immediately actualized, so we only have to refresh the browser to see the effect of the code modification. But the application has to be restarted if we modify the Java code in either of the following ways adding a new type adding/removing/modifying an annotation adding a new class-or member variable, or method we changed the signature of a method. Simply put, when modifying code that is not inside of an existing method, a restart will be needed. Solution 4.a exercise In the dao package open ProductRepository interface that implements the Spring Data JpaRepository . There are a few methods for other exercises. Some define a @Query annotation and the query as text, some work without such annotation. We will not need the @Query annotation, but rather have Spring Data infer the SQL instruction from the method name as follows: package hu.bme.aut.adatvez.webshop.dao ; import java.math.BigDecimal ; import java.util.List ; import hu.bme.aut.adatvez.webshop.model.Product ; import org.springframework.data.jpa.repository.JpaRepository ; public interface ProductRepository extends JpaRepository < Product , Long > , ProductRepositoryCustom { ... List < Product > findByStockGreaterThan ( BigDecimal limit ); } WebshopController already contains an injected ProductRepository ; let us call this method at TODO 4.a: @Controller public class WebshopController { @Autowired ProductRepository productRepository ; //... // 4.a private List < Product > findProductsOver30 () { return productRepository . findByStockGreaterThan ( BigDecimal . valueOf ( 30 )); } } 4.b exercise In the dao package find ProductRepositoryCustom interface add a new method findProductsOrderedAtLeastTwice : package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; public interface ProductRepositoryCustom { List < Product > findProductsOrderedAtLeastTwice (); } The implementation class ProductRepositoryImpl will contain an error now as it does not implement ProductRepositoryCustom . Let us open this class, and line of the class declaration there will be a light bulb we can click to generate the method skeleton: Add the method's implementation as follows: use the injected EntityManager to create and run the query. package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; import javax.persistence.EntityManager ; import javax.persistence.PersistenceContext ; public class ProductRepositoryImpl implements ProductRepositoryCustom { @PersistenceContext EntityManager em ; @Override public List < Product > findProductsOrderedAtLeastTwice (){ return em . createQuery ( \"SELECT DISTINCT p FROM Product p LEFT JOIN FETCH p.orderitems WHERE size(p.orderitems) >= :itemsMin\" , Product . class ) . setParameter ( \"itemsMin\" , 2 ) . getResultList (); } } Note: we might try this command: SELECT p FROM Product p WHERE size(p.orderitems) /= :itemsMin , which will yield an org.hibernate.LazyInitializationException error, hence the LEFT JOIN FETCH above. Call this in WebshopController : // 4.b private List < Product > findProductsOrderedAtLeastTwice () { // TODO return productRepository . findProductsOrderedAtLeastTwice (); } 4.c exercise Open entity class Product where we can find a few named querys; we need the second one: @NamedQueries ({ @NamedQuery ( name = \"Product.findAll\" , query = \"SELECT p FROM Product p\" ), @NamedQuery ( name = \"Product.findMostExpensive\" , query = \"SELECT p FROM Product p WHERE p.price IN (SELECT MAX(p2.price) FROM Product p2)\" ) }) This named query can be called in two ways. The first is to create a method in ProductRepository with the same name (without the Product. prefix.), that is: public List < Product > findMostExpensive (); The second option is to execute it manually in ProductRepositoryImpl using EntityManager : @Override public List < Product > findMostExpensiveProducts (){ return em . createNamedQuery ( \"Product.findMostExpensive\" , Product . class ). getResultList (); } This method also needs to be added to the ProductRepositoryCustom interface. E.g. right-click / Refactor / Pull up Finally, call the method in WebshopController : // 4.c private List < Product > findMostExpensiveProducts () { // TODO // return productRepository.findMostExpensiveProducts(); return productRepository . findMostExpensive (); } Exercise 5: Data modification \u00b6 JPA can also be used to modify the database content. a) Write a JPQL query into the ProductRepository interface that raises the price of \"Building items\" by 10 percent! b) Write a method that creates a new category called \"Expensive toys\", if it does not exist yet, and move all the products with a price higher than 8000 into this category! c) Simple individual task: create a CategoryRepository interface, and implement a method name-derived query that you can use in task 5.b) instead of the query created with the injected EntityManager. Solution 5.a exercise Crate an UPDATE query in ProductRepository interface. We have to denote that this is a @Modifying query, and also add @Transactional (from package org.springframework...`): @Modifying @Transactional @Query ( \"UPDATE Product p SET p.price=p.price*1.1 WHERE p.id IN (SELECT p2.id FROM Product p2 WHERE p2.category.name=:categoryName)\" ) void categoryRaisePrice ( @Param ( \"categoryName\" ) String categoryName ); Call in WebshopController : // 5.a @RequestMapping ( value = \"/raisePriceOfBuildingItems\" , method = { RequestMethod . POST , RequestMethod . GET }) private String raisePriceOfBuildingItems () { // TODO productRepository . categoryRaisePrice ( \"Building items\" ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.b exercise In the dao package add a new class CategoryService with a @Service annotation with a @Transactional method: @Service public class CategoryService { @PersistenceContext private EntityManager em ; @Autowired ProductRepository productRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ String name = \"Expensive toys\" ; Category categoryExpensive = null ; List < Category > resultList = em . createQuery ( \"SELECT c from Category c WHERE c.name=:name\" , Category . class ) . setParameter ( \"name\" , name ) . getResultList (); if ( resultList . isEmpty ()){ // 0 or null id triggers @GeneratedValue; this is a scalar, hence use 0 categoryExpensive = new Category ( 0 , name ); em . persist ( categoryExpensive ); } else { categoryExpensive = resultList . get ( 0 ); } List < Product > expensiveProducts = productRepository . findByPriceGreaterThan ( priceLimit ); for ( Product product : expensiveProducts ) { categoryExpensive . addProduct ( product ); } } } Let us note that the managed entities (fetched through queries within the transaction, or added as new with persist) need no explicit save; the transaction saves them to DB automatically. Call in WebshopController : @Autowired CategoryService categoryService ; ... // 5.b @RequestMapping ( value = \"/moveToExpensiveToys\" , method = { RequestMethod . POST , RequestMethod . GET }) private String moveToExpensiveToys () { // TODO categoryService . moveToExpensiveToys ( 8000.0 ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.c exercise In dao package add a new interface CategoryRepository , similar to ProductRepository (without the Custom inheritance) with one method: public interface CategoryRepository extends JpaRepository < Category , Long > { List < Category > findByName ( String name ); } This simplifies the CategoryService as follows: @Service public class CategoryService { ... @Autowired CategoryRepository categoryRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ // ... List < Category > resultList = categoryRepository . findByName ( name ); // ... } } Exercise 6: Using stored procedures \u00b6 Use the CreatePaymentMethod stored procedure to create a new Paymentmethod ! Check in SQL Server Management Studio, whether the database contains the stored procedure with the name CreatePaymentMethod ! If not, create the procedure with the code below! CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Solution The PaymentMethod entity has the following annotation. Compare it to the stored procedure code! @NamedStoredProcedureQueries ({ @NamedStoredProcedureQuery ( name = \"createMethodSP\" , procedureName = \"CreateNewPaymentMethod\" , parameters = { @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Method\" , type = String . class ), @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Deadline\" , type = BigDecimal . class ) }) }) public class Paymentmethod implements Serializable { ... The named stored procedure query can be called from a Spring Data repository ( dao package New Interface ... / PaymentmethodRepository ): public interface PaymentmethodRepository extends JpaRepository < Paymentmethod , Long > { @Procedure ( name = \"createMethodSP\" ) void newMethod ( @Param ( \"Method\" ) String method , @Param ( \"Deadline\" ) BigDecimal deadline ); } Without Spring Data we could use EntityManager : @Service public class PaymentmethodService { @PersistenceContext private EntityManager em ; public void createNewMethod ( Paymentmethod paymentMethod ){ StoredProcedureQuery sp = em . createNamedStoredProcedureQuery ( \"createMethodSP\" ); sp . setParameter ( \"Method\" , paymentMethod . getMethod ()); sp . setParameter ( \"Deadline\" , paymentMethod . getDeadline ()); sp . execute (); } } Call from the web layer: Inject into WebshopController the PaymentmethodRepository interface: @Autowired PaymentmethodRepository paymentmethodRepository ; Call the method from WebshopController at the last TODO paymentmethodRepository . newMethod ( paymentMethod . getMethod (), paymentMethod . getDeadline ());","title":"JPA & Spring Data"},{"location":"seminar/jpa/#jpa-spring-data","text":"To goal of this seminar is to practice working with JPA and Spring Data. Main topics of focus: working with entities, querying the database with various techniques, updating the database. The code is integrated into a skeleton web application with a UI for testing.","title":"JPA &amp; Spring Data"},{"location":"seminar/jpa/#pre-requisites","text":"Required tools to complete the tasks: Spring Tool Suite (an IDE based on Eclipse) Microsoft SQL Server Express edition (localdb does not work here) SQL Server Management Studio Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo Recommended to review: JPA lecture EJB, Spring lecture","title":"Pre-requisites"},{"location":"seminar/jpa/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/jpa/#tips-for-using-the-ide","text":"Search Type(class, interface, enum): Ctrl+Shift+T (instead of opening folders in Project explorer) Search file: Ctrl+Shift+R Fix missing imports:Ctrl+Shift+O Format code: Ctrl+Shift+F In Java Resources right-click a package / New Class/Interfaces will create the source in this package Restore default layout of views: Window > Reset perspective Increase font size: Window menu / Preferences, start typing font to locate Fonts and Colors Select it and under Basic choose Text Font and increase the size","title":"Tips for using the IDE"},{"location":"seminar/jpa/#exercise-0-create-a-database","text":"Use Microsoft SQL Server Management Studio to connect to the database. We are not using localdb here; the address is: localhost\\sqlexpress and use SQL Server Authentication with username and password sa . Create a new database with the name adatvez . You should use this exact name or will have to update the Java project . To create a new database see the instructions in the first seminar material . If a database with this name already exists, no need to re-create it. Run the database initialization script on this database. If the database exists on this machine, run the script anyway to reset any changes made in the schema.","title":"Exercise 0: Create a database"},{"location":"seminar/jpa/#exercise-1-start-the-ide","text":"Start Spring Tool Suite from here: c:\\Work\\hatteralkalmazasok\\sts-4.5.1.RELEASE\\SpringToolSuite4.exe . It will ask for a workspace, select: c:\\Work\\hatteralkalmazasok\\workspaces\\adatvez If there is a webshop project in the Project Explorer already, delete it: right-click the project / Delete , and check Delete project contents on disk","title":"Exercise 1: Start the IDE"},{"location":"seminar/jpa/#exercise-2-import-project","text":"Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo.git Import the downloaded project into the workspace: Open File / Import... Start typing Existing Maven Projects and choose it Locate the downloaded webshop project (the webshop folder in the checked-out repository), OK, check the webshop project in the dialog Finish Overview of the projects It is a maven based project. Maven is a command-line build tool that can be integrated with IDEs as well. It can download the libraries our projects depend on from public repositories. After opening the pom.xml file, the maven project's config file, you can see some dependency tags that will transitively download Hibernate, our JPA implementation, Spring Boot, Spring Data, Spring MVC and Thymeleaf. The application.properties file contains some basic settings. Let us verify the database name (spring.datasource.url), the user name (spring.datasource.username) and password (spring.datasource.password) for the DB access here. In classic Java EE web applications, this JNDI name of the database should be defined in the persistence.xml , but Spring Boot supports XML-less configuration. WebshopApplication is the entry point and configuration of the Spring Boot application. A traditional web application should be deployed to a web container (e.g., Tomcat, Jetty) running in a separate process. In the case of Spring Boot, however, Spring Boot itself will start an embedded web container (Tomcat, by default). The web interface is one page: src\\main\\resources\\templates\\testPage.html . We will not modify it. It contains standard HTML and some Thymeleaf attributes. WebshopController : the controller class implementing the web layer (its methods handle the HTTP requests). These methods typically call a query implemented in a repository or a service method and put the result into the model with a name that we can reference via Thymeleaf. You should call the methods implementing the tasks at the //TODO comments.","title":"Exercise 2: Import project"},{"location":"seminar/jpa/#exercise-3-overview-of-the-entities","text":"The entities can be found in the hu.bme.aut.adatvez.webshop.model package. We could have written them by hand, but in this case, they were generated from the DB tables via the JPA plugin of Eclipse. Open an entity class, e.g., Vat, and check the JPA-related code. You can see the @Entity , @Id annotations, and @OneToMany or @ManyToOne for defining relationships.","title":"Exercise 3: Overview of the entities"},{"location":"seminar/jpa/#exercise-4-queries","text":"Implement the following queries on the data model. In JPA and Spring Data, you can write queries by different means. In the following tasks, we specify how to write the query so that multiple ways can be demonstrated. It is important to note that each task could be written using any technology and style. The requirements are provided for demonstrating all technologies. The methods implementing the queries should always be called in the WebshopController class, at the corresponding //TODO comment, run the application and test the query from a browser at address http://localhost:9080 . a) List the names and stock of those products of which we have more than 30 pieces in stock! Method: Spring Data repository interface with method name-derived query. b) Write a query that lists those products that were ordered at least twice! Method: JPQL query created with an injected EntityManager in a Spring Data custom repository implementation. c) List the data of the most expensive product! Method: Named query, called from Spring Data repository or with an injected EntityManager. When running the application, the SQL statements generated by Hibernate can be observed in the Console view of Eclipse, due to this config line in application.properties: spring.jpa.show-sql=true","title":"Exercise 4: Queries"},{"location":"seminar/jpa/#running-the-application","text":"Right-click on the webshop project in the Project Explorer > Debug As > Spring Boot App. This starts the application in debug mode, which starts an embedded web container, and the application is available at http://localhost:9080 from a browser. Having done this once, we can do it more easily: Click on the Debug icon on the toolbar, and you will see the webshop run there. If under the Debug icon you find webshop run , the method above is unnecessary. The running application can be stopped with the red Terminate icon in the Console view. If we run the application twice, without terminating the first run, the second run will report a port collision on the port 9080 and stop. This second execution will be visible in the Console view, and the Terminate command will be inactive, as this copy has been terminated already. Click on the gray double C icon next to Terminate to close this view, and only the active running process will be visible. If we close the Console view by mistake, use shortcut Alt+Shift+Q, C or menu Window / Show View / Console to reopen. After shutdown, we can re-run using F11. When running the application in debug mode, the modifications in HTML files and some Java ode modifications are immediately actualized, so we only have to refresh the browser to see the effect of the code modification. But the application has to be restarted if we modify the Java code in either of the following ways adding a new type adding/removing/modifying an annotation adding a new class-or member variable, or method we changed the signature of a method. Simply put, when modifying code that is not inside of an existing method, a restart will be needed. Solution 4.a exercise In the dao package open ProductRepository interface that implements the Spring Data JpaRepository . There are a few methods for other exercises. Some define a @Query annotation and the query as text, some work without such annotation. We will not need the @Query annotation, but rather have Spring Data infer the SQL instruction from the method name as follows: package hu.bme.aut.adatvez.webshop.dao ; import java.math.BigDecimal ; import java.util.List ; import hu.bme.aut.adatvez.webshop.model.Product ; import org.springframework.data.jpa.repository.JpaRepository ; public interface ProductRepository extends JpaRepository < Product , Long > , ProductRepositoryCustom { ... List < Product > findByStockGreaterThan ( BigDecimal limit ); } WebshopController already contains an injected ProductRepository ; let us call this method at TODO 4.a: @Controller public class WebshopController { @Autowired ProductRepository productRepository ; //... // 4.a private List < Product > findProductsOver30 () { return productRepository . findByStockGreaterThan ( BigDecimal . valueOf ( 30 )); } } 4.b exercise In the dao package find ProductRepositoryCustom interface add a new method findProductsOrderedAtLeastTwice : package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; public interface ProductRepositoryCustom { List < Product > findProductsOrderedAtLeastTwice (); } The implementation class ProductRepositoryImpl will contain an error now as it does not implement ProductRepositoryCustom . Let us open this class, and line of the class declaration there will be a light bulb we can click to generate the method skeleton: Add the method's implementation as follows: use the injected EntityManager to create and run the query. package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; import javax.persistence.EntityManager ; import javax.persistence.PersistenceContext ; public class ProductRepositoryImpl implements ProductRepositoryCustom { @PersistenceContext EntityManager em ; @Override public List < Product > findProductsOrderedAtLeastTwice (){ return em . createQuery ( \"SELECT DISTINCT p FROM Product p LEFT JOIN FETCH p.orderitems WHERE size(p.orderitems) >= :itemsMin\" , Product . class ) . setParameter ( \"itemsMin\" , 2 ) . getResultList (); } } Note: we might try this command: SELECT p FROM Product p WHERE size(p.orderitems) /= :itemsMin , which will yield an org.hibernate.LazyInitializationException error, hence the LEFT JOIN FETCH above. Call this in WebshopController : // 4.b private List < Product > findProductsOrderedAtLeastTwice () { // TODO return productRepository . findProductsOrderedAtLeastTwice (); } 4.c exercise Open entity class Product where we can find a few named querys; we need the second one: @NamedQueries ({ @NamedQuery ( name = \"Product.findAll\" , query = \"SELECT p FROM Product p\" ), @NamedQuery ( name = \"Product.findMostExpensive\" , query = \"SELECT p FROM Product p WHERE p.price IN (SELECT MAX(p2.price) FROM Product p2)\" ) }) This named query can be called in two ways. The first is to create a method in ProductRepository with the same name (without the Product. prefix.), that is: public List < Product > findMostExpensive (); The second option is to execute it manually in ProductRepositoryImpl using EntityManager : @Override public List < Product > findMostExpensiveProducts (){ return em . createNamedQuery ( \"Product.findMostExpensive\" , Product . class ). getResultList (); } This method also needs to be added to the ProductRepositoryCustom interface. E.g. right-click / Refactor / Pull up Finally, call the method in WebshopController : // 4.c private List < Product > findMostExpensiveProducts () { // TODO // return productRepository.findMostExpensiveProducts(); return productRepository . findMostExpensive (); }","title":"Running the application"},{"location":"seminar/jpa/#exercise-5-data-modification","text":"JPA can also be used to modify the database content. a) Write a JPQL query into the ProductRepository interface that raises the price of \"Building items\" by 10 percent! b) Write a method that creates a new category called \"Expensive toys\", if it does not exist yet, and move all the products with a price higher than 8000 into this category! c) Simple individual task: create a CategoryRepository interface, and implement a method name-derived query that you can use in task 5.b) instead of the query created with the injected EntityManager. Solution 5.a exercise Crate an UPDATE query in ProductRepository interface. We have to denote that this is a @Modifying query, and also add @Transactional (from package org.springframework...`): @Modifying @Transactional @Query ( \"UPDATE Product p SET p.price=p.price*1.1 WHERE p.id IN (SELECT p2.id FROM Product p2 WHERE p2.category.name=:categoryName)\" ) void categoryRaisePrice ( @Param ( \"categoryName\" ) String categoryName ); Call in WebshopController : // 5.a @RequestMapping ( value = \"/raisePriceOfBuildingItems\" , method = { RequestMethod . POST , RequestMethod . GET }) private String raisePriceOfBuildingItems () { // TODO productRepository . categoryRaisePrice ( \"Building items\" ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.b exercise In the dao package add a new class CategoryService with a @Service annotation with a @Transactional method: @Service public class CategoryService { @PersistenceContext private EntityManager em ; @Autowired ProductRepository productRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ String name = \"Expensive toys\" ; Category categoryExpensive = null ; List < Category > resultList = em . createQuery ( \"SELECT c from Category c WHERE c.name=:name\" , Category . class ) . setParameter ( \"name\" , name ) . getResultList (); if ( resultList . isEmpty ()){ // 0 or null id triggers @GeneratedValue; this is a scalar, hence use 0 categoryExpensive = new Category ( 0 , name ); em . persist ( categoryExpensive ); } else { categoryExpensive = resultList . get ( 0 ); } List < Product > expensiveProducts = productRepository . findByPriceGreaterThan ( priceLimit ); for ( Product product : expensiveProducts ) { categoryExpensive . addProduct ( product ); } } } Let us note that the managed entities (fetched through queries within the transaction, or added as new with persist) need no explicit save; the transaction saves them to DB automatically. Call in WebshopController : @Autowired CategoryService categoryService ; ... // 5.b @RequestMapping ( value = \"/moveToExpensiveToys\" , method = { RequestMethod . POST , RequestMethod . GET }) private String moveToExpensiveToys () { // TODO categoryService . moveToExpensiveToys ( 8000.0 ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.c exercise In dao package add a new interface CategoryRepository , similar to ProductRepository (without the Custom inheritance) with one method: public interface CategoryRepository extends JpaRepository < Category , Long > { List < Category > findByName ( String name ); } This simplifies the CategoryService as follows: @Service public class CategoryService { ... @Autowired CategoryRepository categoryRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ // ... List < Category > resultList = categoryRepository . findByName ( name ); // ... } }","title":"Exercise 5: Data modification"},{"location":"seminar/jpa/#exercise-6-using-stored-procedures","text":"Use the CreatePaymentMethod stored procedure to create a new Paymentmethod ! Check in SQL Server Management Studio, whether the database contains the stored procedure with the name CreatePaymentMethod ! If not, create the procedure with the code below! CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Solution The PaymentMethod entity has the following annotation. Compare it to the stored procedure code! @NamedStoredProcedureQueries ({ @NamedStoredProcedureQuery ( name = \"createMethodSP\" , procedureName = \"CreateNewPaymentMethod\" , parameters = { @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Method\" , type = String . class ), @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Deadline\" , type = BigDecimal . class ) }) }) public class Paymentmethod implements Serializable { ... The named stored procedure query can be called from a Spring Data repository ( dao package New Interface ... / PaymentmethodRepository ): public interface PaymentmethodRepository extends JpaRepository < Paymentmethod , Long > { @Procedure ( name = \"createMethodSP\" ) void newMethod ( @Param ( \"Method\" ) String method , @Param ( \"Deadline\" ) BigDecimal deadline ); } Without Spring Data we could use EntityManager : @Service public class PaymentmethodService { @PersistenceContext private EntityManager em ; public void createNewMethod ( Paymentmethod paymentMethod ){ StoredProcedureQuery sp = em . createNamedStoredProcedureQuery ( \"createMethodSP\" ); sp . setParameter ( \"Method\" , paymentMethod . getMethod ()); sp . setParameter ( \"Deadline\" , paymentMethod . getDeadline ()); sp . execute (); } } Call from the web layer: Inject into WebshopController the PaymentmethodRepository interface: @Autowired PaymentmethodRepository paymentmethodRepository ; Call the method from WebshopController at the last TODO paymentmethodRepository . newMethod ( paymentMethod . getMethod (), paymentMethod . getDeadline ());","title":"Exercise 6: Using stored procedures"},{"location":"seminar/mongodb/","text":"MongoDB \u00b6 The seminar's goal is to understand the concepts of the MongoDB document database and the usage of the MongoDB C#/.NET Driver . Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2022 ( not VS Code) MongoDB Community Edition Robo 3T Database initialization script: mongo.js Starter code: https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo Recommended to review: C# language and Linq queries MongoDB lecture Using MongoDB guide How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create database, open starter code \u00b6 Open a PowerShell console (search for PowerShell in the Start menu and start it, but not the one with \"ISE\" in the same - that is not the console). Copy and paste the script into the console and run it by pressing enter. Please note, that you might need to change the the directory name in the last command, e.g., if the app version is different. Remove-Item c :\\ work \\ mongodatabase -Recurse -ErrorAction Ignore New-Item -Type Directory c :\\ work \\ mongodatabase c :\\ tools \\ mongodb-win32-x86_64-windows - 4 . 4 . 10 \\ bin \\ mongod . exe - -dbpath c :\\ work \\ mongodatabase Keep this window open because the server is running here. You can stop it by pressing Ctrl+C at the end of the class. Launch Robo3, which you find in directory c:\\tools\\robo3t... and connect to the MongoDB server. Let us create a new database by right-clicking the connection ( localhost ). Let the database name be datadriven . Open a new shell into the database by right-clicking it and choosing the Open Shell command. This will open a single-line textbox to the right. Copy the database initialization script from here into the textbox and execute it by pressing the green play button on the toolbar. This will create our collections - expand the Collections item to check them. Download the starter solution! Open a new command prompt or PowerShell console (do not use the one the server is running in) Navigate to a folder, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo.git Open the sln file from the newly created folder using Visual Studio. Let us examine this project. This is a .NET console application. The structure resembles the structure of the Entity Framework project seen before: directory Entities contains the database entities while our code will be written into Program.cs . Program.cs already contains the initialization of the connection to MongoDB. Interface IMongoClient is used for all communication with the database. We will not use this directly. Interface IMongoDatabase represents the database datadriven within the MongoDB server. And the IMongoCollection<TEntity> interfaces represent the specific collections we can use to execute queries and modification commands. The database documents are mapped to the C# entity classes in folder Entities . A major difference compared to the behavior previously seen in Entity Framework is that these classes were not generated by written manually. Most entities are already mapped. We will create one more class during an exercise. Exercise 1: Queries \u00b6 Write C# code using the MongoDB C#/.NET Driver in the following exercises. Print the results to the console. List the names and the amount of stock of all products that we have more than 30 in stock! List the orders that consist of at least two items! List the orders that have a total value of at least 30.000! For each order, print the customer name, and list all items of the order (with the product name, amount, and price). Find the most expensive product! List the products that have been ordered at least twice! Solution We need only the product collection and execute a simple query. The filter criteria can be written as a Lambda-expression and with the builder syntax too. Console . WriteLine ( \"***** Exercise one *****\" ); // 1.1 first solution Console . WriteLine ( \"\\t1.1 First solution:\" ); var qProductAndStock1 = productsCollection . Find ( p => p . Stock > 30 ) . ToList (); foreach ( var p in qProductAndStock1 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); // 1.1 second solution Console . WriteLine ( \"\\t1.1 Second solution:\" ); var qProductAndStock2 = productsCollection . Find ( Builders < Product >. Filter . Gt ( p => p . Stock , 30 )) . ToList (); foreach ( var p in qProductAndStock2 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); This is similar to the previous one. We may note that we would have needed a join in a relational database, but we have everything at hand here. // 1.2 first solution Console . WriteLine ( \"\\t1.2 First solution:\" ); var qOrderItems1 = ordersCollection . Find ( o => o . OrderItems . Length >= 2 ) . ToList (); foreach ( var o in qOrderItems1 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); // 1.2 second solution Console . WriteLine ( \"\\t1.2 Second solution:\" ); var qOrderItems2 = ordersCollection . Find ( Builders < Order >. Filter . SizeGte ( o => o . OrderItems , 2 )) . ToList (); foreach ( var o in qOrderItems2 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); A simple query is not sufficient for this exercise; thus, we need the aggregation pipeline. We may still note that every information we need is still available in one collection. // 1.3 Console . WriteLine ( \"\\t1.3:\" ); var qOrderTotal = ordersCollection . Aggregate () . Project ( order => new { CustomerID = order . CustomerID , OrderItems = order . OrderItems , Total = order . OrderItems . Sum ( oi => oi . Amount * oi . Price ) }) . Match ( order => order . Total > 30000 ) . ToList (); foreach ( var o in qOrderTotal ) { Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\" ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( $\"\\t\\t\\tProductID={oi.ProductID}\\tPrice={oi.Price}\\tAmount={oi.Amount}\" ); } To find the most expensive product, we need two queries: first, find the largest price value, then find the products with this price. // 1.4 Console . WriteLine ( \"\\t1.4:\" ); var maxPrice = productsCollection . Find ( _ => true ) . SortByDescending ( p => p . Price ) . Limit ( 1 ) . Project ( p => p . Price ) . Single (); var qProductMax = productsCollection . Find ( p => p . Price == maxPrice ) . ToList (); foreach ( var t in qProductMax ) Console . WriteLine ( $\"\\t\\tName={t.Name}\\tPrice={t.Price}\" ); This exercise is complicated with our current database scheme because we do not have everything at hand within one collection. We need the product information from one collection, and the order details from another one. We will be doing a \"join\" in the client-side, that is, in C# code. The solution's outline is to query the orders, then in C# gather the orders by product, and finally, query the product details. // 1.5 Console . WriteLine ( \"\\t1.5:\" ); var qOrders = ordersCollection . Find ( _ => true ) . ToList (); var productOrders = qOrders . SelectMany ( o => o . OrderItems ) // All order items into one list . GroupBy ( oi => oi . ProductID ) . Where ( p => p . Count () >= 2 ); var qProducts = productsCollection . Find ( _ => true ) . ToList (); var productLookup = qProducts . ToDictionary ( p => p . ID ); foreach ( var p in productOrders ) { var product = productLookup . GetValueOrDefault ( p . Key ); Console . WriteLine ( $\"\\t\\tName={product?.Name}\\tStock={product?.Stock}\\tOrders={p.Count()}\" ); } This solution is very elegant and works only for small databases. Suppose we face a similar task under real-life circumstances. We have two choices: denormalize the database scheme and copy product details into the orders, or create an aggregation pipeline executed by the server that does something similar to the code above (MongoDB can do that, but it will not be very fast). Exercise 2: Create a new entity class \u00b6 Examine the classes Product and VAT . Why is there a field with a [BsonId] attribute in class Product and not in class VAT ? Create a new entity class for mapping Category document, then add and initialize a IMongoCollection<Category> field next to the others. Solution Class Product represents the products collection; therefore each item has a unique ObjectID . On the other hand, class VAT is an embedded field used by Product and has no collection on its own; hence it needs no id. Create our new Category POCO class. Let us check a few sample documents using Robo3T in categories collection. Create a new class Category in folder Entities with matching fields as below. using MongoDB.Bson ; using MongoDB.Bson.Serialization.Attributes ; namespace BME.DataDriven.Mongo.Entitites { public class Category { [BsonId] public ObjectId ID { get ; set ; } public string Name { get ; set ; } public ObjectId ? ParentCategoryID { get ; set ; } } } Add a new collection interface field in Program.cs as follows. private static IMongoCollection < Category > categoriesCollection ; And assign the value in the initialize method to get the collection. categoriesCollection = database . GetCollection < Category >( \"categories\" ); Exercise 3: Data modification \u00b6 The collection classes IMongoColection<TEntity> can also be used to execute modification operations. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Delete all categories that contain no products. Solution Find the ID of the category then update all products that have this category id. Console . WriteLine ( \"***** Exercise three *****\" ); //3.1 Console . WriteLine ( \"\\t3.1:\" ); var categoryLegoId = categoriesCollection . Find ( c => c . Name == \"LEGO\" ) . Project ( c => c . ID ) . Single (); var qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tBefore modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); productsCollection . UpdateMany ( filter : p => p . CategoryID == categoryLegoId , update : Builders < Product >. Update . Mul ( p => p . Price , 1.1 )); qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tAfter modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); MongoDB can execute the following sequence of steps in a single atomic step: \"Get me category Expensive toys . If it does not exist, create it.\" We will use FindOneAndUpdate to achieve this. //3.2 Console . WriteLine ( \"\\t3.2:\" ); var catExpensiveToys = categoriesCollection . FindOneAndUpdate < Category >( filter : c => c . Name == \"Expensive toys\" , update : Builders < Category >. Update . SetOnInsert ( c => c . Name , \"Expensive toys\" ), options : new FindOneAndUpdateOptions < Category , Category > { IsUpsert = true , ReturnDocument = ReturnDocument . After }); productsCollection . UpdateMany ( filter : p => p . Price > 8000 , update : Builders < Product >. Update . Set ( p => p . CategoryID , catExpensiveToys . ID )); var qProdExpensive = productsCollection . Find ( p => p . CategoryID == catExpensiveToys . ID ) . ToList (); foreach ( var p in qProdExpensive ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tPrice={p.Price}\" ); Query categories that contain any product, then delete the ones that do not belong among this list. //3.3 Console . WriteLine ( \"\\t3.3:\" ); Console . WriteLine ( $\"\\t\\tBefore modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); var qProductCategory = new HashSet < ObjectId >( productsCollection . Find ( _ => true ) . Project ( p => p . CategoryID ) . ToList ()); categoriesCollection . DeleteMany ( c => ! qProductCategory . Contains ( c . ID )); Console . WriteLine ( $\"\\t\\tAfter modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); Let us note that this is not an atomic operation. If a product was added concurrently, we could have deleted its category.","title":"MongoDB"},{"location":"seminar/mongodb/#mongodb","text":"The seminar's goal is to understand the concepts of the MongoDB document database and the usage of the MongoDB C#/.NET Driver .","title":"MongoDB"},{"location":"seminar/mongodb/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2022 ( not VS Code) MongoDB Community Edition Robo 3T Database initialization script: mongo.js Starter code: https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo Recommended to review: C# language and Linq queries MongoDB lecture Using MongoDB guide","title":"Pre-requisites"},{"location":"seminar/mongodb/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/mongodb/#exercise-0-create-database-open-starter-code","text":"Open a PowerShell console (search for PowerShell in the Start menu and start it, but not the one with \"ISE\" in the same - that is not the console). Copy and paste the script into the console and run it by pressing enter. Please note, that you might need to change the the directory name in the last command, e.g., if the app version is different. Remove-Item c :\\ work \\ mongodatabase -Recurse -ErrorAction Ignore New-Item -Type Directory c :\\ work \\ mongodatabase c :\\ tools \\ mongodb-win32-x86_64-windows - 4 . 4 . 10 \\ bin \\ mongod . exe - -dbpath c :\\ work \\ mongodatabase Keep this window open because the server is running here. You can stop it by pressing Ctrl+C at the end of the class. Launch Robo3, which you find in directory c:\\tools\\robo3t... and connect to the MongoDB server. Let us create a new database by right-clicking the connection ( localhost ). Let the database name be datadriven . Open a new shell into the database by right-clicking it and choosing the Open Shell command. This will open a single-line textbox to the right. Copy the database initialization script from here into the textbox and execute it by pressing the green play button on the toolbar. This will create our collections - expand the Collections item to check them. Download the starter solution! Open a new command prompt or PowerShell console (do not use the one the server is running in) Navigate to a folder, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo.git Open the sln file from the newly created folder using Visual Studio. Let us examine this project. This is a .NET console application. The structure resembles the structure of the Entity Framework project seen before: directory Entities contains the database entities while our code will be written into Program.cs . Program.cs already contains the initialization of the connection to MongoDB. Interface IMongoClient is used for all communication with the database. We will not use this directly. Interface IMongoDatabase represents the database datadriven within the MongoDB server. And the IMongoCollection<TEntity> interfaces represent the specific collections we can use to execute queries and modification commands. The database documents are mapped to the C# entity classes in folder Entities . A major difference compared to the behavior previously seen in Entity Framework is that these classes were not generated by written manually. Most entities are already mapped. We will create one more class during an exercise.","title":"Exercise 0: Create database, open starter code"},{"location":"seminar/mongodb/#exercise-1-queries","text":"Write C# code using the MongoDB C#/.NET Driver in the following exercises. Print the results to the console. List the names and the amount of stock of all products that we have more than 30 in stock! List the orders that consist of at least two items! List the orders that have a total value of at least 30.000! For each order, print the customer name, and list all items of the order (with the product name, amount, and price). Find the most expensive product! List the products that have been ordered at least twice! Solution We need only the product collection and execute a simple query. The filter criteria can be written as a Lambda-expression and with the builder syntax too. Console . WriteLine ( \"***** Exercise one *****\" ); // 1.1 first solution Console . WriteLine ( \"\\t1.1 First solution:\" ); var qProductAndStock1 = productsCollection . Find ( p => p . Stock > 30 ) . ToList (); foreach ( var p in qProductAndStock1 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); // 1.1 second solution Console . WriteLine ( \"\\t1.1 Second solution:\" ); var qProductAndStock2 = productsCollection . Find ( Builders < Product >. Filter . Gt ( p => p . Stock , 30 )) . ToList (); foreach ( var p in qProductAndStock2 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); This is similar to the previous one. We may note that we would have needed a join in a relational database, but we have everything at hand here. // 1.2 first solution Console . WriteLine ( \"\\t1.2 First solution:\" ); var qOrderItems1 = ordersCollection . Find ( o => o . OrderItems . Length >= 2 ) . ToList (); foreach ( var o in qOrderItems1 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); // 1.2 second solution Console . WriteLine ( \"\\t1.2 Second solution:\" ); var qOrderItems2 = ordersCollection . Find ( Builders < Order >. Filter . SizeGte ( o => o . OrderItems , 2 )) . ToList (); foreach ( var o in qOrderItems2 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); A simple query is not sufficient for this exercise; thus, we need the aggregation pipeline. We may still note that every information we need is still available in one collection. // 1.3 Console . WriteLine ( \"\\t1.3:\" ); var qOrderTotal = ordersCollection . Aggregate () . Project ( order => new { CustomerID = order . CustomerID , OrderItems = order . OrderItems , Total = order . OrderItems . Sum ( oi => oi . Amount * oi . Price ) }) . Match ( order => order . Total > 30000 ) . ToList (); foreach ( var o in qOrderTotal ) { Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\" ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( $\"\\t\\t\\tProductID={oi.ProductID}\\tPrice={oi.Price}\\tAmount={oi.Amount}\" ); } To find the most expensive product, we need two queries: first, find the largest price value, then find the products with this price. // 1.4 Console . WriteLine ( \"\\t1.4:\" ); var maxPrice = productsCollection . Find ( _ => true ) . SortByDescending ( p => p . Price ) . Limit ( 1 ) . Project ( p => p . Price ) . Single (); var qProductMax = productsCollection . Find ( p => p . Price == maxPrice ) . ToList (); foreach ( var t in qProductMax ) Console . WriteLine ( $\"\\t\\tName={t.Name}\\tPrice={t.Price}\" ); This exercise is complicated with our current database scheme because we do not have everything at hand within one collection. We need the product information from one collection, and the order details from another one. We will be doing a \"join\" in the client-side, that is, in C# code. The solution's outline is to query the orders, then in C# gather the orders by product, and finally, query the product details. // 1.5 Console . WriteLine ( \"\\t1.5:\" ); var qOrders = ordersCollection . Find ( _ => true ) . ToList (); var productOrders = qOrders . SelectMany ( o => o . OrderItems ) // All order items into one list . GroupBy ( oi => oi . ProductID ) . Where ( p => p . Count () >= 2 ); var qProducts = productsCollection . Find ( _ => true ) . ToList (); var productLookup = qProducts . ToDictionary ( p => p . ID ); foreach ( var p in productOrders ) { var product = productLookup . GetValueOrDefault ( p . Key ); Console . WriteLine ( $\"\\t\\tName={product?.Name}\\tStock={product?.Stock}\\tOrders={p.Count()}\" ); } This solution is very elegant and works only for small databases. Suppose we face a similar task under real-life circumstances. We have two choices: denormalize the database scheme and copy product details into the orders, or create an aggregation pipeline executed by the server that does something similar to the code above (MongoDB can do that, but it will not be very fast).","title":"Exercise 1: Queries"},{"location":"seminar/mongodb/#exercise-2-create-a-new-entity-class","text":"Examine the classes Product and VAT . Why is there a field with a [BsonId] attribute in class Product and not in class VAT ? Create a new entity class for mapping Category document, then add and initialize a IMongoCollection<Category> field next to the others. Solution Class Product represents the products collection; therefore each item has a unique ObjectID . On the other hand, class VAT is an embedded field used by Product and has no collection on its own; hence it needs no id. Create our new Category POCO class. Let us check a few sample documents using Robo3T in categories collection. Create a new class Category in folder Entities with matching fields as below. using MongoDB.Bson ; using MongoDB.Bson.Serialization.Attributes ; namespace BME.DataDriven.Mongo.Entitites { public class Category { [BsonId] public ObjectId ID { get ; set ; } public string Name { get ; set ; } public ObjectId ? ParentCategoryID { get ; set ; } } } Add a new collection interface field in Program.cs as follows. private static IMongoCollection < Category > categoriesCollection ; And assign the value in the initialize method to get the collection. categoriesCollection = database . GetCollection < Category >( \"categories\" );","title":"Exercise 2: Create a new entity class"},{"location":"seminar/mongodb/#exercise-3-data-modification","text":"The collection classes IMongoColection<TEntity> can also be used to execute modification operations. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Delete all categories that contain no products. Solution Find the ID of the category then update all products that have this category id. Console . WriteLine ( \"***** Exercise three *****\" ); //3.1 Console . WriteLine ( \"\\t3.1:\" ); var categoryLegoId = categoriesCollection . Find ( c => c . Name == \"LEGO\" ) . Project ( c => c . ID ) . Single (); var qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tBefore modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); productsCollection . UpdateMany ( filter : p => p . CategoryID == categoryLegoId , update : Builders < Product >. Update . Mul ( p => p . Price , 1.1 )); qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tAfter modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); MongoDB can execute the following sequence of steps in a single atomic step: \"Get me category Expensive toys . If it does not exist, create it.\" We will use FindOneAndUpdate to achieve this. //3.2 Console . WriteLine ( \"\\t3.2:\" ); var catExpensiveToys = categoriesCollection . FindOneAndUpdate < Category >( filter : c => c . Name == \"Expensive toys\" , update : Builders < Category >. Update . SetOnInsert ( c => c . Name , \"Expensive toys\" ), options : new FindOneAndUpdateOptions < Category , Category > { IsUpsert = true , ReturnDocument = ReturnDocument . After }); productsCollection . UpdateMany ( filter : p => p . Price > 8000 , update : Builders < Product >. Update . Set ( p => p . CategoryID , catExpensiveToys . ID )); var qProdExpensive = productsCollection . Find ( p => p . CategoryID == catExpensiveToys . ID ) . ToList (); foreach ( var p in qProdExpensive ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tPrice={p.Price}\" ); Query categories that contain any product, then delete the ones that do not belong among this list. //3.3 Console . WriteLine ( \"\\t3.3:\" ); Console . WriteLine ( $\"\\t\\tBefore modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); var qProductCategory = new HashSet < ObjectId >( productsCollection . Find ( _ => true ) . Project ( p => p . CategoryID ) . ToList ()); categoriesCollection . DeleteMany ( c => ! qProductCategory . Contains ( c . ID )); Console . WriteLine ( $\"\\t\\tAfter modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); Let us note that this is not an atomic operation. If a product was added concurrently, we could have deleted its category.","title":"Exercise 3: Data modification"},{"location":"seminar/mssql/","text":"Microsoft SQL Server programming \u00b6 The seminar's goal is to get to know the server-side programming capabilities of the Microsoft SQL Server platform. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: SQL language Microsoft SQL Server programming (stored procedures, triggers) Microsoft SQL Server usage guide How to work during the seminar \u00b6 The first four exercises are solved together with the instructor. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: SQL commands (review) \u00b6 Write SQL commands/queries for the following exercises. How many uncompleted orders are there (look for a status other than \"Delivered\")? Solution select count ( * ) from [ Order ] o join Status s on o . StatusID = s . ID where s . Name != 'Delivered' We see a join and an aggregation here. (There are other syntaxes for joining tables; refer to the lecture notes.) Which payment methods have not been used at all? Solution select p . Method from [ Order ] o right outer join PaymentMethod p on o . PaymentMethodID = p . ID where o . ID is null The key in the solution is the outer join , through which we can see the payment method records that have no orders. Let us insert a new customer and query the auto-assigned primary key! Solution insert into Customer ( Name , Login , Password , Email ) values ( 'Test Test' , 'tt' , '********' , 'tt@email.com' ) select @@ IDENTITY It is recommended (though not required) to name the columns after insert to be unambiguous. No value was assigned to the ID column, as the definition of that column mandates that the database automatically assigns a new value upon insert. We can query this ID after the insert is completed. One of the categories has the wrong name. Let us change Tricycle to Tricycles ! Solution update Category set Name = 'Tricycles' where Name = 'Tricycle' Which category contains the largest number of products? Solution select top 1 Name , ( select count ( * ) from Product where Product . CategoryID = c . ID ) as cnt from Category c order by cnt desc There are many ways this query can be formulated. This is only one possible solution. It also serves as an example of the usage of subqueries. Exercise 2: Inserting a new product category \u00b6 Create a new stored procedure that helps inserting a new product category. The procedure's inputs are the name of the new category, and optionally the name of the parent category. Raise an error if the category already exists, or the parent category does not exist. Let the database generate the primary key for the insertion. Solution Stored procedure create or alter procedure AddNewCategory @ Name nvarchar ( 50 ), @ ParentName nvarchar ( 50 ) as begin tran -- Is there a category with identical name? declare @ ID int select @ ID = ID from Category with ( TABLOCKX ) where upper ( Name ) = upper ( @ Name ) if @ ID is not null begin rollback raiserror ( 'Category %s already exists' , 16 , 1 , @ Name ) return end declare @ ParentID int if @ ParentName is not null begin select @ ParentID = ID from Category where upper ( Name ) = upper ( @ ParentName ) if @ ParentID is null begin rollback raiserror ( 'Category %s does not exist' , 16 , 1 , @ ParentName ) return end end insert into Category values ( @ Name , @ ParentID ) commit Testing Let us open a new Query window and execute the following testing instructions. exec AddNewCategory 'Beach balls', NULL This shall succeed. Let us verify the table contents afterward. Let us repeat the same command; it shall fail now. We can also try with a parent category. exec AddNewCategory 'LEGO Star Wars', 'LEGO' Exercise 3: Maintenance of order status \u00b6 Create a trigger that updates the status of each item of an order when the status of the order changes. Do this only for those items of the order that have the same status the order had before the change. Other items in the order should not be affected. Solution Trigger create or alter trigger UpdateOrderStatus on [ Order ] for update as update OrderItem set StatusID = i . StatusID from OrderItem oi inner join inserted i on i . Id = oi . OrderID inner join deleted d on d . ID = oi . OrderID where i . StatusID != d . StatusID and oi . StatusID = d . StatusID Let us make sure we understand the update ... from syntax. The behavior is as follows. We use this command when some of the changes we want to make during the update require data from another table. The syntax is based on the usual update ... set... format extended with a from part, which follows the same syntax as a select from , including the join to gather information from other tables. This allows us to use the joined records and their content in the set statement (that is, a value from a joined record can be on the right side of an assignment). Testing Let us check the status of the order and each item in the order: select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Let us change the status of the order: update [ Order ] set StatusID = 4 where ID = 1 Check the status now (the update should have updated all stauses): select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Exercise 4: Summing the total purchases of a customer \u00b6 Let us calculate and store the value of all purchases made by a customer! Add a new column to the table: alter table Customer add Total float Calculate the current totals. Let us use a cursor for iterating through all customers. Solution declare cur_customer cursor for select ID from Customer declare @ CustomerId int declare @ Total float open cur_customer fetch next from cur_customer into @ CustomerId while @@ FETCH_STATUS = 0 begin select @ Total = sum ( oi . Amount * oi . Price ) from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join OrderItem oi on oi . OrderID = o . ID where s . CustomerID = @ CustomerId update Customer set Total = ISNULL ( @ Total , 0 ) where ID = @ CustomerId fetch next from cur_customer into @ CustomerId end close cur_customer deallocate cur_customer To verify check the contents of the Customer table. Exercise 5: Maintenance of the total value (individual exercise) \u00b6 The values calculated in the previous exercise contain the current state. Create a trigger that updates this value whenever a related order is changed. Instead of re-calculating the value, update it with the changes made! Solution The key in the solution is recognizing which table the trigger should be placed on. We are interested in changes in order, but the total value actually depends on the items registered for an order; thus the trigger should react to changes in the order items. The exercise is complicated because the inserted and deleted tables may contain multiple records, possibly even related to multiple customers. A solution for overcoming this obstacle is to use a cursor to process all changes; another option, as below, is aggregating the changes by customer. Trigger create or alter trigger CustomerTotalUpdate on OrderItem for insert , update , delete as update Customer set Total = isnull ( Total , 0 ) + TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join inserted i on i . OrderID = o . ID group by s . CustomerId ) CustomerChange on Customer . ID = CustomerChange . CustomerId update Customer set Total = isnull ( Total , 0 ) - TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join deleted d on d . OrderID = o . ID group by s . CustomerID ) CustomerChange on Customer . ID = CustomerChange . CustomerId Testing Let us remember the total for the customers. select ID , Total from Customer Change the ordered amount. update OrderItem set Amount = 3 where ID = 1 Check the totals again, should have changed. select ID , Total from Customer","title":"Microsoft SQL Server programming"},{"location":"seminar/mssql/#microsoft-sql-server-programming","text":"The seminar's goal is to get to know the server-side programming capabilities of the Microsoft SQL Server platform.","title":"Microsoft SQL Server programming"},{"location":"seminar/mssql/#pre-requisites","text":"Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: SQL language Microsoft SQL Server programming (stored procedures, triggers) Microsoft SQL Server usage guide","title":"Pre-requisites"},{"location":"seminar/mssql/#how-to-work-during-the-seminar","text":"The first four exercises are solved together with the instructor. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/mssql/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/mssql/#exercise-1-sql-commands-review","text":"Write SQL commands/queries for the following exercises. How many uncompleted orders are there (look for a status other than \"Delivered\")? Solution select count ( * ) from [ Order ] o join Status s on o . StatusID = s . ID where s . Name != 'Delivered' We see a join and an aggregation here. (There are other syntaxes for joining tables; refer to the lecture notes.) Which payment methods have not been used at all? Solution select p . Method from [ Order ] o right outer join PaymentMethod p on o . PaymentMethodID = p . ID where o . ID is null The key in the solution is the outer join , through which we can see the payment method records that have no orders. Let us insert a new customer and query the auto-assigned primary key! Solution insert into Customer ( Name , Login , Password , Email ) values ( 'Test Test' , 'tt' , '********' , 'tt@email.com' ) select @@ IDENTITY It is recommended (though not required) to name the columns after insert to be unambiguous. No value was assigned to the ID column, as the definition of that column mandates that the database automatically assigns a new value upon insert. We can query this ID after the insert is completed. One of the categories has the wrong name. Let us change Tricycle to Tricycles ! Solution update Category set Name = 'Tricycles' where Name = 'Tricycle' Which category contains the largest number of products? Solution select top 1 Name , ( select count ( * ) from Product where Product . CategoryID = c . ID ) as cnt from Category c order by cnt desc There are many ways this query can be formulated. This is only one possible solution. It also serves as an example of the usage of subqueries.","title":"Exercise 1: SQL commands (review)"},{"location":"seminar/mssql/#exercise-2-inserting-a-new-product-category","text":"Create a new stored procedure that helps inserting a new product category. The procedure's inputs are the name of the new category, and optionally the name of the parent category. Raise an error if the category already exists, or the parent category does not exist. Let the database generate the primary key for the insertion. Solution Stored procedure create or alter procedure AddNewCategory @ Name nvarchar ( 50 ), @ ParentName nvarchar ( 50 ) as begin tran -- Is there a category with identical name? declare @ ID int select @ ID = ID from Category with ( TABLOCKX ) where upper ( Name ) = upper ( @ Name ) if @ ID is not null begin rollback raiserror ( 'Category %s already exists' , 16 , 1 , @ Name ) return end declare @ ParentID int if @ ParentName is not null begin select @ ParentID = ID from Category where upper ( Name ) = upper ( @ ParentName ) if @ ParentID is null begin rollback raiserror ( 'Category %s does not exist' , 16 , 1 , @ ParentName ) return end end insert into Category values ( @ Name , @ ParentID ) commit Testing Let us open a new Query window and execute the following testing instructions. exec AddNewCategory 'Beach balls', NULL This shall succeed. Let us verify the table contents afterward. Let us repeat the same command; it shall fail now. We can also try with a parent category. exec AddNewCategory 'LEGO Star Wars', 'LEGO'","title":"Exercise 2: Inserting a new product category"},{"location":"seminar/mssql/#exercise-3-maintenance-of-order-status","text":"Create a trigger that updates the status of each item of an order when the status of the order changes. Do this only for those items of the order that have the same status the order had before the change. Other items in the order should not be affected. Solution Trigger create or alter trigger UpdateOrderStatus on [ Order ] for update as update OrderItem set StatusID = i . StatusID from OrderItem oi inner join inserted i on i . Id = oi . OrderID inner join deleted d on d . ID = oi . OrderID where i . StatusID != d . StatusID and oi . StatusID = d . StatusID Let us make sure we understand the update ... from syntax. The behavior is as follows. We use this command when some of the changes we want to make during the update require data from another table. The syntax is based on the usual update ... set... format extended with a from part, which follows the same syntax as a select from , including the join to gather information from other tables. This allows us to use the joined records and their content in the set statement (that is, a value from a joined record can be on the right side of an assignment). Testing Let us check the status of the order and each item in the order: select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Let us change the status of the order: update [ Order ] set StatusID = 4 where ID = 1 Check the status now (the update should have updated all stauses): select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1","title":"Exercise 3: Maintenance of order status"},{"location":"seminar/mssql/#exercise-4-summing-the-total-purchases-of-a-customer","text":"Let us calculate and store the value of all purchases made by a customer! Add a new column to the table: alter table Customer add Total float Calculate the current totals. Let us use a cursor for iterating through all customers. Solution declare cur_customer cursor for select ID from Customer declare @ CustomerId int declare @ Total float open cur_customer fetch next from cur_customer into @ CustomerId while @@ FETCH_STATUS = 0 begin select @ Total = sum ( oi . Amount * oi . Price ) from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join OrderItem oi on oi . OrderID = o . ID where s . CustomerID = @ CustomerId update Customer set Total = ISNULL ( @ Total , 0 ) where ID = @ CustomerId fetch next from cur_customer into @ CustomerId end close cur_customer deallocate cur_customer To verify check the contents of the Customer table.","title":"Exercise 4: Summing the total purchases of a customer"},{"location":"seminar/mssql/#exercise-5-maintenance-of-the-total-value-individual-exercise","text":"The values calculated in the previous exercise contain the current state. Create a trigger that updates this value whenever a related order is changed. Instead of re-calculating the value, update it with the changes made! Solution The key in the solution is recognizing which table the trigger should be placed on. We are interested in changes in order, but the total value actually depends on the items registered for an order; thus the trigger should react to changes in the order items. The exercise is complicated because the inserted and deleted tables may contain multiple records, possibly even related to multiple customers. A solution for overcoming this obstacle is to use a cursor to process all changes; another option, as below, is aggregating the changes by customer. Trigger create or alter trigger CustomerTotalUpdate on OrderItem for insert , update , delete as update Customer set Total = isnull ( Total , 0 ) + TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join inserted i on i . OrderID = o . ID group by s . CustomerId ) CustomerChange on Customer . ID = CustomerChange . CustomerId update Customer set Total = isnull ( Total , 0 ) - TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join deleted d on d . OrderID = o . ID group by s . CustomerID ) CustomerChange on Customer . ID = CustomerChange . CustomerId Testing Let us remember the total for the customers. select ID , Total from Customer Change the ordered amount. update OrderItem set Amount = 3 where ID = 1 Check the totals again, should have changed. select ID , Total from Customer","title":"Exercise 5: Maintenance of the total value (individual exercise)"},{"location":"seminar/rest/","text":"REST API & ASP.NET Web API \u00b6 The seminar's goal is to practice working with REST APIs and the .NET Web API technology. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2022 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Postman: https://www.getpostman.com/downloads/ Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-rest-kiindulo Recommended to review: C# language Entity Framework and Linq REST API and Web API lecture How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: Open starter project \u00b6 Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-rest-kiindulo.git Open the sln file in the rest folder using Visual Studio. Let us examine this project. This is an ASP.NET Core Web API project. This project is created for hosting REST API backends. It contains a web server internally; thus, when running it using F5 we get a fully functional API able to respond to http requests. Let us examine Program.cs . We do not need to understand everything here. This is like a console application; the Main method here, the entry point that starts a web server. The Entity Framework Code First mapping of our database is in the Dal folder. Class DataDrivenDbContext is the data access class. We need to fix the connection string in the OnConfiguring method in this class. The connection string usually should not be hard-wired in the source code. This is for the sake of simplicity here. There is a test controller in folder Controllers . Let us open and examine the code. Let us note the [ApiController] and [Route] attributes and the inheritance. These make a class a Web API controller . The behavior is automatic: the controller's methods are invoked by the framework when they match the expected signature. This means that no additional configuration is needed here. Start the application. After building the source code, a console application will start where we will see diagnostic messages. Let us open a browser and navigate to http://localhost:5000/api/values . We should receive a JSON response. Stop the application by pressing Ctrl-C in the console, or stop with Visual Studio. Exercise 2: First controller and testing with Postman \u00b6 Create a new Web API controller that responds with a greeting. Test the behavior using Postman. Delete the existing class ValuesController . Add a new empty Api Controller with the name HelloController : in Solution Explorer right-click the Controllers folder and choose Add / Controller... / API Controller - Empty . The HelloController should respond to URL /api/hello . The application shall respond with a text when GET request is received. Test this endpoint using Postman by sending a GET request to http://localhost:5000/api/hello . Change the REST endpoint by expecting an optional name as a query parameter ; if such value is provided, the response greeting should include this name. Test this with Postman: send a name by calling URL http://localhost:5000/api/hello?name=apple . Create a new REST API endpoint that responds to URL http://localhost:5000/api/hello/apple just like the previous one, but the name is in the path here. Solution [Route(\"api/hello\")] [ApiController] public class HelloController : ControllerBase { // 2. //[HttpGet] //public ActionResult<string> Hello() //{ // return \"Hello!\"; //} // 3. [HttpGet] public ActionResult < string > Hello ([ FromQuery ] string name ) { if ( string . IsNullOrEmpty ( name )) return \"Hello noname!\" ; else return \"Hello \" + name ; } // 4. [HttpGet] [Route(\"{personName}\")] // the liter inside {} in this route must match the parameter name public ActionResult < string > HelloRoute ( string personName ) { return \"Hello route \" + personName ; } } Let us summarize what we need to create a new WebAPI endpoint: Inherit from the ControllerBase class and add the [ApiController] attribute. Specify the URL route on the class or above the method (or on both) using the [Route] attribute. Define a method with the right signature (return value and parameters). Choose what type of http queries to respond to using one of the [Http*] attributes. Exercise 3: Product search API \u00b6 A real API does not return constant strings. Create an API for searching among the products of our webshop. Create a new controller. Enable listing products; 5 per page. Enable search based on the name. The data returned should not be the database entity; instead create a new DTO (data transfer object) class in a new folder called Models . Test the new endpoints. Solution // ********************************* // Models/Product.cs namespace BME.DataDriven.REST.Models { public class Product { public Product ( int id , string name , double? price , int? stock ) { Id = id ; Name = name ; Price = price ; Stock = stock ; } // Contains only the relevant data; e.g. the database foreign keys are of no use here. // Assignment only via the constructor; this makes it unambiguous // that this is a snapshot of information that cannot be modified. public int Id { get ; private set ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs using System.Linq ; using Microsoft.AspNetCore.Mvc ; namespace BME.DataDriven.REST.Controllers { [Route(\"api/products\")] // it is better to explicitly specify the url [ApiController] public class ProductsController : ControllerBase { private readonly Dal . DataDrivenDbContext dbContext ; // The database is obtained through the Dependency Injection service of the framework. // The DbContext is automatically disposed at the end of the request. public ProductsController ( Dal . DataDrivenDbContext dbContext ) { this . dbContext = dbContext ; } [HttpGet] public ActionResult < Models . Product []> List ([ FromQuery ] string search = null , [ FromQuery ] int from = 0 ) { IQueryable < Dal . Product > filteredList ; if ( string . IsNullOrEmpty ( search )) // no search yields all products filteredList = dbContext . Product ; else // search by name filteredList = dbContext . Product . Where ( p => p . Name . Contains ( search )); return filteredList . Skip ( from ) // paging: from which product . Take ( 5 ) // 5 items on one page . Select ( p => new Models . Product ( p . Id , p . Name , p . Price , p . Stock )) // db to dto conversion . ToArray (); // enforce evaluating the IQueryable - otherwise would yield an error } } } Let us note that we did not need to concern ourselves with JSON serialization. The API returns objects. The framework automatically handles the serialization. Paging is useful to limit the size of the response (and paging is also customary on UIs). Specifying a \u201cfrom\u201d is a simple and frequently used solution. The result of the method before the ToArray is an IQueryable . We may remember that the IQueryable does not contain the result; it is merely a descriptor of the query. If we had no ToArray , we would see an error. When the framework would begin the serialization to JSON, it would start iterating the query; but at this point, the database connection has already been closed. Therefore WebAPI endpoints should not return IEnumerable or IQueryable . Exercise 4: Editing products via the API \u00b6 Add the following functionality to our API: Fetch the data of a particular product specified by id at url /api/products/id . Update the name, price, and stock of a product. Add a new product (create a new DTO class for input that contains only the name, price, and stock). Delete a product by specifying the id. Test each endpoint! Inserting a new product you will need the following settings in Postman: POST request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"name\" : \"BME pen\" , \"price\" : 8900 , \"stock\" : 100 } Note: In our case the JSON data is deserialized into a newly introduced (see later) Models.NewProduct object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Updating a product you will need the following settings: PUT request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"ID\" : 10 , \"name\" : \"Silence for one hour\" , \"price\" : 440 , \"stock\" : 10 } Note: In our case the JSON data is deserialized into a Models.Product object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Make sure to check the headers of the response too! Update and insert should add the Location header. This header should contain the URL to fetch the record. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { public NewProduct ( string name , double? price , int? stock ) { Name = name ; Price = price ; Stock = stock ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { public class ProductsController : ControllerBase { // ... // GET api/products/id [HttpGet] [Route(\"{id}\")] public ActionResult < Models . Product > Get ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // expected response when an item is not found else return new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock ); // in case of success return the item itself } // PUT api/products/id [HttpPut] [Route(\"{id}\")] public ActionResult Modify ([ FromRoute ] int id , [ FromBody ] Models . Product updated ) { if ( id != updated . Id ) return BadRequest (); var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // modifications performed here dbProduct . Name = updated . Name ; dbProduct . Price = updated . Price ; dbProduct . Stock = updated . Stock ; // save to database dbContext . SaveChanges (); return NoContent (); // response 204 NoContent } // POST api/products [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , CategoryId = 1 , // not nice, temporary solution VatId = 1 // not nice, temporary solution }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } // DELETE api/products/id [HttpDelete] [Route(\"{id}\")] public ActionResult Delete ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); dbContext . Product . Remove ( dbProduct ); dbContext . SaveChanges (); return NoContent (); // successful delete is signaled with 204 NoContent (could be 200 OK as well if we included the entity) } } } Exercise 5: Add new product with category and VAT \u00b6 When creating the new product, we have to specify the category, as well as the value-added tax. Change the insert operation from before by allowing the category name and the tax percentage to be specified. Find the VAT and Category records based on the provided data, or create new records if needed. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { // ... // Also extend the constructor! // Important note: It's important how constructor parameters are named. // Our properties have private setters, and thanks to this json deserialization // maps JSON object field names to constructor parameter names (in a case // insensitive manner). public int VATPercentage { get ; private set ; } public string CategoryName { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { // ... [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = dbContext . Vat . FirstOrDefault ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = dbContext . Category . FirstOrDefault ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } } Exercise 6: Asynchronous controller method \u00b6 Let us refactor the previous exercise code for asynchronous execution, that is, let us use async-await . Asynchronous execution utilizes the execution threads of the server more efficiently while waiting for database operations. We can easily make our code asynchronous by relying on the asynchronous support of Entity Framework. Solution [HttpPost] public async Task < ActionResult > Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = await dbContext . Vat . FirstOrDefaultAsync ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = await dbContext . Category . FirstOrDefaultAsync ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); await dbContext . SaveChangesAsync (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } Let us see how simple this was. Entity Framework provides us the ...Async methods, and we only have to await them, and update the method signature a litte. Everything else is taken care of by the framework. Note. The async-await is a .NET frmaework feature supported by both ASP.NET Core and Entity Framework. It is also supported by a lot of other libraries as well.","title":"REST API & ASP.NET Web API"},{"location":"seminar/rest/#rest-api-aspnet-web-api","text":"The seminar's goal is to practice working with REST APIs and the .NET Web API technology.","title":"REST API &amp; ASP.NET Web API"},{"location":"seminar/rest/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2022 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Postman: https://www.getpostman.com/downloads/ Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-rest-kiindulo Recommended to review: C# language Entity Framework and Linq REST API and Web API lecture","title":"Pre-requisites"},{"location":"seminar/rest/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/rest/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/rest/#exercise-1-open-starter-project","text":"Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-rest-kiindulo.git Open the sln file in the rest folder using Visual Studio. Let us examine this project. This is an ASP.NET Core Web API project. This project is created for hosting REST API backends. It contains a web server internally; thus, when running it using F5 we get a fully functional API able to respond to http requests. Let us examine Program.cs . We do not need to understand everything here. This is like a console application; the Main method here, the entry point that starts a web server. The Entity Framework Code First mapping of our database is in the Dal folder. Class DataDrivenDbContext is the data access class. We need to fix the connection string in the OnConfiguring method in this class. The connection string usually should not be hard-wired in the source code. This is for the sake of simplicity here. There is a test controller in folder Controllers . Let us open and examine the code. Let us note the [ApiController] and [Route] attributes and the inheritance. These make a class a Web API controller . The behavior is automatic: the controller's methods are invoked by the framework when they match the expected signature. This means that no additional configuration is needed here. Start the application. After building the source code, a console application will start where we will see diagnostic messages. Let us open a browser and navigate to http://localhost:5000/api/values . We should receive a JSON response. Stop the application by pressing Ctrl-C in the console, or stop with Visual Studio.","title":"Exercise 1: Open starter project"},{"location":"seminar/rest/#exercise-2-first-controller-and-testing-with-postman","text":"Create a new Web API controller that responds with a greeting. Test the behavior using Postman. Delete the existing class ValuesController . Add a new empty Api Controller with the name HelloController : in Solution Explorer right-click the Controllers folder and choose Add / Controller... / API Controller - Empty . The HelloController should respond to URL /api/hello . The application shall respond with a text when GET request is received. Test this endpoint using Postman by sending a GET request to http://localhost:5000/api/hello . Change the REST endpoint by expecting an optional name as a query parameter ; if such value is provided, the response greeting should include this name. Test this with Postman: send a name by calling URL http://localhost:5000/api/hello?name=apple . Create a new REST API endpoint that responds to URL http://localhost:5000/api/hello/apple just like the previous one, but the name is in the path here. Solution [Route(\"api/hello\")] [ApiController] public class HelloController : ControllerBase { // 2. //[HttpGet] //public ActionResult<string> Hello() //{ // return \"Hello!\"; //} // 3. [HttpGet] public ActionResult < string > Hello ([ FromQuery ] string name ) { if ( string . IsNullOrEmpty ( name )) return \"Hello noname!\" ; else return \"Hello \" + name ; } // 4. [HttpGet] [Route(\"{personName}\")] // the liter inside {} in this route must match the parameter name public ActionResult < string > HelloRoute ( string personName ) { return \"Hello route \" + personName ; } } Let us summarize what we need to create a new WebAPI endpoint: Inherit from the ControllerBase class and add the [ApiController] attribute. Specify the URL route on the class or above the method (or on both) using the [Route] attribute. Define a method with the right signature (return value and parameters). Choose what type of http queries to respond to using one of the [Http*] attributes.","title":"Exercise 2: First controller and testing with Postman"},{"location":"seminar/rest/#exercise-3-product-search-api","text":"A real API does not return constant strings. Create an API for searching among the products of our webshop. Create a new controller. Enable listing products; 5 per page. Enable search based on the name. The data returned should not be the database entity; instead create a new DTO (data transfer object) class in a new folder called Models . Test the new endpoints. Solution // ********************************* // Models/Product.cs namespace BME.DataDriven.REST.Models { public class Product { public Product ( int id , string name , double? price , int? stock ) { Id = id ; Name = name ; Price = price ; Stock = stock ; } // Contains only the relevant data; e.g. the database foreign keys are of no use here. // Assignment only via the constructor; this makes it unambiguous // that this is a snapshot of information that cannot be modified. public int Id { get ; private set ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs using System.Linq ; using Microsoft.AspNetCore.Mvc ; namespace BME.DataDriven.REST.Controllers { [Route(\"api/products\")] // it is better to explicitly specify the url [ApiController] public class ProductsController : ControllerBase { private readonly Dal . DataDrivenDbContext dbContext ; // The database is obtained through the Dependency Injection service of the framework. // The DbContext is automatically disposed at the end of the request. public ProductsController ( Dal . DataDrivenDbContext dbContext ) { this . dbContext = dbContext ; } [HttpGet] public ActionResult < Models . Product []> List ([ FromQuery ] string search = null , [ FromQuery ] int from = 0 ) { IQueryable < Dal . Product > filteredList ; if ( string . IsNullOrEmpty ( search )) // no search yields all products filteredList = dbContext . Product ; else // search by name filteredList = dbContext . Product . Where ( p => p . Name . Contains ( search )); return filteredList . Skip ( from ) // paging: from which product . Take ( 5 ) // 5 items on one page . Select ( p => new Models . Product ( p . Id , p . Name , p . Price , p . Stock )) // db to dto conversion . ToArray (); // enforce evaluating the IQueryable - otherwise would yield an error } } } Let us note that we did not need to concern ourselves with JSON serialization. The API returns objects. The framework automatically handles the serialization. Paging is useful to limit the size of the response (and paging is also customary on UIs). Specifying a \u201cfrom\u201d is a simple and frequently used solution. The result of the method before the ToArray is an IQueryable . We may remember that the IQueryable does not contain the result; it is merely a descriptor of the query. If we had no ToArray , we would see an error. When the framework would begin the serialization to JSON, it would start iterating the query; but at this point, the database connection has already been closed. Therefore WebAPI endpoints should not return IEnumerable or IQueryable .","title":"Exercise 3: Product search API"},{"location":"seminar/rest/#exercise-4-editing-products-via-the-api","text":"Add the following functionality to our API: Fetch the data of a particular product specified by id at url /api/products/id . Update the name, price, and stock of a product. Add a new product (create a new DTO class for input that contains only the name, price, and stock). Delete a product by specifying the id. Test each endpoint! Inserting a new product you will need the following settings in Postman: POST request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"name\" : \"BME pen\" , \"price\" : 8900 , \"stock\" : 100 } Note: In our case the JSON data is deserialized into a newly introduced (see later) Models.NewProduct object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Updating a product you will need the following settings: PUT request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"ID\" : 10 , \"name\" : \"Silence for one hour\" , \"price\" : 440 , \"stock\" : 10 } Note: In our case the JSON data is deserialized into a Models.Product object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Make sure to check the headers of the response too! Update and insert should add the Location header. This header should contain the URL to fetch the record. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { public NewProduct ( string name , double? price , int? stock ) { Name = name ; Price = price ; Stock = stock ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { public class ProductsController : ControllerBase { // ... // GET api/products/id [HttpGet] [Route(\"{id}\")] public ActionResult < Models . Product > Get ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // expected response when an item is not found else return new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock ); // in case of success return the item itself } // PUT api/products/id [HttpPut] [Route(\"{id}\")] public ActionResult Modify ([ FromRoute ] int id , [ FromBody ] Models . Product updated ) { if ( id != updated . Id ) return BadRequest (); var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // modifications performed here dbProduct . Name = updated . Name ; dbProduct . Price = updated . Price ; dbProduct . Stock = updated . Stock ; // save to database dbContext . SaveChanges (); return NoContent (); // response 204 NoContent } // POST api/products [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , CategoryId = 1 , // not nice, temporary solution VatId = 1 // not nice, temporary solution }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } // DELETE api/products/id [HttpDelete] [Route(\"{id}\")] public ActionResult Delete ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); dbContext . Product . Remove ( dbProduct ); dbContext . SaveChanges (); return NoContent (); // successful delete is signaled with 204 NoContent (could be 200 OK as well if we included the entity) } } }","title":"Exercise 4: Editing products via the API"},{"location":"seminar/rest/#exercise-5-add-new-product-with-category-and-vat","text":"When creating the new product, we have to specify the category, as well as the value-added tax. Change the insert operation from before by allowing the category name and the tax percentage to be specified. Find the VAT and Category records based on the provided data, or create new records if needed. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { // ... // Also extend the constructor! // Important note: It's important how constructor parameters are named. // Our properties have private setters, and thanks to this json deserialization // maps JSON object field names to constructor parameter names (in a case // insensitive manner). public int VATPercentage { get ; private set ; } public string CategoryName { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { // ... [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = dbContext . Vat . FirstOrDefault ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = dbContext . Category . FirstOrDefault ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } }","title":"Exercise 5: Add new product with category and VAT"},{"location":"seminar/rest/#exercise-6-asynchronous-controller-method","text":"Let us refactor the previous exercise code for asynchronous execution, that is, let us use async-await . Asynchronous execution utilizes the execution threads of the server more efficiently while waiting for database operations. We can easily make our code asynchronous by relying on the asynchronous support of Entity Framework. Solution [HttpPost] public async Task < ActionResult > Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = await dbContext . Vat . FirstOrDefaultAsync ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = await dbContext . Category . FirstOrDefaultAsync ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); await dbContext . SaveChangesAsync (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } Let us see how simple this was. Entity Framework provides us the ...Async methods, and we only have to await them, and update the method signature a litte. Everything else is taken care of by the framework. Note. The async-await is a .NET frmaework feature supported by both ASP.NET Core and Entity Framework. It is also supported by a lot of other libraries as well.","title":"Exercise 6: Asynchronous controller method"},{"location":"seminar/transactions/","text":"Transactions \u00b6 The goal is to examine transaction handling of MS SQL Server, understand the practical limits to serializable isolation level, and controlling data dependency in read committed isolation level. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: Transaction properties, isolation levels Microsoft SQL Server usage guide How to work during the seminar \u00b6 The seminar is lead by the instructor. After getting to know the tools we use, the exercises are solved together. Experienced behavior is summarized and explained. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 1: Create a database in MS SQL Server \u00b6 We need a database first. Usually, the database is located on a central server, but we often run a server on our machine for development. Connect to Microsoft SQL Server using SQL Server Management Studio. Start Management Studio and use the following connection details: Server name: (localdb)\\mssqllocaldb Authentication: Windows authentication Create a new database (if it does not exist yet); the name should be your Neptun code: in Object Explorer right-click Databases and choose Create Database . Instantiate the sample database using the script. Open a new Query window, paste the script into the window, then execute it. Make sure to select the right database in the toolbar dropdown. Verify that the tables are created. If the Tables folder was open before, you need to refresh it. . Exercise 2: Concurrent transactions \u00b6 To simulate concurrent transactions, you need two Query windows by clicking the New Query button twice. You can align the windows next to each other by right-clicking the Query header and choosing New Vertical Tab group: Use the following scheduling. Transaction T1 checks the status of order 4, while transaction T2 changes the status. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID [Order] The brackets in [Order] are needed to distinguish it from the order by command. T2 transaction -- Change the status or the order update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Change the status of each item in the order update OrderItem set StatusID = 4 where OrderID = 4 T1 transaction : repeat the same command as in step 1 What did you experience? Why? In the beginning, everything was in status \"Packaged\", which is fine (the items in the order and the order itself had the same status). But after we changed the status of the order, the status seemed controversial: the order and the items had different statuses. We have to understand that the database itself was not inconsistent, as the database's integration requirements allow this. However, from a business perspective, there was an inconsistency. SQL Server, by default, runs in auto-commit mode. That is, every sql statement is a transaction by itself, which is committed when completed. Thus the problem was that our modifications were executed in separate transactions. In order to handle the two changes together, we would need to combine them into a single transaction. Exercise 3: using transactions, read committed isolation level \u00b6 Let us repeat the previous exercise so that the two modifications form a single transaction: T2 transaction should begin with a begin tran command, and finish with a commit statement. When changing the status, the new status should be 3 (to have an actual change in the data). What did you experience? Why? While data modification is underway in T2 , the query statement in T1 will wait. It will wait until the data modification transaction is completed. The select statement wants to place a reader lock on the records, but the other concurrent transaction is editing these records and has an exclusive writer lock on them. Let us remember that the default isolation level is read committed . This isolation level on this platform means that data under modification cannot be accessed, not even for reading. This is a matter of implementation; the SQL standard does not specify this (e.g., in Oracle Server the previously committed state of each record is available). In other isolation levels, MSSQL Server behaves differently; e.g., in the snapshot isolation level, the version of the data before the modification is accessible. Exercise 4: aborting transactions ( rollback ) in read committed isolation level \u00b6 Let us repeat the same command sequence, including the transaction, but let us abort the modification operation in the middle. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID T2 transaction -- Start new transaction begin tran -- Change the order status update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Abort the transaction rollback What did you experience? Why? Similarly to the previous exercise, the read operation was forced to wait while the modification transaction was underway. When this modification was aborted, the result of the read query arrived immediately. We are still using read committed isolation level; hence we must not see data being modified. But once the modification transaction finished, either successfully with commit or with a rollback , the data records are once again available. Let us understand that we have just avoided the problem of dirty read. If the read query showed us the uncommitted modification, we would have seen values that would have been invalid after the rollback . Exercise 5: Placing an order using serializable isolation level \u00b6 Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us have two concurrent transactions, both placing an order. We must allow an order for a product only if we have enough stock left. To properly isolate the effect of the transactions, let use serializable isolation level. T1 transaction set transaction isolation level serializable begin tran -- Check the product stock select * from Product where ID = 2 T2 transaction set transaction isolation level serializable begin tran select * from Product where ID = 2 T1 transaction -- Check the registered, but unprocessed orders for the same product select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T1 transaction -- Since the order can be completed, store the order insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 2 , 3 , 1 ) T1 transaction commit T2 transaction commit What did you experience? Why? A deadlock will occur due to the serializable isolation level, as both transactions require exclusive access to the table OrderItem . The query select sum - with the requirement to protect from unrepeatable reads - adds reader locks to the records. Thus the insert cannot complete, as it needs write access. This effectively means that both transactions are waiting for a lock that the other one holds. The result of the deadlock is the abortion of one of the transactions. This is the expected and correct behavior in these cases because it prevents the same problem we are trying to avoid (to sell more products than we have in stock). Let us repeat the same sequence of steps, but this time, the products should be different. This simulates two concurrent orders for different products. Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us replace the ID or ProductID values: one of the transactions should use ID 2 and the other one ID 3. What did you experience? Why? Even when the two orders reference different products a deadlock occurs. The locking mechanism behind the statement select sum locks the entire table, because it is not able to distinguish the records by ProductID . This is not unexpected, as the fact that two concurrent orders referencing different products are allowed, is a business requirement; the database has no knowledge of this. In other words, the serializable isolation level is too strict in this case. This is the reason that serializable is not frequently used in practice. Exercise 6: Order registration with read committed isolation level \u00b6 Let us consider what would happen in the previous exercise if the isolation level was left at default? Would there be any deadlock? Would the result be correct? What would we expect? Why? If we used the default isolation level, the behavior would be incorrect. The read committed isolation level would not protect us from a concurrent transaction inserting a new record that could potentially result in selling more products than available. This would be a manifestation of the unrepeatable read concurrency problem. We can conclude thus that the serializable isolation level was not unnecessary. It did in fact, protect us from a valid concurrency problem. Exercise 7: Manual locking \u00b6 Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Using read committed isolation level, let us find a solution that only prohibits concurrent orders of the same product . You can assume that all copies of the concurrent program run the same logic. The solution is based on manual locks we can place on records. These locks |(similarly to the automatic ones) have a lifespan identical to the transaction. select * from tablename with ( XLOCK ) ... Where do we place this lock? How does the order registration process look like? The key to this exercise is understanding where the lock should be placed. The question is what to lock. The answer is the product : we want to avoid placing orders of the same product. Thus we place a lock on the product record. The order process is as follows: T1 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 2 T2 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 3 T1 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 3 and StatusID = 1 T1 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 3 , 3 , 1 ) T1 transaction commit T2 transaction commit Exercise 8: Table locking \u00b6 There is another option for manual locking by locking entire tables: select * from tablename with ( TABLOCKX ) ... This might seem a simple solution, but why is this a not recommended option? In our scenario, the table lock should be placed on the order item table. But effectively, this would mean the same thing as using serializable isolation level: there would be no deadlocks; however, there would be no concurrency allowed either.","title":"Transactions"},{"location":"seminar/transactions/#transactions","text":"The goal is to examine transaction handling of MS SQL Server, understand the practical limits to serializable isolation level, and controlling data dependency in read committed isolation level.","title":"Transactions"},{"location":"seminar/transactions/#pre-requisites","text":"Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: Transaction properties, isolation levels Microsoft SQL Server usage guide","title":"Pre-requisites"},{"location":"seminar/transactions/#how-to-work-during-the-seminar","text":"The seminar is lead by the instructor. After getting to know the tools we use, the exercises are solved together. Experienced behavior is summarized and explained. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/transactions/#exercise-1-create-a-database-in-ms-sql-server","text":"We need a database first. Usually, the database is located on a central server, but we often run a server on our machine for development. Connect to Microsoft SQL Server using SQL Server Management Studio. Start Management Studio and use the following connection details: Server name: (localdb)\\mssqllocaldb Authentication: Windows authentication Create a new database (if it does not exist yet); the name should be your Neptun code: in Object Explorer right-click Databases and choose Create Database . Instantiate the sample database using the script. Open a new Query window, paste the script into the window, then execute it. Make sure to select the right database in the toolbar dropdown. Verify that the tables are created. If the Tables folder was open before, you need to refresh it. .","title":"Exercise 1: Create a database in MS SQL Server"},{"location":"seminar/transactions/#exercise-2-concurrent-transactions","text":"To simulate concurrent transactions, you need two Query windows by clicking the New Query button twice. You can align the windows next to each other by right-clicking the Query header and choosing New Vertical Tab group: Use the following scheduling. Transaction T1 checks the status of order 4, while transaction T2 changes the status. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID [Order] The brackets in [Order] are needed to distinguish it from the order by command. T2 transaction -- Change the status or the order update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Change the status of each item in the order update OrderItem set StatusID = 4 where OrderID = 4 T1 transaction : repeat the same command as in step 1 What did you experience? Why? In the beginning, everything was in status \"Packaged\", which is fine (the items in the order and the order itself had the same status). But after we changed the status of the order, the status seemed controversial: the order and the items had different statuses. We have to understand that the database itself was not inconsistent, as the database's integration requirements allow this. However, from a business perspective, there was an inconsistency. SQL Server, by default, runs in auto-commit mode. That is, every sql statement is a transaction by itself, which is committed when completed. Thus the problem was that our modifications were executed in separate transactions. In order to handle the two changes together, we would need to combine them into a single transaction.","title":"Exercise 2: Concurrent transactions"},{"location":"seminar/transactions/#exercise-3-using-transactions-read-committed-isolation-level","text":"Let us repeat the previous exercise so that the two modifications form a single transaction: T2 transaction should begin with a begin tran command, and finish with a commit statement. When changing the status, the new status should be 3 (to have an actual change in the data). What did you experience? Why? While data modification is underway in T2 , the query statement in T1 will wait. It will wait until the data modification transaction is completed. The select statement wants to place a reader lock on the records, but the other concurrent transaction is editing these records and has an exclusive writer lock on them. Let us remember that the default isolation level is read committed . This isolation level on this platform means that data under modification cannot be accessed, not even for reading. This is a matter of implementation; the SQL standard does not specify this (e.g., in Oracle Server the previously committed state of each record is available). In other isolation levels, MSSQL Server behaves differently; e.g., in the snapshot isolation level, the version of the data before the modification is accessible.","title":"Exercise 3: using transactions, read committed isolation level"},{"location":"seminar/transactions/#exercise-4-aborting-transactions-rollback-in-read-committed-isolation-level","text":"Let us repeat the same command sequence, including the transaction, but let us abort the modification operation in the middle. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID T2 transaction -- Start new transaction begin tran -- Change the order status update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Abort the transaction rollback What did you experience? Why? Similarly to the previous exercise, the read operation was forced to wait while the modification transaction was underway. When this modification was aborted, the result of the read query arrived immediately. We are still using read committed isolation level; hence we must not see data being modified. But once the modification transaction finished, either successfully with commit or with a rollback , the data records are once again available. Let us understand that we have just avoided the problem of dirty read. If the read query showed us the uncommitted modification, we would have seen values that would have been invalid after the rollback .","title":"Exercise 4: aborting transactions (rollback) in read committed isolation level"},{"location":"seminar/transactions/#exercise-5-placing-an-order-using-serializable-isolation-level","text":"Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us have two concurrent transactions, both placing an order. We must allow an order for a product only if we have enough stock left. To properly isolate the effect of the transactions, let use serializable isolation level. T1 transaction set transaction isolation level serializable begin tran -- Check the product stock select * from Product where ID = 2 T2 transaction set transaction isolation level serializable begin tran select * from Product where ID = 2 T1 transaction -- Check the registered, but unprocessed orders for the same product select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T1 transaction -- Since the order can be completed, store the order insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 2 , 3 , 1 ) T1 transaction commit T2 transaction commit What did you experience? Why? A deadlock will occur due to the serializable isolation level, as both transactions require exclusive access to the table OrderItem . The query select sum - with the requirement to protect from unrepeatable reads - adds reader locks to the records. Thus the insert cannot complete, as it needs write access. This effectively means that both transactions are waiting for a lock that the other one holds. The result of the deadlock is the abortion of one of the transactions. This is the expected and correct behavior in these cases because it prevents the same problem we are trying to avoid (to sell more products than we have in stock). Let us repeat the same sequence of steps, but this time, the products should be different. This simulates two concurrent orders for different products. Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us replace the ID or ProductID values: one of the transactions should use ID 2 and the other one ID 3. What did you experience? Why? Even when the two orders reference different products a deadlock occurs. The locking mechanism behind the statement select sum locks the entire table, because it is not able to distinguish the records by ProductID . This is not unexpected, as the fact that two concurrent orders referencing different products are allowed, is a business requirement; the database has no knowledge of this. In other words, the serializable isolation level is too strict in this case. This is the reason that serializable is not frequently used in practice.","title":"Exercise 5: Placing an order using serializable isolation level"},{"location":"seminar/transactions/#exercise-6-order-registration-with-read-committed-isolation-level","text":"Let us consider what would happen in the previous exercise if the isolation level was left at default? Would there be any deadlock? Would the result be correct? What would we expect? Why? If we used the default isolation level, the behavior would be incorrect. The read committed isolation level would not protect us from a concurrent transaction inserting a new record that could potentially result in selling more products than available. This would be a manifestation of the unrepeatable read concurrency problem. We can conclude thus that the serializable isolation level was not unnecessary. It did in fact, protect us from a valid concurrency problem.","title":"Exercise 6: Order registration with read committed isolation level"},{"location":"seminar/transactions/#exercise-7-manual-locking","text":"Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Using read committed isolation level, let us find a solution that only prohibits concurrent orders of the same product . You can assume that all copies of the concurrent program run the same logic. The solution is based on manual locks we can place on records. These locks |(similarly to the automatic ones) have a lifespan identical to the transaction. select * from tablename with ( XLOCK ) ... Where do we place this lock? How does the order registration process look like? The key to this exercise is understanding where the lock should be placed. The question is what to lock. The answer is the product : we want to avoid placing orders of the same product. Thus we place a lock on the product record. The order process is as follows: T1 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 2 T2 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 3 T1 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 3 and StatusID = 1 T1 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 3 , 3 , 1 ) T1 transaction commit T2 transaction commit","title":"Exercise 7: Manual locking"},{"location":"seminar/transactions/#exercise-8-table-locking","text":"There is another option for manual locking by locking entire tables: select * from tablename with ( TABLOCKX ) ... This might seem a simple solution, but why is this a not recommended option? In our scenario, the table lock should be placed on the order item table. But effectively, this would mean the same thing as using serializable isolation level: there would be no deadlocks; however, there would be no concurrency allowed either.","title":"Exercise 8: Table locking"}]}