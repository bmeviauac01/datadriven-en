{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data-driven systems \u00b6 Lecture notes, seminar materials and homework exercises for course BMEVIAUAC01 Data-driven systems. Pull requests welcome As a student of this course you can earn extra points by contributing to the materials! Open a pull request to fix an error or contribute to the materials in any way! Check the link to the repository in the upper right corner. License The materials provided here are created for the students of course BMEVIAUAC01. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. These materials are to be viewed within the context of the course. For any other usage scenarios the material is provided as-is.","title":"Data-driven systems"},{"location":"#data-driven-systems","text":"Lecture notes, seminar materials and homework exercises for course BMEVIAUAC01 Data-driven systems. Pull requests welcome As a student of this course you can earn extra points by contributing to the materials! Open a pull request to fix an error or contribute to the materials in any way! Check the link to the repository in the upper right corner. License The materials provided here are created for the students of course BMEVIAUAC01. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. These materials are to be viewed within the context of the course. For any other usage scenarios the material is provided as-is.","title":"Data-driven systems"},{"location":"db/","text":"Sample database scheme \u00b6 The examples and seminars during the semester will use a sample database. The database is a simplified retail management system with products, customers, and orders. This description details the relational schema of the database; the MongoDB variant is the appropriate mirror of this scheme. The context of the database \u00b6 The system is designed to help the retail process of products. The products are grouped into hierarchical categories . Customers can browse products, place orders , and track the status of the orders. Customers can have multiple sites (e.g., retail stores of one company with multiple addresses). The order can be completed to any of these sites. Each customer has exactly one \"main site,\" which is where the invoices are addressed. An order can have multiple items ; each item has its status. An invoice is printed if the order is ready. An invoice cannot be changed once it is created. Different products have different VAT (value-added tax) rates. These VAT rates are subject to change over time, but these changes must not affect existing invoices. Data scheme \u00b6 The model of the whole database is depicted below. Tables and columns \u00b6 Table Column Description VAT ID Auto-generated primary key. Percentage The percentage of the value-added tax. PaymentMethod ID Auto-generated primary key. Mode Short name of the payment method, e.g., cash or wire transfer. Deadline The deadline of the payment method, that is, the deadline for completing the transaction after the invoice is received. Status ID Auto-generated primary key. Name Short name of the status (e.g., new, processed). Category ID Auto-generated primary key. Name Name of the category, e.g., toys, LEGO, etc. ParentCategoryID Foreign key indicating the parent category; null if this is a top-level category. Product ID Auto-generated primary key. Name Product name. Price Product price without tax. Stock Amount of this product in stock. VATID Foreign key to the VAT table. CategoryID Foreign key to the Category table. Description XML description of the product. Customer ID Auto-generated primary key. Name Customer name. BankAccount Back account number of the customer. Login Login name for the webshop. Password Password for the webshop. Email Email address of the customer. MainCustomerSiteID The main site of the customer; a foreign key to the CustomerSite table. CustomerSite ID Auto-generated primary key. Zip The zip code of the address. City The city part of the address. Street The street and house number part of the address. Tel Telephone number. Fax Fax number. CustomerID Foreign key to the Customer table. Order ID Auto-generated primary key. Date Date when the order was placed. Deadline Deadline until the order must be completed. CustomerSiteID Foreign key to the CustomerSite table; the order is billed and shipped to this site. StatusID Foreign key to the Status table; the actual status of the order. PaymentMethodID Foreign key to the PaymentMethod table; the chosen method of payment. OrderItem ID Auto-generated primary key. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g. for bulk order). OrderID Foreign key to the Order table; identifier the order this item belongs to. ProductID Foreign key to the Product table; identifies the product that is ordered. StatusID Foreign key to the Status table; the actual status of the item. InvoiceIssuer ID Auto-generated primary key. Name Name of the company selling the products. Zip The zip code of the address. City The city part of the address. Street The street and house number part of the address. TaxIdentifier Tax identifier of the company. BankAccount Bank account number of the company. Invoice ID Auto-generated primary key. CustomerName The name of the customer; printed on the invoice. CustomerZipCode The zip code of the address. CustomerCity The city part of the address. CustomerStreet The street and house number part of the address. PrintedCopies Number of copies printed of this invoice. Cancelled Has the invoice been cancelled? PaymentMethod Payment method of the invoice (text name). CreationDate Date when the invoice was created. DeliveryDate Delivery date of the invoice. PaymentDeadline Deadline of the payment. InvoiceIssuerID Foreign key to the InvoiceIssuer table; the issuer of the invoice. OrderID Foreign key to the Order table; the order out of which this invoice was created. InvoiceItem ID Auto-generated primary key. Name Name of the product; printed on the invoice. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g., for bulk order). VATPercentage The effective percentage of the tax applied. InvoiceID Foreign key to the Invoice table; the invoice this item is part of. OrderItemID Foreign key to the OrderItem table; the item out of which this invoice item was created. Peculiarities \u00b6 Invoicing \u00b6 Invoices cannot be altered once issued; they can only be canceled. Therefore all data that appears on the order are copied into the Invoice and InvoiceItem tables once the invoice is created. There is only a single original copy of the invoice; therefore, the number of printed copies is recorded. Invoice issuer \u00b6 The issuer of the invoices changes very infrequently. However, in case it changes, existing invoices must remain unaltered. The issuer of the invoice is recorded in a separate table, and only one is in effect at all times. Each invoice references the right InvoiceIssuerId at the time. VAT \u00b6 The VAT percentage of products can change at any time. However, existing invoices must not be altered. Therefore the actual VAT percentage is stored with the invoice when created and not referenced from the VAT table. Product description \u00b6 Some products contain an additional XML description, such as the following example. <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <package_parameters> <number_of_packages> 1 </number_of_packages> <package_size> <unit> cm </unit> <width> 150 </width> <height> 20 </height> <depth> 20 </depth> </package_size> </package_parameters> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product>","title":"Sample database scheme"},{"location":"db/#sample-database-scheme","text":"The examples and seminars during the semester will use a sample database. The database is a simplified retail management system with products, customers, and orders. This description details the relational schema of the database; the MongoDB variant is the appropriate mirror of this scheme.","title":"Sample database scheme"},{"location":"db/#the-context-of-the-database","text":"The system is designed to help the retail process of products. The products are grouped into hierarchical categories . Customers can browse products, place orders , and track the status of the orders. Customers can have multiple sites (e.g., retail stores of one company with multiple addresses). The order can be completed to any of these sites. Each customer has exactly one \"main site,\" which is where the invoices are addressed. An order can have multiple items ; each item has its status. An invoice is printed if the order is ready. An invoice cannot be changed once it is created. Different products have different VAT (value-added tax) rates. These VAT rates are subject to change over time, but these changes must not affect existing invoices.","title":"The context of the database"},{"location":"db/#data-scheme","text":"The model of the whole database is depicted below.","title":"Data scheme"},{"location":"db/#tables-and-columns","text":"Table Column Description VAT ID Auto-generated primary key. Percentage The percentage of the value-added tax. PaymentMethod ID Auto-generated primary key. Mode Short name of the payment method, e.g., cash or wire transfer. Deadline The deadline of the payment method, that is, the deadline for completing the transaction after the invoice is received. Status ID Auto-generated primary key. Name Short name of the status (e.g., new, processed). Category ID Auto-generated primary key. Name Name of the category, e.g., toys, LEGO, etc. ParentCategoryID Foreign key indicating the parent category; null if this is a top-level category. Product ID Auto-generated primary key. Name Product name. Price Product price without tax. Stock Amount of this product in stock. VATID Foreign key to the VAT table. CategoryID Foreign key to the Category table. Description XML description of the product. Customer ID Auto-generated primary key. Name Customer name. BankAccount Back account number of the customer. Login Login name for the webshop. Password Password for the webshop. Email Email address of the customer. MainCustomerSiteID The main site of the customer; a foreign key to the CustomerSite table. CustomerSite ID Auto-generated primary key. Zip The zip code of the address. City The city part of the address. Street The street and house number part of the address. Tel Telephone number. Fax Fax number. CustomerID Foreign key to the Customer table. Order ID Auto-generated primary key. Date Date when the order was placed. Deadline Deadline until the order must be completed. CustomerSiteID Foreign key to the CustomerSite table; the order is billed and shipped to this site. StatusID Foreign key to the Status table; the actual status of the order. PaymentMethodID Foreign key to the PaymentMethod table; the chosen method of payment. OrderItem ID Auto-generated primary key. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g. for bulk order). OrderID Foreign key to the Order table; identifier the order this item belongs to. ProductID Foreign key to the Product table; identifies the product that is ordered. StatusID Foreign key to the Status table; the actual status of the item. InvoiceIssuer ID Auto-generated primary key. Name Name of the company selling the products. Zip The zip code of the address. City The city part of the address. Street The street and house number part of the address. TaxIdentifier Tax identifier of the company. BankAccount Bank account number of the company. Invoice ID Auto-generated primary key. CustomerName The name of the customer; printed on the invoice. CustomerZipCode The zip code of the address. CustomerCity The city part of the address. CustomerStreet The street and house number part of the address. PrintedCopies Number of copies printed of this invoice. Cancelled Has the invoice been cancelled? PaymentMethod Payment method of the invoice (text name). CreationDate Date when the invoice was created. DeliveryDate Delivery date of the invoice. PaymentDeadline Deadline of the payment. InvoiceIssuerID Foreign key to the InvoiceIssuer table; the issuer of the invoice. OrderID Foreign key to the Order table; the order out of which this invoice was created. InvoiceItem ID Auto-generated primary key. Name Name of the product; printed on the invoice. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g., for bulk order). VATPercentage The effective percentage of the tax applied. InvoiceID Foreign key to the Invoice table; the invoice this item is part of. OrderItemID Foreign key to the OrderItem table; the item out of which this invoice item was created.","title":"Tables and columns"},{"location":"db/#peculiarities","text":"","title":"Peculiarities"},{"location":"db/#invoicing","text":"Invoices cannot be altered once issued; they can only be canceled. Therefore all data that appears on the order are copied into the Invoice and InvoiceItem tables once the invoice is created. There is only a single original copy of the invoice; therefore, the number of printed copies is recorded.","title":"Invoicing"},{"location":"db/#invoice-issuer","text":"The issuer of the invoices changes very infrequently. However, in case it changes, existing invoices must remain unaltered. The issuer of the invoice is recorded in a separate table, and only one is in effect at all times. Each invoice references the right InvoiceIssuerId at the time.","title":"Invoice issuer"},{"location":"db/#vat","text":"The VAT percentage of products can change at any time. However, existing invoices must not be altered. Therefore the actual VAT percentage is stored with the invoice when created and not referenced from the VAT table.","title":"VAT"},{"location":"db/#product-description","text":"Some products contain an additional XML description, such as the following example. <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <package_parameters> <number_of_packages> 1 </number_of_packages> <package_size> <unit> cm </unit> <width> 150 </width> <height> 20 </height> <depth> 20 </depth> </package_size> </package_parameters> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product>","title":"Product description"},{"location":"db/mongodb/","text":"Using MongoDB \u00b6 MongoDB is a free, open-source database server. We are using the community edition and Robo 3T as the client. Download links: https://www.mongodb.com/download-center/community https://robomongo.org/download Installation instructions: https://docs.mongodb.com/manual/administration/install-community/ Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/9bb0c06f-7b5f-454b-b5ff-c784bd96d84b Starting MongoDB server \u00b6 Depending on the installation model, the server might automatically start. If we opted out of this option, we could start the server with the following command within the installation directory. (Note, that the server application is the mongo\u200b d executable.) mongod.exe --dbpath = \"<workdir>\" The database will be stored in directory workdir . When started with the command above, the server is alive until the console is closed. We can shut down the server using Ctrl + C . Mongo shell \u00b6 The Mongo shell is a simple console client. The official documentation uses this shell in the examples; however, we will not use this app. Robo 3T \u00b6 Robo 3T is a simple free client application for accessing MongoDB databases. There are other client applications (e.g., Studio 3T), but the free version is sufficient. When the app starts, it displays out previous connections, or we can create a new one. By default, the address is localhost , and the port is 27017 . After the connection is established, the databases and collections are displayed in a tree-view on the left. To begin with, we will not have any database or collections. (We can create them manually: right-click on the server and Create Database . We will not use this, though.) A collection can be opened by double-clicking. This will open a new tab, where a search command is executed. This command can be edited, and we can issue custom queries too. The content of the collection (the documents) is listed below. Each document is a separate row. A document can be viewed, edited and deleted by right clicking a document. Edit is performed by editing the raw JSON document. A new document can be inserted by right-clicking and writing the JSON content. It is advised to copy an existing document and change it to ensure that key names are correct.","title":"Using MongoDB"},{"location":"db/mongodb/#using-mongodb","text":"MongoDB is a free, open-source database server. We are using the community edition and Robo 3T as the client. Download links: https://www.mongodb.com/download-center/community https://robomongo.org/download Installation instructions: https://docs.mongodb.com/manual/administration/install-community/ Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/9bb0c06f-7b5f-454b-b5ff-c784bd96d84b","title":"Using MongoDB"},{"location":"db/mongodb/#starting-mongodb-server","text":"Depending on the installation model, the server might automatically start. If we opted out of this option, we could start the server with the following command within the installation directory. (Note, that the server application is the mongo\u200b d executable.) mongod.exe --dbpath = \"<workdir>\" The database will be stored in directory workdir . When started with the command above, the server is alive until the console is closed. We can shut down the server using Ctrl + C .","title":"Starting MongoDB server"},{"location":"db/mongodb/#mongo-shell","text":"The Mongo shell is a simple console client. The official documentation uses this shell in the examples; however, we will not use this app.","title":"Mongo shell"},{"location":"db/mongodb/#robo-3t","text":"Robo 3T is a simple free client application for accessing MongoDB databases. There are other client applications (e.g., Studio 3T), but the free version is sufficient. When the app starts, it displays out previous connections, or we can create a new one. By default, the address is localhost , and the port is 27017 . After the connection is established, the databases and collections are displayed in a tree-view on the left. To begin with, we will not have any database or collections. (We can create them manually: right-click on the server and Create Database . We will not use this, though.) A collection can be opened by double-clicking. This will open a new tab, where a search command is executed. This command can be edited, and we can issue custom queries too. The content of the collection (the documents) is listed below. Each document is a separate row. A document can be viewed, edited and deleted by right clicking a document. Edit is performed by editing the raw JSON document. A new document can be inserted by right-clicking and writing the JSON content. It is advised to copy an existing document and change it to ensure that key names are correct.","title":"Robo 3T"},{"location":"db/mssql/","text":"Using Microsoft SQL Server \u00b6 Microsoft SQL Server is accessed using SQL Server Management Studio. We are using the so-called LocalDB version that hosts the server locally for development purposes, but you can also use the Express edition (any version). Download links: LocalDB is installed with Visual Studio https://www.microsoft.com/en-us/sql-server/sql-server-editions-express https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/98a6697d-daec-4a5f-82b6-8e96f06302e8 Using SQL Server Management Studio \u00b6 In university computer laboratories, you can start the software from the start menu. Connection details are configured when the application starts. When using LocalDB, the Server name is (localdb)\\mssqllocaldb ; for Express Edition the name is .\\sqlexpress (if installed with default settings). Either case use Windows Authentication . When the connection is established, the databases are listed on the left in the Object Explorer window under Databases . A database can be expanded, and the tables (along with other schema elements) are listed here. SQL code can be executed in a new Query window opened using on the toolbar. The commands in the Query window are executed on the currently selected database. This database is selected from a dropdown in the toolbar (see in the image below with yellow). We can open multiple Query windows. The SQL command is executed using the button on the toolbar. Only the selection is executed if any text is selected; otherwise, the entire window content is sent to the server. The result and errors are printed under the script. Creating a new database \u00b6 If we have no database, we must create one first. In Object Explorer right-click Databases to open a dialog. We need to specify a name and leave all other options as-is. After creating a new database, we shall not forget to select the toolbar's current database for any Query window we have open! Concurrent transactions \u00b6 To simulate concurrent transactions, we need two Query windows; open two by pressing the New Query button twice. You can align these windows next to each other by right-clicking the Query tab title and selecting New Vertical Tab Group . Listing and editing table content \u00b6 To view any database table's content, open the database in Object Explorer , locate the table under Tables , then right-click and chose Select Top 1000 Rows or Edit Top 200 Rows . Intellisense reload \u00b6 Intellisense often does not work in SQL Management Studio query windows. Press Control+Shift+R-t to trigger a reload of Intellisense cache. We also need to use this after creating a new object (e.g., a new stored procedure). Creating stored procedures and triggers \u00b6 We can create new stored procedures or triggers by writing the T-SQL code to create them in a Query window. Once an item with the same name exists, we cannot create it but have to modify the existing one using the proper command. Existing stored procedures are listed in Object Explorer under our database in the Programability/Stored Procedures folder. (Newly created items do not appear in the folder, but we have to refresh the folder content by right-clicking and choosing Refresh .) Triggers are found in Object Explorer under the table on which they are defined in the Triggers folder (system-level triggers are in the Programability folder under the database itself). The code of any existing stored procedure or trigger can be opened by locating them (see above), then right-clicking and choosing the Modify command. This will open a new Query window with an alter command and the current code of the program.","title":"Using Microsoft SQL Server"},{"location":"db/mssql/#using-microsoft-sql-server","text":"Microsoft SQL Server is accessed using SQL Server Management Studio. We are using the so-called LocalDB version that hosts the server locally for development purposes, but you can also use the Express edition (any version). Download links: LocalDB is installed with Visual Studio https://www.microsoft.com/en-us/sql-server/sql-server-editions-express https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms Video guide of the tools How to use these tools: https://web.microsoftstream.com/video/98a6697d-daec-4a5f-82b6-8e96f06302e8","title":"Using Microsoft SQL Server"},{"location":"db/mssql/#using-sql-server-management-studio","text":"In university computer laboratories, you can start the software from the start menu. Connection details are configured when the application starts. When using LocalDB, the Server name is (localdb)\\mssqllocaldb ; for Express Edition the name is .\\sqlexpress (if installed with default settings). Either case use Windows Authentication . When the connection is established, the databases are listed on the left in the Object Explorer window under Databases . A database can be expanded, and the tables (along with other schema elements) are listed here. SQL code can be executed in a new Query window opened using on the toolbar. The commands in the Query window are executed on the currently selected database. This database is selected from a dropdown in the toolbar (see in the image below with yellow). We can open multiple Query windows. The SQL command is executed using the button on the toolbar. Only the selection is executed if any text is selected; otherwise, the entire window content is sent to the server. The result and errors are printed under the script.","title":"Using SQL Server Management Studio"},{"location":"db/mssql/#creating-a-new-database","text":"If we have no database, we must create one first. In Object Explorer right-click Databases to open a dialog. We need to specify a name and leave all other options as-is. After creating a new database, we shall not forget to select the toolbar's current database for any Query window we have open!","title":"Creating a new database"},{"location":"db/mssql/#concurrent-transactions","text":"To simulate concurrent transactions, we need two Query windows; open two by pressing the New Query button twice. You can align these windows next to each other by right-clicking the Query tab title and selecting New Vertical Tab Group .","title":"Concurrent transactions"},{"location":"db/mssql/#listing-and-editing-table-content","text":"To view any database table's content, open the database in Object Explorer , locate the table under Tables , then right-click and chose Select Top 1000 Rows or Edit Top 200 Rows .","title":"Listing and editing table content"},{"location":"db/mssql/#intellisense-reload","text":"Intellisense often does not work in SQL Management Studio query windows. Press Control+Shift+R-t to trigger a reload of Intellisense cache. We also need to use this after creating a new object (e.g., a new stored procedure).","title":"Intellisense reload"},{"location":"db/mssql/#creating-stored-procedures-and-triggers","text":"We can create new stored procedures or triggers by writing the T-SQL code to create them in a Query window. Once an item with the same name exists, we cannot create it but have to modify the existing one using the proper command. Existing stored procedures are listed in Object Explorer under our database in the Programability/Stored Procedures folder. (Newly created items do not appear in the folder, but we have to refresh the folder content by right-clicking and choosing Refresh .) Triggers are found in Object Explorer under the table on which they are defined in the Triggers folder (system-level triggers are in the Programability folder under the database itself). The code of any existing stored procedure or trigger can be opened by locating them (see above), then right-clicking and choosing the Modify command. This will open a new Query window with an alter command and the current code of the program.","title":"Creating stored procedures and triggers"},{"location":"homework/","text":"Optional homework \u00b6 These exercises are optional . You can earn extra points that are added to your exam score. In the exercises and the evaluation results, you will see a text \u201ciMsc\u201d; please ignore that (it is for the Hungarian students). All exercises are available for extra points on this course. Here you find the exercise descriptions; the submission of the solutions is expected via GitHub Classroom. Working code You are expected to write code that actually works! Your code will be executed, and it is required to fulfill the specified task. The exercises \u00b6 MSSQL server-side programming ADO.NET data access Entity Framework MongoDB REST API and Web API Submission \u00b6 Each homework must be submitted in a personal git repository. Please refer to the detailed guideline here . You must carefully study these guidelines! IMPORTANT The submissions of the homework must follow these guidelines. Submissions not adhering to the expected format are not considered. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized. Screenshots \u00b6 Some of the exercises require you to create a screenshot. This screenshot is proof of the completion of the exercise. The expected content of these screenshots is detailed in the exercise description. The screenshot may include the entire desktop or just the required portion of the screen. The screenshots must be submitted as part of the solution code, uploaded to the git repository. The repositories are private; only you and the instructions can access them. If there is any content on the screenshot that is not relevant to the exercise and you would like to remove, you can obscure these parts. Submission evaluation \u00b6 The evaluation of the exercises is semi-automatic . Your code will be executed; therefore, it is vital to follow the exercise descriptions precisely (e.g., use the provided code skeleton, change only the allowed parts of the code, etc.). You will receive a preliminary result about your submission in GitHub; see in the guideline here ). If there are some issues you need to diagnose, the entire log of the execution is available for you on the GitHub Actions web page. A short introduction is provided here . Verification In some of the exercises, where the technology permits, you will find unit tests. These tests help you verify your work, but these are no substitution for your validation . When you upload your work, more exhaustive testing will evaluate your submission.","title":"Optional homework"},{"location":"homework/#optional-homework","text":"These exercises are optional . You can earn extra points that are added to your exam score. In the exercises and the evaluation results, you will see a text \u201ciMsc\u201d; please ignore that (it is for the Hungarian students). All exercises are available for extra points on this course. Here you find the exercise descriptions; the submission of the solutions is expected via GitHub Classroom. Working code You are expected to write code that actually works! Your code will be executed, and it is required to fulfill the specified task.","title":"Optional homework"},{"location":"homework/#the-exercises","text":"MSSQL server-side programming ADO.NET data access Entity Framework MongoDB REST API and Web API","title":"The exercises"},{"location":"homework/#submission","text":"Each homework must be submitted in a personal git repository. Please refer to the detailed guideline here . You must carefully study these guidelines! IMPORTANT The submissions of the homework must follow these guidelines. Submissions not adhering to the expected format are not considered. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized.","title":"Submission"},{"location":"homework/#screenshots","text":"Some of the exercises require you to create a screenshot. This screenshot is proof of the completion of the exercise. The expected content of these screenshots is detailed in the exercise description. The screenshot may include the entire desktop or just the required portion of the screen. The screenshots must be submitted as part of the solution code, uploaded to the git repository. The repositories are private; only you and the instructions can access them. If there is any content on the screenshot that is not relevant to the exercise and you would like to remove, you can obscure these parts.","title":"Screenshots"},{"location":"homework/#submission-evaluation","text":"The evaluation of the exercises is semi-automatic . Your code will be executed; therefore, it is vital to follow the exercise descriptions precisely (e.g., use the provided code skeleton, change only the allowed parts of the code, etc.). You will receive a preliminary result about your submission in GitHub; see in the guideline here ). If there are some issues you need to diagnose, the entire log of the execution is available for you on the GitHub Actions web page. A short introduction is provided here . Verification In some of the exercises, where the technology permits, you will find unit tests. These tests help you verify your work, but these are no substitution for your validation . When you upload your work, more exhaustive testing will evaluate your submission.","title":"Submission evaluation"},{"location":"homework/GitHub-Actions/","text":"Using GitHub Actions \u00b6 The semi-automatic evaluation of the exercises uses GitHub Actions . It is a CI system capable of running jobs on git repositories. We use this system, for example, to compile your code and test it. You will receive a notification about the results in a pull request. But if you need more details, such as check the application logs, you can access these using the web interface of GitHub under Actions . Here, you will see a list of Workflows . Each evaluation (each commit) is a separate item here (so the history is also available). By selecting one (e.g., the last one is always at the top of the list), you see this workflow's details. To get to the logs, you need to click once more on the left. The log will be on the right side. Each green checkmark is a successful step. These steps do not correspond to the exercises; these describe the evaluation process. These steps include preparations, such as setting up the .NET environment for compiling your code (since each workflow starts in a clean environment, these steps are performed each time). Most of these steps should be successful, even if your submission contains an error. The two exceptions when these tasks might fail due to your changes are: (1) if neptun.txt is missing, or (2) your C# code does not compile. The neptun.txt is mandatory, and no evaluation is performed until that is provided. The C# compilation is a step that must succeed; otherwise, your application cannot be started. There might be transient errors in these workflows. An example is when a download, such as the download of the .NET environment fails. The workflow execution can be repeated if this occurs. Retrying the execution may only help if the problem is indeed transient; a retry will not resolve a C# compilation error. (You can deduce the cause from the name of the step and the error message.) You might also be able to access the application logs. E.g., when testing a .NET application, it is started, and the logs will be printed here. The image below shows the initialization of an Entity Framework application, where you can also see the translated and executed SQL commands. (You would see the same in Visual Studio Output while debugging.) The content here, obviously, depends on the actual exercise.","title":"Using GitHub Actions"},{"location":"homework/GitHub-Actions/#using-github-actions","text":"The semi-automatic evaluation of the exercises uses GitHub Actions . It is a CI system capable of running jobs on git repositories. We use this system, for example, to compile your code and test it. You will receive a notification about the results in a pull request. But if you need more details, such as check the application logs, you can access these using the web interface of GitHub under Actions . Here, you will see a list of Workflows . Each evaluation (each commit) is a separate item here (so the history is also available). By selecting one (e.g., the last one is always at the top of the list), you see this workflow's details. To get to the logs, you need to click once more on the left. The log will be on the right side. Each green checkmark is a successful step. These steps do not correspond to the exercises; these describe the evaluation process. These steps include preparations, such as setting up the .NET environment for compiling your code (since each workflow starts in a clean environment, these steps are performed each time). Most of these steps should be successful, even if your submission contains an error. The two exceptions when these tasks might fail due to your changes are: (1) if neptun.txt is missing, or (2) your C# code does not compile. The neptun.txt is mandatory, and no evaluation is performed until that is provided. The C# compilation is a step that must succeed; otherwise, your application cannot be started. There might be transient errors in these workflows. An example is when a download, such as the download of the .NET environment fails. The workflow execution can be repeated if this occurs. Retrying the execution may only help if the problem is indeed transient; a retry will not resolve a C# compilation error. (You can deduce the cause from the name of the step and the error message.) You might also be able to access the application logs. E.g., when testing a .NET application, it is started, and the logs will be printed here. The image below shows the initialization of an Entity Framework application, where you can also see the translated and executed SQL commands. (You would see the same in Visual Studio Output while debugging.) The content here, obviously, depends on the actual exercise.","title":"Using GitHub Actions"},{"location":"homework/GitHub/","text":"Submitting your work (GitHub) \u00b6 We are using GitHub to submit the solutions. Each homework is submitted in a GitHub repository. The repository is created through a link included in the exercise description. The solution of the exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the instructor (with GitHub username akosdudas ). IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded. Short version, aka. TL;DR \u00b6 The detailed description below shows the entire procedure. This summary is an overview of the whole process. The exercises are solved in a dedicated GitHub repository created using a GitHub Classroom invitation link published in Moodle. Your solution is submitted on a new branch, not on master. You can create any number of committed on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your instructor use the @name annotation in the comment text. Starting your work: git checkout \u00b6 Register a GitHub account if you don't have one yet. Open the course page in Moodle and find the invitation URL. This link is different for each homework; make sure to use the right one. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone your repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the shell or the console, the following command performs the clone (if the git command is available): git clone <repository link> If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. Check the name used for committing Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" To avoid having to set this for all repositories, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push futher commits git push Submitting the solution \u00b6 When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the instructor to evaluate your submission more easily by seeing all changes at once. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result of this evaluation as a comment added to the pull request thread. This will be different for each homework. Your code will be executed and tested, and you will receive a preliminary result too. If you need more information about the evaluation and the results, GitHub Actions can provide you more. A short introduction is provided here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to the instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request. Questions and complaints regarding the final result \u00b6 If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Submitting your work (GitHub)"},{"location":"homework/GitHub/#submitting-your-work-github","text":"We are using GitHub to submit the solutions. Each homework is submitted in a GitHub repository. The repository is created through a link included in the exercise description. The solution of the exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the instructor (with GitHub username akosdudas ). IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded.","title":"Submitting your work (GitHub)"},{"location":"homework/GitHub/#short-version-aka-tldr","text":"The detailed description below shows the entire procedure. This summary is an overview of the whole process. The exercises are solved in a dedicated GitHub repository created using a GitHub Classroom invitation link published in Moodle. Your solution is submitted on a new branch, not on master. You can create any number of committed on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your instructor use the @name annotation in the comment text.","title":"Short version, aka. TL;DR"},{"location":"homework/GitHub/#starting-your-work-git-checkout","text":"Register a GitHub account if you don't have one yet. Open the course page in Moodle and find the invitation URL. This link is different for each homework; make sure to use the right one. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone your repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the shell or the console, the following command performs the clone (if the git command is available): git clone <repository link> If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. Check the name used for committing Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" To avoid having to set this for all repositories, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push futher commits git push","title":"Starting your work: git checkout"},{"location":"homework/GitHub/#submitting-the-solution","text":"When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the instructor to evaluate your submission more easily by seeing all changes at once. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result of this evaluation as a comment added to the pull request thread. This will be different for each homework. Your code will be executed and tested, and you will receive a preliminary result too. If you need more information about the evaluation and the results, GitHub Actions can provide you more. A short introduction is provided here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to the instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request.","title":"Submitting the solution"},{"location":"homework/GitHub/#questions-and-complaints-regarding-the-final-result","text":"If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Questions and complaints regarding the final result"},{"location":"homework/VisualStudio/","text":"Install Visual Studio & .NET Core SDK \u00b6 In some of the exercises require Microsoft Visual Studio version 2019 16.6 or newer . The free Community edition is sufficient for solving these exercises. You can check the version by starting the Visual Studio Installer : VS Code The exercises can also be solved using the platform-independent Visual Studio Code . The skeletons of the exercises are prepared for Visual Studio. If you are working with VS Code, you need to configure your environment. Visual Studio workloads \u00b6 When installing Visual Studio, the following workloads have to be selected: ASP.NET and web development .NET Core cross-platform development An existing installation can be modified using the Visual Studio Installer . Check and install .NET Core SDK \u00b6 Visual Studio might install certain versions of the .NET Core SDK. To check if you have the right version, use the dotnet CLI: in a console, execute the dotnet --list-sdks command. This command works on Linux and Mac too. It will print something similar: C:\\>dotnet --list-sdks 3.1.404 [C:\\Program Files\\dotnet\\sdk] 5.0.101 [C:\\Program Files\\dotnet\\sdk] If you see version 3.1 in this list, then you are good to go. Otherwise, install the SDK from here .","title":"Install Visual Studio & .NET Core SDK"},{"location":"homework/VisualStudio/#install-visual-studio-net-core-sdk","text":"In some of the exercises require Microsoft Visual Studio version 2019 16.6 or newer . The free Community edition is sufficient for solving these exercises. You can check the version by starting the Visual Studio Installer : VS Code The exercises can also be solved using the platform-independent Visual Studio Code . The skeletons of the exercises are prepared for Visual Studio. If you are working with VS Code, you need to configure your environment.","title":"Install Visual Studio &amp; .NET Core SDK"},{"location":"homework/VisualStudio/#visual-studio-workloads","text":"When installing Visual Studio, the following workloads have to be selected: ASP.NET and web development .NET Core cross-platform development An existing installation can be modified using the Visual Studio Installer .","title":"Visual Studio workloads"},{"location":"homework/VisualStudio/#check-and-install-net-core-sdk","text":"Visual Studio might install certain versions of the .NET Core SDK. To check if you have the right version, use the dotnet CLI: in a console, execute the dotnet --list-sdks command. This command works on Linux and Mac too. It will print something similar: C:\\>dotnet --list-sdks 3.1.404 [C:\\Program Files\\dotnet\\sdk] 5.0.101 [C:\\Program Files\\dotnet\\sdk] If you see version 3.1 in this list, then you are good to go. Otherwise, install the SDK from here .","title":"Check and install .NET Core SDK"},{"location":"homework/adonet/","text":"Exercise: ADO.NET data access \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Required tools \u00b6 Windows, Linux or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Product repository (2 points) \u00b6 Create repository class for managing the Product entities; use ADO.NET Connection technology. Open the sln file from the checked-out folder using Visual Studio. Find classes Repository.ProductRepository and Model.Product . Implement the following methods of class ProductRepository : Search(string name) : find all products in the database matching the provided name, and return them as C# objects. If the name filter argument is null , the method should return all products; otherwise, it should match names that contain the specified string in a case-insensitive manner! FindById(int id) : returns a single product matched by the ID, or returns null if not found. Update(Product p) updates the properties of a product in the database based on the values received as parameter. Update the Name , Price , and Stock values, and you may ignore the rest. You should mind the following requirements: Only make changes to class ProductRepository ! In the repository code open the ADO.NET connection using the connection string in field connectionString (and do not use TestConnectionStringHelper here). You need to find the tax percentage of the product too. In the returned instance of Model.Product you must include the percentage of the referenced VAT record and not the ID of this VAT record! The name of the category of the product has to be retrieved similarly. You may only use ADO.NET. You must prohibit SQL injection. Make no changes to Model.Product in this exercise! Do not change the definition of class ProductRepository (do not change the class's name, nor the constructor or method declarations); only write the method bodies. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code and dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository (as much as you can fit on the screenshot), and the test execution outcome ! Save the screenshot as f1.png and upload it as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Optimistic concurrency handling (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. When updating a product in the database, the code shall identify and prohibit overwriting a previously unseen modification. Implement this behavior in ProductRepository.UpdateWithConcurrencyCheck . This method shall deny the update if it discovers a lost update concurrency issue. The specific sequence of events that we want to prohibit: User A queries a product. User B fetches the same product. User A changes a property, such as the price, then updates the database with this change. User B makes a change to the product properties (either the price property, or another one), and overwrites the changes made by user A without noticing it. Optimistic concurrency handling Use the technique of optimistic concurrency handling to resolve this issue. You must not use transactions here, since the query and the data update happens over a longer period without maintaining a database connection. Do not use multiple SQL statements either, as in-between the execution of multiple statements the database content can change resulting in your application not working with the latest data. Implement the method ProductRepository.UpdateWithConcurrencyCheck , and also update Model.Product as needed. You may not add any new columns to the database. You should mind the following requirements: Only make changes to the method ProductRepository.UpdateWithConcurrencyCheck and class Model.Product ! The method shall indicate as return value whether the change was saved (that it, it discovered no concurrency issues). Explain the behavior in a C# comment in method UpdateWithConcurrencyCheck (in 2-3 sentences). Solve the exercise with using a single SQL command! You may only use ADO.NET. You must prohibit SQL injection. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the single method body. Do not change the constructor signature of the class Model.Product (number, order, or names of the parameters), but you may change the body. Do not alter any existing properties of the class, but you can add new ones. SUBMISSION Upload the changed C# source code. Do not forget the explanation comment!","title":"Exercise: ADO.NET data access"},{"location":"homework/adonet/#exercise-adonet-data-access","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission.","title":"Exercise: ADO.NET data access"},{"location":"homework/adonet/#required-tools","text":"Windows, Linux or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.","title":"Required tools"},{"location":"homework/adonet/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/adonet/#exercise-1-product-repository-2-points","text":"Create repository class for managing the Product entities; use ADO.NET Connection technology. Open the sln file from the checked-out folder using Visual Studio. Find classes Repository.ProductRepository and Model.Product . Implement the following methods of class ProductRepository : Search(string name) : find all products in the database matching the provided name, and return them as C# objects. If the name filter argument is null , the method should return all products; otherwise, it should match names that contain the specified string in a case-insensitive manner! FindById(int id) : returns a single product matched by the ID, or returns null if not found. Update(Product p) updates the properties of a product in the database based on the values received as parameter. Update the Name , Price , and Stock values, and you may ignore the rest. You should mind the following requirements: Only make changes to class ProductRepository ! In the repository code open the ADO.NET connection using the connection string in field connectionString (and do not use TestConnectionStringHelper here). You need to find the tax percentage of the product too. In the returned instance of Model.Product you must include the percentage of the referenced VAT record and not the ID of this VAT record! The name of the category of the product has to be retrieved similarly. You may only use ADO.NET. You must prohibit SQL injection. Make no changes to Model.Product in this exercise! Do not change the definition of class ProductRepository (do not change the class's name, nor the constructor or method declarations); only write the method bodies. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code and dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository (as much as you can fit on the screenshot), and the test execution outcome ! Save the screenshot as f1.png and upload it as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Product repository (2 points)"},{"location":"homework/adonet/#exercise-2-optimistic-concurrency-handling-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. When updating a product in the database, the code shall identify and prohibit overwriting a previously unseen modification. Implement this behavior in ProductRepository.UpdateWithConcurrencyCheck . This method shall deny the update if it discovers a lost update concurrency issue. The specific sequence of events that we want to prohibit: User A queries a product. User B fetches the same product. User A changes a property, such as the price, then updates the database with this change. User B makes a change to the product properties (either the price property, or another one), and overwrites the changes made by user A without noticing it. Optimistic concurrency handling Use the technique of optimistic concurrency handling to resolve this issue. You must not use transactions here, since the query and the data update happens over a longer period without maintaining a database connection. Do not use multiple SQL statements either, as in-between the execution of multiple statements the database content can change resulting in your application not working with the latest data. Implement the method ProductRepository.UpdateWithConcurrencyCheck , and also update Model.Product as needed. You may not add any new columns to the database. You should mind the following requirements: Only make changes to the method ProductRepository.UpdateWithConcurrencyCheck and class Model.Product ! The method shall indicate as return value whether the change was saved (that it, it discovered no concurrency issues). Explain the behavior in a C# comment in method UpdateWithConcurrencyCheck (in 2-3 sentences). Solve the exercise with using a single SQL command! You may only use ADO.NET. You must prohibit SQL injection. Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the single method body. Do not change the constructor signature of the class Model.Product (number, order, or names of the parameters), but you may change the body. Do not alter any existing properties of the class, but you can add new ones. SUBMISSION Upload the changed C# source code. Do not forget the explanation comment!","title":"Exercise 2: Optimistic concurrency handling (2 points)"},{"location":"homework/ef/","text":"Exercise: Entity Framework \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Entity Framework Core We are using Entity Framework Core in this exercise. This is different from Entity Framework used in the seminar exercises; this is a platform-independent technology. Required tools \u00b6 Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Database mapping using Code First model and queries (2 points) \u00b6 Prepare the (partial) mapping of the database using Entity Framework Code First modeling. The Entity Framework Core package is part of the project, so you can start coding. The central class for database access is the DbContext. This class already exists with the name ProductDBContext . Map the product entity. Create a new class with the name DbProduct with the following code. (The Db prefix indicates that this class is within the scope of the database. This will be relevant in the next exercise.) We rely on conventions as much as possible: use property names that match the column names to make mapping automatic. using System.ComponentModel.DataAnnotations.Schema ; namespace ef { [Table(\"Product\")] public class DbProduct { [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int ID { get ; set ; } public string Name { get ; set ; } public double Price { get ; set ; } public int Stock { get ; set ; } } } Open the source code of class ProductDbContext and uncomment the Products property. Create a new class with the name DbVat for mapping the VAT database table similarly as seen before. Do not forget to add a new DbSet property into ProductContext with the name Vat . Map the Product - VAT connection. Add a new get-set property into class DbProduct with name Vat and type DbVat . Use the ForeignKey attribute on this property , to indicate the foreign key used to store the relationship (\"VatID\"). Create the \u201cother side\u201d of this one-to-many connection from class DbVat to DbProduct . This should be a new property of type System.Collections.Generic.List with name Products . (See an example in the link above.) There are unit tests available in the solution. The test codes are commented out because it does not compile until you write the code. Select the whole test code and use Edit / Advanced / Uncomment Selection . You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code, or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. If the tests do not compile If the test code does not compile, you may have used slightly different property names. Fix these in your code and not in the tests ! OnConfiguring You need no connection string in the DbContext . The constructor handles the connection to the database. Do not create OnConfiguring method in this class! SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the DbContext and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes here and there. That is, if the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Repository implementation using Entity Framework (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The Entity Framework DbContext created above has some drawbacks. For example, we need to trigger loading related entities using Include in every query, and the mapped entities are bound to precisely match the database schema. In complex applications, the DbContext is frequently wrapped in a repository that handles all peculiarities of the data access layer. Implement class ProductRepository that helps with listing and inserting products. You are provided with a so-called model class representing the product entity, only in a more user-friendly way: it contains the tax percentage value directly. An instance of this class is built from database entities, but represents all information in one instance instead of having to handle a product and a VAT record separately. Class Model.Product contains most properties of class DbProduct , but instead of the navigation property to DbVat it contains the referenced percentage value ( VAT.Percentage ) directly. Implement the methods of class `ProductRepository. List shall return all products mapped to instances of Model.Product . Insert shall insert a new product into the database. This method shall find the matching VAT record in the database based on the tax percentage value in the model class; if there is no match, it shall insert a new VAT record too! Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the method bodies. In the repository code use ProductRepository.createDbContext() to instantiate the DbContext (do not use TestConnectionStringHelper here). SUBMISSION Upload the changed C# source code.","title":"Exercise: Entity Framework"},{"location":"homework/ef/#exercise-entity-framework","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Entity Framework Core We are using Entity Framework Core in this exercise. This is different from Entity Framework used in the seminar exercises; this is a platform-independent technology.","title":"Exercise: Entity Framework"},{"location":"homework/ef/#required-tools","text":"Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.","title":"Required tools"},{"location":"homework/ef/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/ef/#exercise-1-database-mapping-using-code-first-model-and-queries-2-points","text":"Prepare the (partial) mapping of the database using Entity Framework Code First modeling. The Entity Framework Core package is part of the project, so you can start coding. The central class for database access is the DbContext. This class already exists with the name ProductDBContext . Map the product entity. Create a new class with the name DbProduct with the following code. (The Db prefix indicates that this class is within the scope of the database. This will be relevant in the next exercise.) We rely on conventions as much as possible: use property names that match the column names to make mapping automatic. using System.ComponentModel.DataAnnotations.Schema ; namespace ef { [Table(\"Product\")] public class DbProduct { [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int ID { get ; set ; } public string Name { get ; set ; } public double Price { get ; set ; } public int Stock { get ; set ; } } } Open the source code of class ProductDbContext and uncomment the Products property. Create a new class with the name DbVat for mapping the VAT database table similarly as seen before. Do not forget to add a new DbSet property into ProductContext with the name Vat . Map the Product - VAT connection. Add a new get-set property into class DbProduct with name Vat and type DbVat . Use the ForeignKey attribute on this property , to indicate the foreign key used to store the relationship (\"VatID\"). Create the \u201cother side\u201d of this one-to-many connection from class DbVat to DbProduct . This should be a new property of type System.Collections.Generic.List with name Products . (See an example in the link above.) There are unit tests available in the solution. The test codes are commented out because it does not compile until you write the code. Select the whole test code and use Edit / Advanced / Uncomment Selection . You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code, or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestConnectionStringHelper if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. If the tests do not compile If the test code does not compile, you may have used slightly different property names. Fix these in your code and not in the tests ! OnConfiguring You need no connection string in the DbContext . The constructor handles the connection to the database. Do not create OnConfiguring method in this class! SUBMISSION Upload the changed C# source code. Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the DbContext and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes here and there. That is, if the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Database mapping using Code First model and queries (2 points)"},{"location":"homework/ef/#exercise-2-repository-implementation-using-entity-framework-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The Entity Framework DbContext created above has some drawbacks. For example, we need to trigger loading related entities using Include in every query, and the mapped entities are bound to precisely match the database schema. In complex applications, the DbContext is frequently wrapped in a repository that handles all peculiarities of the data access layer. Implement class ProductRepository that helps with listing and inserting products. You are provided with a so-called model class representing the product entity, only in a more user-friendly way: it contains the tax percentage value directly. An instance of this class is built from database entities, but represents all information in one instance instead of having to handle a product and a VAT record separately. Class Model.Product contains most properties of class DbProduct , but instead of the navigation property to DbVat it contains the referenced percentage value ( VAT.Percentage ) directly. Implement the methods of class `ProductRepository. List shall return all products mapped to instances of Model.Product . Insert shall insert a new product into the database. This method shall find the matching VAT record in the database based on the tax percentage value in the model class; if there is no match, it shall insert a new VAT record too! Do not change the definition of class ProductRepository (do not change the name of the class, nor the constructor or method declarations); only write the method bodies. In the repository code use ProductRepository.createDbContext() to instantiate the DbContext (do not use TestConnectionStringHelper here). SUBMISSION Upload the changed C# source code.","title":"Exercise 2: Repository implementation using Entity Framework (2 points)"},{"location":"homework/mongodb/","text":"Exercise: MongoDB \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Required tools \u00b6 Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. MongoDB Community Server ( download ) Robo 3T ( download ) Sample database initialization script: mongo.js Create and initialize the database; use the steps the seminar exercises describe. GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux, or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Product with the largest total value (2 points) \u00b6 The task is to find the product that has the largest total value within a product category. The total value is the price of the product multiplied by the amount of the product in stock . You need to implement the following method in class ProductRepository . ( string , double? ) ProductWithLargestTotalValue ( ObjectId categoryId ) Let us check the test related to this exercise in file TestExercise1.cs to understand what is expected here. The method accepts a category name as an argument; products have to be filtered for this category. The return value should be the name of the product (with the largest total value) and the total value itself. If there are no products in the specified category, the return value should be (null, null) . Use the aggregation pipeline of MongoDB. To see how this aggregation pipeline works, you can refer to the seminar material. You should build a pipeline consisting of the following stages: Filter the products for the specified category. Use a $match ( Match ) stage to specify the filter. Calculate for each product the total value (multiply the price and the stock) using a $project ( Project ) stage. Make sure to include the name of the product, you will need it for the final result. Order the items based on this calculated total value descending. Use a $sort ( SortByDescending ) stage. Since it is the largest value that we need, take the first item after sorting. Do not forget that there might not be any product in the specified category. Therefore you should use FirstOrDefault to fetch this item. If the syntax (string, double?) is unfamiliar: return ( \"test\" , 0.0 ); The function will return with these two values. Implement the repository method. The repository class receives the database as a parameter and saves the collection as a local variable in the class; use this field to manipulate the collection. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestDbFactory if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot. Exercise 2: Estimating storage space (2 points) \u00b6 The company is moving to a new location. We need to know whether the current stock can be moved and will fit into the new storage facility. Implement the method that calculates the total volume of all products in stock ! The products have the necessary information in description.product.package_parameters : Use this to calculate the total volume of all items: Use the information from package_parameters (and not from product_size ). A product might have multiple packages; this information is available in package_parameters.number_of_packages . This number shall be used as a multiplicator. Each product has a single size, and if it has multiple packages, then all packages are of the same size. The final total: for all products \u03a3 (product stock * number of packages * width * height * depth). If a product does not have these information, it's volume should be calculated as 0. Mind, that the size also has a unit: either cm or m , but the final value is expected in cubic meter. Implement method double GetAllProductsCumulativeVolume() that returns a single scalar total of the volume in cubic meters . The calculation should be performed by the database (not in C#); use the aggregation pipeline. Sum aggregation You will need the $group aggregation stage. Although, we do not need to group the products, still, this will allow us to aggregate all of them. Map each product into the same group (that is, in the $group stage the id should be a constant for all items), then use the projection part to perform a $sum aggregation according to the formula above. Handling of the cm and m units can be solved with a conditional multiplication in the sum . If this does not work, you can use two aggregations: one for cm and one for m units, each with a filter in the pipeline then the aggregation afterwards. The required parts of the products are not mapped to C# classes yet. You need to do this. Note, that the field names in the BSON do not conform to the usual syntax, thus, when mapping to C# properties, you have to take care of name the properties identically, or use the [BsonElement(elementName: \"...\")] attribute. Use Fluent Api You must use the C# Fluent Api! No not write the query using BsonDocument ! You may test your implementation with the tests provided in class TestExercise2 . The test presume that the database is in its initial state. SUBMISSION Upload the changed C# source code.","title":"Exercise: MongoDB"},{"location":"homework/mongodb/#exercise-mongodb","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission.","title":"Exercise: MongoDB"},{"location":"homework/mongodb/#required-tools","text":"Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. MongoDB Community Server ( download ) Robo 3T ( download ) Sample database initialization script: mongo.js Create and initialize the database; use the steps the seminar exercises describe. GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux, or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.","title":"Required tools"},{"location":"homework/mongodb/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/mongodb/#exercise-1-product-with-the-largest-total-value-2-points","text":"The task is to find the product that has the largest total value within a product category. The total value is the price of the product multiplied by the amount of the product in stock . You need to implement the following method in class ProductRepository . ( string , double? ) ProductWithLargestTotalValue ( ObjectId categoryId ) Let us check the test related to this exercise in file TestExercise1.cs to understand what is expected here. The method accepts a category name as an argument; products have to be filtered for this category. The return value should be the name of the product (with the largest total value) and the total value itself. If there are no products in the specified category, the return value should be (null, null) . Use the aggregation pipeline of MongoDB. To see how this aggregation pipeline works, you can refer to the seminar material. You should build a pipeline consisting of the following stages: Filter the products for the specified category. Use a $match ( Match ) stage to specify the filter. Calculate for each product the total value (multiply the price and the stock) using a $project ( Project ) stage. Make sure to include the name of the product, you will need it for the final result. Order the items based on this calculated total value descending. Use a $sort ( SortByDescending ) stage. Since it is the largest value that we need, take the first item after sorting. Do not forget that there might not be any product in the specified category. Therefore you should use FirstOrDefault to fetch this item. If the syntax (string, double?) is unfamiliar: return ( \"test\" , 0.0 ); The function will return with these two values. Implement the repository method. The repository class receives the database as a parameter and saves the collection as a local variable in the class; use this field to manipulate the collection. There are unit tests available in the solution. You can run the unit tests in Visual Studio , or if you are using another IDE (e.g., VS Code or dotnet cli ), then run the tests using the cli . You may update the database connection string in class TestDbFactory if needed. Tests The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state. Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing. SUBMISSION Upload the changed C# source code. You can run the tests in Visual Studio or using dotnet cli . Make sure that the screenshot includes the source code of the repository and the test execution outcome ! Save the screenshot as f1.png and upload as part of your submission! If you are using dotnet cli to run the tests, make sure to display the test names too. Use the -v n command line switch to set detailed logging. The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.","title":"Exercise 1: Product with the largest total value (2 points)"},{"location":"homework/mongodb/#exercise-2-estimating-storage-space-2-points","text":"The company is moving to a new location. We need to know whether the current stock can be moved and will fit into the new storage facility. Implement the method that calculates the total volume of all products in stock ! The products have the necessary information in description.product.package_parameters : Use this to calculate the total volume of all items: Use the information from package_parameters (and not from product_size ). A product might have multiple packages; this information is available in package_parameters.number_of_packages . This number shall be used as a multiplicator. Each product has a single size, and if it has multiple packages, then all packages are of the same size. The final total: for all products \u03a3 (product stock * number of packages * width * height * depth). If a product does not have these information, it's volume should be calculated as 0. Mind, that the size also has a unit: either cm or m , but the final value is expected in cubic meter. Implement method double GetAllProductsCumulativeVolume() that returns a single scalar total of the volume in cubic meters . The calculation should be performed by the database (not in C#); use the aggregation pipeline. Sum aggregation You will need the $group aggregation stage. Although, we do not need to group the products, still, this will allow us to aggregate all of them. Map each product into the same group (that is, in the $group stage the id should be a constant for all items), then use the projection part to perform a $sum aggregation according to the formula above. Handling of the cm and m units can be solved with a conditional multiplication in the sum . If this does not work, you can use two aggregations: one for cm and one for m units, each with a filter in the pipeline then the aggregation afterwards. The required parts of the products are not mapped to C# classes yet. You need to do this. Note, that the field names in the BSON do not conform to the usual syntax, thus, when mapping to C# properties, you have to take care of name the properties identically, or use the [BsonElement(elementName: \"...\")] attribute. Use Fluent Api You must use the C# Fluent Api! No not write the query using BsonDocument ! You may test your implementation with the tests provided in class TestExercise2 . The test presume that the database is in its initial state. SUBMISSION Upload the changed C# source code.","title":"Exercise 2: Estimating storage space (2 points)"},{"location":"homework/mssql/","text":"Exercise: MSSQL server-side programming \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Required tools \u00b6 Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client Prepare the database \u00b6 Create a new database with a name that matches your Neptun code . Run the database initialization script to create the tables in this database. Neptun code is important The exercise will ask you for a screenshot that must contain the database name with your Neptun code! Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Password expiry maintenance (2 points) \u00b6 Due to security reasons, we would like to enforce password expiry. For this, we will record the date when the password was last updated. Add a new column to the Customer table with the name PasswordExpiry storing a date: alter table [Customer] add [PasswordExpiry] datetime . Create a trigger that automatically fills the PasswordExpiry date column when the password value is updated. The new value should be the current date plus one year. The trigger shall calculate the value. When a new Customer is registered (inserted into the table), the column should always be populated automatically. However, when data is updated, only update the date if the password is changed. (E.g. if only the address is altered, the date should not be updated.) The trigger should only update the date for the inserted/modified record (it should not set it for all records in the table)! In this exercise, you may suppose that there is always one record inserted/modified at any time. Make sure to verify the behavior of the trigger under various circumstances. SUBMISSION Submit the code of the trigger in file f1.sql . This sql file should contain a single statement (a single create trigger command) without any use or go commands. Create a screenshot that displays sample records in the Customer table with the automatically populated date values. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f1.png and upload it as part of your submission! Exercise 2: Product recommended age (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The database contains an xml column with the name Description in the Product table. This column has values for some of the records. An example for the content is below: <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product> We want to extract the recommended_age and move it to a new column in the table. Add a new column to the Product table with name RecommendedAge storing a text: alter table [Product] add [RecommendedAge] nvarchar(200) . (Do not submit this statement in the solution.) Create a T-SQL script that extracts the content of the <recommended_age> tag from the xml and moves the value into the RecommendedAge column of the table. If the xml description is empty or there is no <recommended_age> tag, the column's value should be NULL . Otherwise, take the tag's text content (without the tag name), copy the value into the column, and remove the tag from the xml. You can presume that there is at most one <recommended_age> element in the xml. SUBMISSION Submit the T-SQL code in file f2.sql . Do not use a stored procedure in this exercise; create a simple T-SQL code block. This sql file should be executable by itself and should not contain any use or go commands. Create a screenshot that displays the content of the Product table after running the script. The new column and the populated values should be visible on the screenshot. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise: MSSQL server-side programming"},{"location":"homework/mssql/#exercise-mssql-server-side-programming","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission.","title":"Exercise: MSSQL server-side programming"},{"location":"homework/mssql/#required-tools","text":"Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Microsoft SQL Server The free Express version is sufficient, or you may also use localdb installed with Visual Studio A Linux version is also available. On macOS, you can use Docker. SQL Server Management Studio , or you may also use the platform-independent Azure Data Studio is Database initialization script: mssql.sql GitHub account and a git client","title":"Required tools"},{"location":"homework/mssql/#prepare-the-database","text":"Create a new database with a name that matches your Neptun code . Run the database initialization script to create the tables in this database. Neptun code is important The exercise will ask you for a screenshot that must contain the database name with your Neptun code!","title":"Prepare the database"},{"location":"homework/mssql/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/mssql/#exercise-1-password-expiry-maintenance-2-points","text":"Due to security reasons, we would like to enforce password expiry. For this, we will record the date when the password was last updated. Add a new column to the Customer table with the name PasswordExpiry storing a date: alter table [Customer] add [PasswordExpiry] datetime . Create a trigger that automatically fills the PasswordExpiry date column when the password value is updated. The new value should be the current date plus one year. The trigger shall calculate the value. When a new Customer is registered (inserted into the table), the column should always be populated automatically. However, when data is updated, only update the date if the password is changed. (E.g. if only the address is altered, the date should not be updated.) The trigger should only update the date for the inserted/modified record (it should not set it for all records in the table)! In this exercise, you may suppose that there is always one record inserted/modified at any time. Make sure to verify the behavior of the trigger under various circumstances. SUBMISSION Submit the code of the trigger in file f1.sql . This sql file should contain a single statement (a single create trigger command) without any use or go commands. Create a screenshot that displays sample records in the Customer table with the automatically populated date values. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f1.png and upload it as part of your submission!","title":"Exercise 1: Password expiry maintenance (2 points)"},{"location":"homework/mssql/#exercise-2-product-recommended-age-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. The database contains an xml column with the name Description in the Product table. This column has values for some of the records. An example for the content is below: <product> <product_size> <unit> cm </unit> <width> 150 </width> <height> 50 </height> <depth> 150 </depth> </product_size> <description> Requires battery (not part of the package). </description> <recommended_age> 0-18 m </recommended_age> </product> We want to extract the recommended_age and move it to a new column in the table. Add a new column to the Product table with name RecommendedAge storing a text: alter table [Product] add [RecommendedAge] nvarchar(200) . (Do not submit this statement in the solution.) Create a T-SQL script that extracts the content of the <recommended_age> tag from the xml and moves the value into the RecommendedAge column of the table. If the xml description is empty or there is no <recommended_age> tag, the column's value should be NULL . Otherwise, take the tag's text content (without the tag name), copy the value into the column, and remove the tag from the xml. You can presume that there is at most one <recommended_age> element in the xml. SUBMISSION Submit the T-SQL code in file f2.sql . Do not use a stored procedure in this exercise; create a simple T-SQL code block. This sql file should be executable by itself and should not contain any use or go commands. Create a screenshot that displays the content of the Product table after running the script. The new column and the populated values should be visible on the screenshot. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise 2: Product recommended age (2 points)"},{"location":"homework/rest/","text":"Exercise: REST API and Web API \u00b6 This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission. Required tools \u00b6 Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Postman GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux, or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS. Exercise 0: Neptun code \u00b6 Your very first task is to type your Neptun code into neptun.txt in the root of the repository. Exercise 1: Operations on products (2 points) \u00b6 The repository you cloned contains a skeleton application. Open the provided Visual Studio solution and start the application. A console window should appear that hosts the web application. While the web app is running, test it: open a browser to http://localhost:5000/api/product . The page should display a list of products in JSON format. Check the source code. Startup.cs initializes your application. This is an ASP.NET Core web application. There is no database used in this project to make things simpler. Class ProductRepository contains hard-wired data used for testing. ProductsController uses dependency injection to instantiate IProductRepository . Exercises: In class DAL.ProductRepository edit the field Neptun and add your Neptun code here. The string should contain the 6 characters of your Neptun code. IMPORTANT The data altered this way will be displayed on a screenshot in a later exercise; hence this step is essential. Create a new API endpoint for verifying whether a particular product specified by its ID exists. This new endpoint should respond to a HEAD HTTP query on URL /api/product/{id} . The HTTP response should be status code 200 or 404 (without any body either case). Create a new API endpoint that returns a single Product specified by its ID; the query is a GET query on URL /api/product/{id} and the response should be either 200 OK with the product as body, or 404 when the product is not found. Create a new API endpoint for querying the total number of products. (Such an endpoint could be used, for example, by the UI for paging the list of products.) This should be a GET HTTP request to URL /api/product/-/count . The returned result should be a JSON serialized CountResult object with the correct count. Why is there a /- in the URL? In order to understand the need for this, let us consider what the URL should look like: we are querying products, so /api/product is the prefix, but what is the end of the URL? It could be /api/product/count . However, this clashes with /api/product/123 where we can get a particular product. In practice, the two URLs could work side-by-side here since the product ID is an integer, and the framework would recognize that an URL ending in /123 is to get a product and the /count is to get the counts. But this works only as long as the ID is an integer. If the product ID were a string, this would be more problematic. Our solution makes sure that the URLs do not clash. The /- is to indicate that there is no product ID. Note: the way URLs are matched to controller methods is more complicated. ASP.NET Core has a notion of priorities when trying to find a controller method for an URL. This priority can be modified on the [Http*] attributes by setting the Order property . SUBMISSION Upload the changed source code. Create a screenshot from Postman (or any alternative tool you used for testing) that shows a successful query that fetches an existing product. The screenshot should display both the request and response with all information (request type, URL, response code, response body). Save the screenshot as f1.png and upload it as part of your submission! The response body must contain your Neptun code . Exercise 2: OpenAPI documentation (2 points) \u00b6 In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. OpenAPI (formerly Swagger) is a REST API documentation tool. It is similar to the WSDL for Web Services. Its goal is to describe the API in a standardized format. In this exercise, you are required to create a OpenAPI specification and documentation for your REST API. Please follow the official Microsoft tutorial at: https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle?view=aspnetcore-3.1&tabs=visual-studio Make sure to use Swashbuckle . The swagger.json should be generated by your application (you need not write it), and it should be available at URL /swagger/v1/swagger.json . Set up Swagger UI , which should be available at URL /neptun . To achieve this, when configuring UseSwaggerUI set the RoutePrefix as your Neptun code all lower-case . (You can ignore the \"Customize and extend\" parts in the tutorial.) When ready, start the web application and check swagger.json at URL http://localhost:5000/swagger/v1/swagger.json , then open SwaggerUI too at http://localhost:5000/neptun . Test the \u201cTry it out\u201d in SwaggerUI: it will send out the query that your backend will serve. SUBMISSION Upload the changed source code. Make sure to upload the altered csproj file too; it contains a new NuGet package added in this exercise. Create a screenshot of SwaggerUI open in the browser. Make sure that the URL is visible and that it contains /neptun with your Neptun code. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise: REST API and Web API"},{"location":"homework/rest/#exercise-rest-api-and-web-api","text":"This exercise is optional. You may earn 2+2 points by completing this exercise. Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle . Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. After completing the exercises and verifying them, commit and push your submission.","title":"Exercise: REST API and Web API"},{"location":"homework/rest/#required-tools","text":"Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available. Postman GitHub account and a git client Microsoft Visual Studio 2019 with the settings here When using Linux, or macOS, you can use Visual Studio Code, the .NET Core SDK, and dotnet CLI . .NET Core 3.1 SDK .NET Core 3.1 Mind the version! You need .NET Core SDK version 3.1 to solve these exercises. On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.","title":"Required tools"},{"location":"homework/rest/#exercise-0-neptun-code","text":"Your very first task is to type your Neptun code into neptun.txt in the root of the repository.","title":"Exercise 0: Neptun code"},{"location":"homework/rest/#exercise-1-operations-on-products-2-points","text":"The repository you cloned contains a skeleton application. Open the provided Visual Studio solution and start the application. A console window should appear that hosts the web application. While the web app is running, test it: open a browser to http://localhost:5000/api/product . The page should display a list of products in JSON format. Check the source code. Startup.cs initializes your application. This is an ASP.NET Core web application. There is no database used in this project to make things simpler. Class ProductRepository contains hard-wired data used for testing. ProductsController uses dependency injection to instantiate IProductRepository . Exercises: In class DAL.ProductRepository edit the field Neptun and add your Neptun code here. The string should contain the 6 characters of your Neptun code. IMPORTANT The data altered this way will be displayed on a screenshot in a later exercise; hence this step is essential. Create a new API endpoint for verifying whether a particular product specified by its ID exists. This new endpoint should respond to a HEAD HTTP query on URL /api/product/{id} . The HTTP response should be status code 200 or 404 (without any body either case). Create a new API endpoint that returns a single Product specified by its ID; the query is a GET query on URL /api/product/{id} and the response should be either 200 OK with the product as body, or 404 when the product is not found. Create a new API endpoint for querying the total number of products. (Such an endpoint could be used, for example, by the UI for paging the list of products.) This should be a GET HTTP request to URL /api/product/-/count . The returned result should be a JSON serialized CountResult object with the correct count. Why is there a /- in the URL? In order to understand the need for this, let us consider what the URL should look like: we are querying products, so /api/product is the prefix, but what is the end of the URL? It could be /api/product/count . However, this clashes with /api/product/123 where we can get a particular product. In practice, the two URLs could work side-by-side here since the product ID is an integer, and the framework would recognize that an URL ending in /123 is to get a product and the /count is to get the counts. But this works only as long as the ID is an integer. If the product ID were a string, this would be more problematic. Our solution makes sure that the URLs do not clash. The /- is to indicate that there is no product ID. Note: the way URLs are matched to controller methods is more complicated. ASP.NET Core has a notion of priorities when trying to find a controller method for an URL. This priority can be modified on the [Http*] attributes by setting the Order property . SUBMISSION Upload the changed source code. Create a screenshot from Postman (or any alternative tool you used for testing) that shows a successful query that fetches an existing product. The screenshot should display both the request and response with all information (request type, URL, response code, response body). Save the screenshot as f1.png and upload it as part of your submission! The response body must contain your Neptun code .","title":"Exercise 1: Operations on products (2 points)"},{"location":"homework/rest/#exercise-2-openapi-documentation-2-points","text":"In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that. OpenAPI (formerly Swagger) is a REST API documentation tool. It is similar to the WSDL for Web Services. Its goal is to describe the API in a standardized format. In this exercise, you are required to create a OpenAPI specification and documentation for your REST API. Please follow the official Microsoft tutorial at: https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle?view=aspnetcore-3.1&tabs=visual-studio Make sure to use Swashbuckle . The swagger.json should be generated by your application (you need not write it), and it should be available at URL /swagger/v1/swagger.json . Set up Swagger UI , which should be available at URL /neptun . To achieve this, when configuring UseSwaggerUI set the RoutePrefix as your Neptun code all lower-case . (You can ignore the \"Customize and extend\" parts in the tutorial.) When ready, start the web application and check swagger.json at URL http://localhost:5000/swagger/v1/swagger.json , then open SwaggerUI too at http://localhost:5000/neptun . Test the \u201cTry it out\u201d in SwaggerUI: it will send out the query that your backend will serve. SUBMISSION Upload the changed source code. Make sure to upload the altered csproj file too; it contains a new NuGet package added in this exercise. Create a screenshot of SwaggerUI open in the browser. Make sure that the URL is visible and that it contains /neptun with your Neptun code. Save the screenshot as f2.png and upload it as part of your submission!","title":"Exercise 2: OpenAPI documentation (2 points)"},{"location":"lecture-notes/architecture/","text":"Data-driven systems and the thee- or multi-tier architecture \u00b6 What is a data-driven system? \u00b6 Every software handles data in some sense since the computer memory stores data, and the software manipulates this data. But not all applications are data-driven. A system or an application is called data-driven if its main purpose is to manage data. In other words, the data-drive application is created to store, display, and manage data. The end-user uses this application to access the data within. A chess game app also stores data in memory: the state of the chessboard. But the chess game app is not created to manipulate this data. The game is designed so that a user can play chess. In a data-driven system, the data itself defines how the application operates. For example, based on a data record's specific attributes, deleting this record may be allowed or may be prohibited. Another example is how the Neptun system enables registration to an exam. The semester schedule, which defines the exam period, is data stored within the system itself. This schedule, stored as data, determines whether the end-user (here: the student) can register for an exam. The fact that the exam period starts on a different day each year does not mean that the software logic (i.e., software code) changes; the data makes the software behave differently. Data-driven system example \u00b6 The Neptun system is a typical example of a data-driven system. Its purpose is the management of all data related to courses, students, grades. Another example is Gmail : it manages emails, attachments, contacts, etc. Every functionality of the application is about managing and displaying these data. And, of course, the data is stored securely, and every change in the system is persisted (i.e., not lost). The structure of a data-driven system \u00b6 Let us consider Gmail as an example. We would like to build a system which is capable of: sending and receiving emails, has a web application and a connected mobile app too, the UI supports multiple display languages, we can attach files, attachments can be referenced from Google Drive too, we can delay sending a composed email, etc. Let us design How do you start developing such a complex application? This might be a complicated question. Let us begin with a more straightforward question. Supposing that the system already works, and now we want to add the delayed sending feature. How do we do this? We could start a timer when the send button is clicked, and this timer, after a minute, sends the email. This will not work if the browser is closed before the countdown is over. We could add the scheduled date of sending to the email as data. We can translate this as an architectural decision: delayed sending is not the responsibility of the user interface. We did not decide yet, which part of the application will be responsible, but we already know it must not be the UI. Let us consider a similar question. The received date of an email should be displayed to the user according to their preference, i.e., in Europe, 15:23 is the preferred date format, while other parts of the world might prefer 3:23 PM. Does this mean that the email as a data record has multiple received dates? Obviously not. The received date is a single date in a universal representation that is transformed by the UI to the appropriate format. To summarize, we established that there are functionalities that the UI is responsible for, and there are other functionalities that the UI has nothing to do with. Now we arrive at the three- or multi-layer architecture. The three- or multi-layered architecture \u00b6 Data-driven systems are usually built on the three- or multi-layered architecture. The two names are treaded as synonyms here. This architecture defines three main components: the presentation layer (or UI), the business layer, and the data access layer. Besides, the architecture also includes: a database or external data sources; and the so-called cross-cutting concerns (see later). The application components are organized such that each component belongs to a single layer, and each layer has its responsibilities. This logical grouping of components enables the software developer to design components with clear responsibilities and well-defined boundaries. Why multi-layered when it only has three? The multi-layered terminology enables each of the previously listed layers to be further decomposed into sub-layers depending on complexity. In other words, an architecture is multi-layered if it has more than two layers. (In the two-layered architecture, the UI and the business logic are not separated.) The layers not only have their responsibilities , but also define their interface provided to the layers on top of them. The data access layer specifies the operations the business layer can use to retrieve data; similarly the business layer defines the functionalities the presentation layer can build upon. Each layer is allowed to communicate only with the layer directly beneath . For example, the presentation layer is not allowed to execute a SQL query in the database. At the same time, the implementation behind the well-defined communication interface can change enabling easier maintenance of the software codebase. By having the software split into layers, we can also move the layers to multiple servers (e.g., to handle larger loads). The simplest form is when the presentation layer runs in a browser on the user's machine, while the rest is hosted on a remote server. The database is also frequently offloaded to a dedicated server. Running the various layers on separate servers is usually motivated by performance reasons. Layer / tier The name of the architecture distinguishes the logical and physical separation of the components. Layers mean logical separation hosted on a single machine. Tiers indicate that at least some of the components have dedicated servers. A system built on a well-designed architecture can be used and maintained over a long period. The separation of layers is neither a burden nor a set of mandatory rules. Instead, the layered architecture is a helpful guide for the software developer. When we develop a three-layered architecture, we must understand the layers, roles, and responsibilities. The layered architecture does not mean that a functionality is present in a single layer. Most features offered to the end-user have some display in the presentation layer, handle data in the business logic layer, and store data in the database. The codebase of a three-layered architecture also reflects the separation of the layers. Depending on the capabilities and the conventions on the given platform, the layers all have a dedicated project or package. This structure also enforced one-way dependency, as the dependency-graph of projects/packages usually does not allow circles. That is, if the business layer uses the data access layer, the latter one cannot use the former one. The three-layered architecture is not the only possibility for implementing a data-driven application. Small and simple applications can be build using the two-layered architecture, while larger and more complex applications usually need further separation (e.g., using the microservices architecture). The responsibilities of the layers \u00b6 Let us examine the layers in more detail. The following diagram represents the architecture. Source Microsoft Application Architecture Guide, 2 nd Edition, https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658109%28v%3dpandp.10%29 We will discuss the layers from bottom-to-top. Data sources \u00b6 The most common data source is a database . It can be a relational-, or a NoSQL database. Its main purpose is the stable, reliable and persistent storage of data. This database is usually a software from a well-known third-party. This component is often hosted on a dedicated server accessible through a local network. Sometimes our application might also work with data outside of our database, hosted by third-party services , that we use similarly to databases. For example, you can attach files in Gmail from Google Drive. Gmail, in this example, fetches the list of available files from Google Drive for the user to select the attachment. Google drive is not a database, yet it is used as a data source. These kinds of external services are grouped with our database, in the architectural sense, because they provide data storage and retrieval services, just like a traditional database. We have no information about their internal operations, and there is no need for users to understand it either. Thus, these services are treated similarly in our architecture. Recently, more and more modern database management systems communicate over HTTP and often offer REST-like interfaces. These trends tend to blur the line between databases and external data sources. Data access layer \u00b6 The responsibility of the data access layer, DAL in short, is to provide convenient access to our data. The main functionality offered here is the storage, retrieval, and modification of data items, data records. The data sources and the data access layer combined is often called the data layer . The data access components provide a bridge towards the databases. Their role is to hide the inherent complexity of data management, and provide these as a convenient service to the upper layers. This includes working with SQL commands, as well as mapping the scheme of the database to a different scheme, consumed by the business layer. When the data is not inside our database, the service agents provide similar services and handle the communication aspects with the external service. This entire layer is often built on a particular technology used to communicate with the database, such as ADO.NET, Entity Framework, or JDBC, JPA, etc. The source code in this layer is often tightly coupled with these data access technologies. It is essential to keep these implementations inside this layer and not let it \"leak\" out of here. IMPORTANT In well-designed systems SQL commands appear only in the data access layer; under no circumstances do other layers assemble or execute SQL queries. Since the data modeling scheme used in databases (i.e., the relational model) and the object-oriented modeling are based on different concepts, this layer is responsible for providing a mapping between the two worlds. The foreign keys used by the relational scheme are transformed into associations and compositions, and we may even need to perform data conversion between data types supported by the various systems. We will re-visit these issues later. Communication with an external system, whether is it is a database or a third-party service, requires specific techniques. For example, establishing network connections, performing handshakes, and managing the lifetime of these connections is important for performance reasons. Establishing certain kinds of connections, such as HTTP, are usually simple; but connecting to a database server using proprietary protocols may be more complex. Therefore it is the responsibility of the data access layer to manage these connections and use appropriate techniques, such as connection pooling, when necessary. These details are often automatically controlled by the libraries we use. The management of concurrent data accesses and related problems is also the responsibility if this layer. We will discuss this in detail later. We should keep in mind that multiple users usually use a three-layered application/system at the same time (just think of the Neptun system or a webshop), thus concurrent modifications can happen. We will discuss how this is handled and what type of issues we have to resolve. Business Layer \u00b6 The business layer is the \"heart\" of our application. The databases, the data access layer and the presentation layer are created so that the system can provide the services implemented in the business logic layer. This layer is always specific to the problem domain. The Neptun system manages exams, semester schedules, grades, etc.; a webshop will, on the other hand, manage products, orders, searches, etc. From a high-level point of view this layer is built of: business entities , business components , and business workflows . The entities contain the data of our domain. Depending on the goal of the system, entities might cover products and product rating (e.g., in a webshop), or courses and exams (e.g., in Neptun). The entities only store data; the components are responsible for manipulating these entities. The components implement the building blocks of the complex services offered by our system. Such a building block is, for example, finding a product in a webshop by name. The workflows are built on these basic services. The workflows represent the functionalities that the end-users will carry out. A workflow may use multiple components. A classic example of a workflow is the checkout procedure in a webshop: check the products, finalize the order, produce an invoice, send a confirmation email, etc. Service interfaces \u00b6 The architecture diagram above has a services sub-layer. This is considered to be part of the business layer. Its purpose is to provide an interface through which the business logic layer's services can be accessed. Generally, all layers have such interfaces towards the layer built on top of them. The business layer is not unique in this sense. However, it is common nowadays that a business layer has not one, but multiple such interfaces published. The reason for multiple service interfaces is the presence of multiple presentation layers. Just take Gmail as an example: it has a web frontend and mobile apps too. The UIs are similar, but they do not provide identical behavior; therefore, the services consumed by the presentation layers also vary. It is equally common that our application offers a UI and has public API (application programming interface) for allowing third-party integration. These APIs often offer different functionalities than the user interface and also frequently use other transport technologies; hence the need for a dedicated service interface. By our application publishing an API, it can effectively act as a data source for third-party applications. We will talk more about publishing services over various APIs. We will consider the web services and the REST technologies too. Presentation layer \u00b6 The terminologies presentation layer, UI, and user interface are commonly used interchanged. The responsibility of this layer is the presentation of the data to the end-user in a convenient fashion and the triggering of operations on these data. Visualizing the data must consider how the data is \"consumed.\" For example, when listing lots of records, the UI shall provide filtering and grouping too. Sorting and filtering Depending on the chosen technology stack, sorting and filtering may also involve other layers. When dealing with large data sets, it is usually not useful to send every record to the UI to perform filtering and sorting there. It would be an unnecessary network overhead, and some/most UI technologies are not exactly designed to handle large data sets. On the other hand, if the data set is not extensive, it is convenient to let the UI handle these aspects to provide faster and more fluent responses (not having to forward filtering to the database). While displaying the data, the presentation layer is also responsible for performing simple data transformations, such as the user-friendly display of dates. As we discussed previously, a date might be printed as \"15:23\" or as \"3:23 PM,\" or better yet, as \"15 minutes ago.\" Furthermore, the presentation layer also handles localization. Localization is about displaying all pieces of information according to a chosen culture, such as dates, currencies, numbers. And finally, the UI handles user interactions. When a button is clicked, the UI translates this to an operation it will request from the business layer. User input must be validated. Validation covers filling required fields, accepting only valid email addresses, handling expected number ranges, etc. Validation It is not enough to perform validation only in the user interface. Depending on the technology used, the UI can often be easily bypassed, and the services in the background can be called directly. If this happens and only the UI performs validation, invalid data can get into the system. Therefore the validation is repeated by the business layer too. Regardless, the UI should still perform validation to give instant feedback to the user. This layer is not discussed further in this course. Cross-cutting services / Cross-cutting concerns \u00b6 Cross-cutting services or cross-cutting concerns cover aspects of the application that are not specific to a single layer. When designing our system, we strive to have a unified solution to the problems raised here. Security \u00b6 Security-related services covert user authentication, authorization, tracing and auditing. Authentication answers the question \"who are you\" while authorization determines \"what you are allowed to do in this system.\" Authentication covers not only the authentication on the user interface. We need to authenticate ourselves in the database too, not to mention accessing a third-party external system. Therefore this aspect is present in multiple layers. We have various options. We can use custom authentication, directory-based authentication, or OAuth. After our system has authenticated a user, we can decide to use this identity in external services (e.g. Gmail fetches the user's files from Google Drive ) or use a central identity (e.g. sending an email notification in the name of a send-only central account). Authorization is about access control: whether users can perform specific actions in the system. The UI usually performs some level of authorization (e.g., to hide unavailable functionality), but as discussed with input validation, the business layer must repeat this process. It is crucial, of course, that these two procedures use the same ruleset. Tracing and auditing make sure that we can check who made specific changes in the system. Its main goal is to keep malicious users from erasing their tracks. Recording the steps and operations of a user may be formed in the business login layer as well as in the database. Operation \u00b6 Keeping operational aspects in mind helps build maintainable software. The operational aspect usually covers error handling, logging, monitoring, and configuration management. Centralized error management should catch all types of errors that are raised in an application. These errors need to be recorded (e.g., by logging them), and usually, the end-user needs to be notified (e.g., whether she should retry or wait for something else). Recording all exceptions is vital because errors raised in the lower layers of the application are not \"seen\" by anyone (but the end-user probably) unless these are adequately treated and recorded. Logging and monitoring help both diagnostics and seeing whether a system behaves as intended. Logging is usually performed by writing a text log file. Monitoring, on the other hand, records so-called KPIs, key performance indicators. For example, KPIs are the memory usage, the number of errors, the number of pending requests, etc. And finally, configuration management is about the control of the system configuration. Such configuration is, for example, server addresses (e.g., the database IP). Or we may also want to configure and change the background color of our UI centrally. The standard approach regarding configuration is to not hard-code them but instead offer re-configuration without re-compiling the application when the operational circumstances change. This might involve configuration files or more complicated configuration management tools. We will not deal with these operational aspects in more detail. Often the chosen platform offers solutions to these problems. Communication \u00b6 By communication, we mean the method and format or data exchange between the layers and the components. Choosing the correct approach depends not only on the architecture but on the deployment model too. If the layers are moved to separate servers, network-based communication is needed, while communication between components on the same server can be achieved with simpler methods. Today, most systems use network communication: most commonly to reach the database and other data sources, and frequently between the presentation layer and the service interfaces. Nowadays, most communication is HTTP-based, however when performance is a concern, TPC-based binary communication methods provide better alternatives. And in more complex systems where the layers themselves are distributed across servers too, messages queues are often used. Encryption is also a factor in communication. Communication over public networks must be encrypted. In case of the communication between the UI and the service interface, this typically means HTTPS/TLS. Backend and frontend \u00b6 When we are talking about data-driven systems we often speak about backend and frontend . The frontend is mostly the user interface, that is, the presentation layer (a web application hosted in a browser, a native mobile app, a thick-client desktop app, etc). This is what the user interacts with. The backend is the service that provides the data to the UI: the APIs, the business layer, the data access, and the databases. Depending on the chosen frontend technology, though, parts of the user interface might be created by the backend, though. This is called server-side rendering . In this course, we will talk about backend technologies. Questions to test your knowledge \u00b6 What are the layers in the three-layered architecture? What are their responsibilities? What are the cross-cutting services? Decide, whether the following statements are true or false: The presentation layer is responsible for validating data input. We shall try to avoid using SQL commands in the business layer. The layers in the three-layered architecture are always hosted on separate servers. The three-layered architecture becomes a multi-layered one when the business layer is moved to its own server. The layered architecture ensures that the implementation of the layers can change without this affecting the other layers. The frontend and the presentation layer are one and the same. Exception handling is important only in the business logic layer.","title":"Data-driven systems and the thee- or multi-tier architecture"},{"location":"lecture-notes/architecture/#data-driven-systems-and-the-thee-or-multi-tier-architecture","text":"","title":"Data-driven systems and the thee- or multi-tier architecture"},{"location":"lecture-notes/architecture/#what-is-a-data-driven-system","text":"Every software handles data in some sense since the computer memory stores data, and the software manipulates this data. But not all applications are data-driven. A system or an application is called data-driven if its main purpose is to manage data. In other words, the data-drive application is created to store, display, and manage data. The end-user uses this application to access the data within. A chess game app also stores data in memory: the state of the chessboard. But the chess game app is not created to manipulate this data. The game is designed so that a user can play chess. In a data-driven system, the data itself defines how the application operates. For example, based on a data record's specific attributes, deleting this record may be allowed or may be prohibited. Another example is how the Neptun system enables registration to an exam. The semester schedule, which defines the exam period, is data stored within the system itself. This schedule, stored as data, determines whether the end-user (here: the student) can register for an exam. The fact that the exam period starts on a different day each year does not mean that the software logic (i.e., software code) changes; the data makes the software behave differently.","title":"What is a data-driven system?"},{"location":"lecture-notes/architecture/#data-driven-system-example","text":"The Neptun system is a typical example of a data-driven system. Its purpose is the management of all data related to courses, students, grades. Another example is Gmail : it manages emails, attachments, contacts, etc. Every functionality of the application is about managing and displaying these data. And, of course, the data is stored securely, and every change in the system is persisted (i.e., not lost).","title":"Data-driven system example"},{"location":"lecture-notes/architecture/#the-structure-of-a-data-driven-system","text":"Let us consider Gmail as an example. We would like to build a system which is capable of: sending and receiving emails, has a web application and a connected mobile app too, the UI supports multiple display languages, we can attach files, attachments can be referenced from Google Drive too, we can delay sending a composed email, etc. Let us design How do you start developing such a complex application? This might be a complicated question. Let us begin with a more straightforward question. Supposing that the system already works, and now we want to add the delayed sending feature. How do we do this? We could start a timer when the send button is clicked, and this timer, after a minute, sends the email. This will not work if the browser is closed before the countdown is over. We could add the scheduled date of sending to the email as data. We can translate this as an architectural decision: delayed sending is not the responsibility of the user interface. We did not decide yet, which part of the application will be responsible, but we already know it must not be the UI. Let us consider a similar question. The received date of an email should be displayed to the user according to their preference, i.e., in Europe, 15:23 is the preferred date format, while other parts of the world might prefer 3:23 PM. Does this mean that the email as a data record has multiple received dates? Obviously not. The received date is a single date in a universal representation that is transformed by the UI to the appropriate format. To summarize, we established that there are functionalities that the UI is responsible for, and there are other functionalities that the UI has nothing to do with. Now we arrive at the three- or multi-layer architecture.","title":"The structure of a data-driven system"},{"location":"lecture-notes/architecture/#the-three-or-multi-layered-architecture","text":"Data-driven systems are usually built on the three- or multi-layered architecture. The two names are treaded as synonyms here. This architecture defines three main components: the presentation layer (or UI), the business layer, and the data access layer. Besides, the architecture also includes: a database or external data sources; and the so-called cross-cutting concerns (see later). The application components are organized such that each component belongs to a single layer, and each layer has its responsibilities. This logical grouping of components enables the software developer to design components with clear responsibilities and well-defined boundaries. Why multi-layered when it only has three? The multi-layered terminology enables each of the previously listed layers to be further decomposed into sub-layers depending on complexity. In other words, an architecture is multi-layered if it has more than two layers. (In the two-layered architecture, the UI and the business logic are not separated.) The layers not only have their responsibilities , but also define their interface provided to the layers on top of them. The data access layer specifies the operations the business layer can use to retrieve data; similarly the business layer defines the functionalities the presentation layer can build upon. Each layer is allowed to communicate only with the layer directly beneath . For example, the presentation layer is not allowed to execute a SQL query in the database. At the same time, the implementation behind the well-defined communication interface can change enabling easier maintenance of the software codebase. By having the software split into layers, we can also move the layers to multiple servers (e.g., to handle larger loads). The simplest form is when the presentation layer runs in a browser on the user's machine, while the rest is hosted on a remote server. The database is also frequently offloaded to a dedicated server. Running the various layers on separate servers is usually motivated by performance reasons. Layer / tier The name of the architecture distinguishes the logical and physical separation of the components. Layers mean logical separation hosted on a single machine. Tiers indicate that at least some of the components have dedicated servers. A system built on a well-designed architecture can be used and maintained over a long period. The separation of layers is neither a burden nor a set of mandatory rules. Instead, the layered architecture is a helpful guide for the software developer. When we develop a three-layered architecture, we must understand the layers, roles, and responsibilities. The layered architecture does not mean that a functionality is present in a single layer. Most features offered to the end-user have some display in the presentation layer, handle data in the business logic layer, and store data in the database. The codebase of a three-layered architecture also reflects the separation of the layers. Depending on the capabilities and the conventions on the given platform, the layers all have a dedicated project or package. This structure also enforced one-way dependency, as the dependency-graph of projects/packages usually does not allow circles. That is, if the business layer uses the data access layer, the latter one cannot use the former one. The three-layered architecture is not the only possibility for implementing a data-driven application. Small and simple applications can be build using the two-layered architecture, while larger and more complex applications usually need further separation (e.g., using the microservices architecture).","title":"The three- or multi-layered architecture"},{"location":"lecture-notes/architecture/#the-responsibilities-of-the-layers","text":"Let us examine the layers in more detail. The following diagram represents the architecture. Source Microsoft Application Architecture Guide, 2 nd Edition, https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658109%28v%3dpandp.10%29 We will discuss the layers from bottom-to-top.","title":"The responsibilities of the layers"},{"location":"lecture-notes/architecture/#data-sources","text":"The most common data source is a database . It can be a relational-, or a NoSQL database. Its main purpose is the stable, reliable and persistent storage of data. This database is usually a software from a well-known third-party. This component is often hosted on a dedicated server accessible through a local network. Sometimes our application might also work with data outside of our database, hosted by third-party services , that we use similarly to databases. For example, you can attach files in Gmail from Google Drive. Gmail, in this example, fetches the list of available files from Google Drive for the user to select the attachment. Google drive is not a database, yet it is used as a data source. These kinds of external services are grouped with our database, in the architectural sense, because they provide data storage and retrieval services, just like a traditional database. We have no information about their internal operations, and there is no need for users to understand it either. Thus, these services are treated similarly in our architecture. Recently, more and more modern database management systems communicate over HTTP and often offer REST-like interfaces. These trends tend to blur the line between databases and external data sources.","title":"Data sources"},{"location":"lecture-notes/architecture/#data-access-layer","text":"The responsibility of the data access layer, DAL in short, is to provide convenient access to our data. The main functionality offered here is the storage, retrieval, and modification of data items, data records. The data sources and the data access layer combined is often called the data layer . The data access components provide a bridge towards the databases. Their role is to hide the inherent complexity of data management, and provide these as a convenient service to the upper layers. This includes working with SQL commands, as well as mapping the scheme of the database to a different scheme, consumed by the business layer. When the data is not inside our database, the service agents provide similar services and handle the communication aspects with the external service. This entire layer is often built on a particular technology used to communicate with the database, such as ADO.NET, Entity Framework, or JDBC, JPA, etc. The source code in this layer is often tightly coupled with these data access technologies. It is essential to keep these implementations inside this layer and not let it \"leak\" out of here. IMPORTANT In well-designed systems SQL commands appear only in the data access layer; under no circumstances do other layers assemble or execute SQL queries. Since the data modeling scheme used in databases (i.e., the relational model) and the object-oriented modeling are based on different concepts, this layer is responsible for providing a mapping between the two worlds. The foreign keys used by the relational scheme are transformed into associations and compositions, and we may even need to perform data conversion between data types supported by the various systems. We will re-visit these issues later. Communication with an external system, whether is it is a database or a third-party service, requires specific techniques. For example, establishing network connections, performing handshakes, and managing the lifetime of these connections is important for performance reasons. Establishing certain kinds of connections, such as HTTP, are usually simple; but connecting to a database server using proprietary protocols may be more complex. Therefore it is the responsibility of the data access layer to manage these connections and use appropriate techniques, such as connection pooling, when necessary. These details are often automatically controlled by the libraries we use. The management of concurrent data accesses and related problems is also the responsibility if this layer. We will discuss this in detail later. We should keep in mind that multiple users usually use a three-layered application/system at the same time (just think of the Neptun system or a webshop), thus concurrent modifications can happen. We will discuss how this is handled and what type of issues we have to resolve.","title":"Data access layer"},{"location":"lecture-notes/architecture/#business-layer","text":"The business layer is the \"heart\" of our application. The databases, the data access layer and the presentation layer are created so that the system can provide the services implemented in the business logic layer. This layer is always specific to the problem domain. The Neptun system manages exams, semester schedules, grades, etc.; a webshop will, on the other hand, manage products, orders, searches, etc. From a high-level point of view this layer is built of: business entities , business components , and business workflows . The entities contain the data of our domain. Depending on the goal of the system, entities might cover products and product rating (e.g., in a webshop), or courses and exams (e.g., in Neptun). The entities only store data; the components are responsible for manipulating these entities. The components implement the building blocks of the complex services offered by our system. Such a building block is, for example, finding a product in a webshop by name. The workflows are built on these basic services. The workflows represent the functionalities that the end-users will carry out. A workflow may use multiple components. A classic example of a workflow is the checkout procedure in a webshop: check the products, finalize the order, produce an invoice, send a confirmation email, etc.","title":"Business Layer"},{"location":"lecture-notes/architecture/#service-interfaces","text":"The architecture diagram above has a services sub-layer. This is considered to be part of the business layer. Its purpose is to provide an interface through which the business logic layer's services can be accessed. Generally, all layers have such interfaces towards the layer built on top of them. The business layer is not unique in this sense. However, it is common nowadays that a business layer has not one, but multiple such interfaces published. The reason for multiple service interfaces is the presence of multiple presentation layers. Just take Gmail as an example: it has a web frontend and mobile apps too. The UIs are similar, but they do not provide identical behavior; therefore, the services consumed by the presentation layers also vary. It is equally common that our application offers a UI and has public API (application programming interface) for allowing third-party integration. These APIs often offer different functionalities than the user interface and also frequently use other transport technologies; hence the need for a dedicated service interface. By our application publishing an API, it can effectively act as a data source for third-party applications. We will talk more about publishing services over various APIs. We will consider the web services and the REST technologies too.","title":"Service interfaces"},{"location":"lecture-notes/architecture/#presentation-layer","text":"The terminologies presentation layer, UI, and user interface are commonly used interchanged. The responsibility of this layer is the presentation of the data to the end-user in a convenient fashion and the triggering of operations on these data. Visualizing the data must consider how the data is \"consumed.\" For example, when listing lots of records, the UI shall provide filtering and grouping too. Sorting and filtering Depending on the chosen technology stack, sorting and filtering may also involve other layers. When dealing with large data sets, it is usually not useful to send every record to the UI to perform filtering and sorting there. It would be an unnecessary network overhead, and some/most UI technologies are not exactly designed to handle large data sets. On the other hand, if the data set is not extensive, it is convenient to let the UI handle these aspects to provide faster and more fluent responses (not having to forward filtering to the database). While displaying the data, the presentation layer is also responsible for performing simple data transformations, such as the user-friendly display of dates. As we discussed previously, a date might be printed as \"15:23\" or as \"3:23 PM,\" or better yet, as \"15 minutes ago.\" Furthermore, the presentation layer also handles localization. Localization is about displaying all pieces of information according to a chosen culture, such as dates, currencies, numbers. And finally, the UI handles user interactions. When a button is clicked, the UI translates this to an operation it will request from the business layer. User input must be validated. Validation covers filling required fields, accepting only valid email addresses, handling expected number ranges, etc. Validation It is not enough to perform validation only in the user interface. Depending on the technology used, the UI can often be easily bypassed, and the services in the background can be called directly. If this happens and only the UI performs validation, invalid data can get into the system. Therefore the validation is repeated by the business layer too. Regardless, the UI should still perform validation to give instant feedback to the user. This layer is not discussed further in this course.","title":"Presentation layer"},{"location":"lecture-notes/architecture/#cross-cutting-services-cross-cutting-concerns","text":"Cross-cutting services or cross-cutting concerns cover aspects of the application that are not specific to a single layer. When designing our system, we strive to have a unified solution to the problems raised here.","title":"Cross-cutting services / Cross-cutting concerns"},{"location":"lecture-notes/architecture/#security","text":"Security-related services covert user authentication, authorization, tracing and auditing. Authentication answers the question \"who are you\" while authorization determines \"what you are allowed to do in this system.\" Authentication covers not only the authentication on the user interface. We need to authenticate ourselves in the database too, not to mention accessing a third-party external system. Therefore this aspect is present in multiple layers. We have various options. We can use custom authentication, directory-based authentication, or OAuth. After our system has authenticated a user, we can decide to use this identity in external services (e.g. Gmail fetches the user's files from Google Drive ) or use a central identity (e.g. sending an email notification in the name of a send-only central account). Authorization is about access control: whether users can perform specific actions in the system. The UI usually performs some level of authorization (e.g., to hide unavailable functionality), but as discussed with input validation, the business layer must repeat this process. It is crucial, of course, that these two procedures use the same ruleset. Tracing and auditing make sure that we can check who made specific changes in the system. Its main goal is to keep malicious users from erasing their tracks. Recording the steps and operations of a user may be formed in the business login layer as well as in the database.","title":"Security"},{"location":"lecture-notes/architecture/#operation","text":"Keeping operational aspects in mind helps build maintainable software. The operational aspect usually covers error handling, logging, monitoring, and configuration management. Centralized error management should catch all types of errors that are raised in an application. These errors need to be recorded (e.g., by logging them), and usually, the end-user needs to be notified (e.g., whether she should retry or wait for something else). Recording all exceptions is vital because errors raised in the lower layers of the application are not \"seen\" by anyone (but the end-user probably) unless these are adequately treated and recorded. Logging and monitoring help both diagnostics and seeing whether a system behaves as intended. Logging is usually performed by writing a text log file. Monitoring, on the other hand, records so-called KPIs, key performance indicators. For example, KPIs are the memory usage, the number of errors, the number of pending requests, etc. And finally, configuration management is about the control of the system configuration. Such configuration is, for example, server addresses (e.g., the database IP). Or we may also want to configure and change the background color of our UI centrally. The standard approach regarding configuration is to not hard-code them but instead offer re-configuration without re-compiling the application when the operational circumstances change. This might involve configuration files or more complicated configuration management tools. We will not deal with these operational aspects in more detail. Often the chosen platform offers solutions to these problems.","title":"Operation"},{"location":"lecture-notes/architecture/#communication","text":"By communication, we mean the method and format or data exchange between the layers and the components. Choosing the correct approach depends not only on the architecture but on the deployment model too. If the layers are moved to separate servers, network-based communication is needed, while communication between components on the same server can be achieved with simpler methods. Today, most systems use network communication: most commonly to reach the database and other data sources, and frequently between the presentation layer and the service interfaces. Nowadays, most communication is HTTP-based, however when performance is a concern, TPC-based binary communication methods provide better alternatives. And in more complex systems where the layers themselves are distributed across servers too, messages queues are often used. Encryption is also a factor in communication. Communication over public networks must be encrypted. In case of the communication between the UI and the service interface, this typically means HTTPS/TLS.","title":"Communication"},{"location":"lecture-notes/architecture/#backend-and-frontend","text":"When we are talking about data-driven systems we often speak about backend and frontend . The frontend is mostly the user interface, that is, the presentation layer (a web application hosted in a browser, a native mobile app, a thick-client desktop app, etc). This is what the user interacts with. The backend is the service that provides the data to the UI: the APIs, the business layer, the data access, and the databases. Depending on the chosen frontend technology, though, parts of the user interface might be created by the backend, though. This is called server-side rendering . In this course, we will talk about backend technologies.","title":"Backend and frontend"},{"location":"lecture-notes/architecture/#questions-to-test-your-knowledge","text":"What are the layers in the three-layered architecture? What are their responsibilities? What are the cross-cutting services? Decide, whether the following statements are true or false: The presentation layer is responsible for validating data input. We shall try to avoid using SQL commands in the business layer. The layers in the three-layered architecture are always hosted on separate servers. The three-layered architecture becomes a multi-layered one when the business layer is moved to its own server. The layered architecture ensures that the implementation of the layers can change without this affecting the other layers. The frontend and the presentation layer are one and the same. Exception handling is important only in the business logic layer.","title":"Questions to test your knowledge"},{"location":"lecture-notes/async/","text":"Asynchronous queries and DTOs (sample WebAPI application) \u00b6 This topic discusses server-side asynchronous queries and the use of DTOs (Data Transfer Objects) through an example application. The web application is an ASP.NET Core WebApi server using Entity Framework data access. The functionality discussed here is the management of a webshop cart. Author The original author of this lecture note in Hungarian is M\u00e1t\u00e9 ZERGI. Asynchronous execution \u00b6 Most of our web applications use a database. When communicating with the database, we have to be aware that: the database might not always be available, the connection might not be stable and fast, and the database might be slow to respond. Therefore, we need to prepare to wait for the results queried from the database in our application. Using asynchronous techniques in the web application, we can make sure that the resources, such as the web server's threads, are used efficiently even while waiting for the database. Asynchronous execution vs. concurrent execution Asynchronous execution is not the same as concurrent execution. A web server always processes incoming requests concurrently (i.e., multiple requests are in the system at all times). On the other hand, asynchronous execution is about handling a single request efficiently by not blocking any thread for waiting to complete an I/O operation (such as database access, file access, or network communication). The database of the sample application \u00b6 The sample application presented here uses a simplified database structure as follows. For simplicity, the UserId of the carts is not a foreign key to a Users table, but a fixed constant of 1. Obviously, in a real-life example, UserId would be a foreign key. The Products stores the things the webshop sells; the Manufactureres contain the producers of these products; finally, OrderItems stores the content of the cart. Server application \u00b6 We would like to create a REST-compatible service for managing the webshop cart using ASP.NET Core WebApi and Entity Framework. Let us follow these steps: Create the C# model of the database tables, Create the database context, Create Data Transfer Objects that represent the information queried by the client in a format convenient for the client, Create WebApi controllers We will go through each of these steps. Create the C# model of the database tables \u00b6 The C# classes that map the database tables are usually placed into a folder often called Models in ASP.NET Core. The following is the class for the Products table. namespace WebshopApi.Models { public class Product { public string Name { get ; set ; } public int ManufacturerID { get ; set ; } public int Price { get ; set ; } public int ID { get ; set ; } } } The following is the class for the Manufacturers table. namespace WebshopApi.Models { public class Manufacturer { public string Name { get ; set ; } public int ID { get ; set ; } } } The following is the class for the OrderItems table. namespace WebshopApi.Models { public class OrderItem { public int ID { get ; set ; } public int ProductID { get ; set ; } public int CartID { get ; set ; } public int Pieces { get ; set ; } } } The following is the class for the Carts table. namespace WebshopApi.Models { public class Cart { public int ID { get ; set ; } public int UserID { get ; set ; } } } Note, that the sole purpose of these classes is to map the data exactly as in the database. Create the database context \u00b6 After mapping the tables, we can now create the class that will represent our database: the DbContext class. This class must inherit from the Entity Framework Core DbContext class. namespace WebshopApi.Models { public class WebshopContext : DbContext { public WebshopContext ( DbContextOptions < WebshopContext > options ) : base ( options ) { } public DbSet < Product > Products { get ; set ; } public DbSet < Manufacturer > Manufacturers { get ; set ; } public DbSet < Cart > Carts { get ; set ; } public DbSet < OrderItem > OrderItems { get ; set ; } } } Each table in the database corresponds to a DbSet property as defined above. Each DbSet specified the type of the entity it stores; e.g., DbSet<Products> will store entities of type Product . The DbContextOptions configures the access to the database, such as the connection string. This is usually configured in the Startup class: public class Startup { // ... // This method is called by the runtime to populate the services of the DI container public void ConfigureServices ( IServiceCollection services ) { services . AddDbContext < WebshopContext >( opt => opt . UseSqlServer ( @\"Data Source=(localdb)\\mssqllocaldb;Initial Catalog=Webshop;Integrated Security=True\" )); // ... } } Defining Data Transfer Objects \u00b6 We have the direct mapping of the database into C# classes. Let us now consider how does the cart of the webshop usually look like: it may contain multiple items. While the OrderItem class can represent a single item, our cart is a list of items. This list of items is what we shall describe with a so-called Data Transfer Object : it is a class that gathers data for the client . Definition: Data Transfer Object A container object that transfers data between application (here: between the client and the server). With the use of DTOs, we can pack all necessary information into one object, making it not only more convenient for the client, but also better in terms of performance: We only send information to the client that it really needs. Furthermore, a DTO can gather various information and send them all in one go. Let us consider, what information do we need to display the cart in the client: the products, the amount in the cart for each, and the total number of items. Class OrderItem has superfluous data that the client does not need: CartID and ID . Removing these properties we can arrive at a class very similar to OrderItem ; but it is still just one item of the cart. The class we can create this way is called CartItem . This CartItem has a Product that also stores some unnecessary data, and some properties that might need adding . For example, the manufacturer of the Product should contain the name of the manufacturer and not the ManufacturerID . Let us, therefore, create a new Product class, and our CartItem should store this class instead. These CartItem objects are gathered in a list , and let us add the total number of items in the cart. This will give us our last DTO, the UserCart . An instance of this UserCart is what that the client will receive. The DTO classes are usually separated from the database entities. Let us put these classes in a DTOs folder. CartItem class contains the data from an OrderItem without the unnecessary properties. namespace WebshopApi.DTOs { public class CartItem { public Product Product { get ; set ; } // This is the product that no longer has the ID of the manufacturer, but the name instead public int Amount { get ; set ; } // The amount in the cart } } The the matching Product DTO: namespace WebshopApi.DTOs { public class Product { public string ProductName { get ; set ; } // The product name, e.g., AB123 Full HD TV public string Manufacturer { get ; set ; } // A !!name!! of the manufacturer, e.g., BMETV public int Price { get ; set ; } // Price of the product public int ID { get ; set ; } // ID of the product } } Why is there an ID here? We might be curious why there is an ID here. An item in the cart is identified by the product itself. E.g., further details of the product in the cart can be queried by knowing this ID. We could create a new identifier for the item in the cart; but the product's ID is sufficient. UserCart collects all items, and adds a total number. namespace WebshopApi.DTOs { public class UserCart { public List < CartItem > CartPieces { get ; set ; } public int NumberOfItems { get ; set ; } } } By storing the CartItem s in a list, we make the job of the client easier. When rendering the contents of the cart the client code only needs to iterate through the array contents. This UserCart is created by gathering the required information, such as the product details, then adding up the number of items. Creating the WebApi controller \u00b6 The controllers are usually placed into the Controllers folder. Here, we have a single controller that uses the WebshopDbContext directly and handles the HTTP queries. This is where we can introduce asynchronous queries. Let us see an example right away: The following is a GET query to fetch all carts. [HttpGet] public async Task < ActionResult < IEnumerable < Cart >>> GetCarts () { var carts = await _context . Carts . ToListAsync (); return carts ; } Let us note the async keyword in the declaration and the Task return type, along with the await instruction in the body. Together, these are called async-await . Let us make sense of all these: The method returns a list of Cart instances, that is, IEnumerable<Cart> ; Which, according to WebApi controller conventions, is wrapped in an ActionResult ; And this whole thing is wrapped in a Task . This one is due to the asynchronous behavior. Although this seems complicated, every part of this is for a different reason. Let us examine the asynchronous behavior: the Task type, and the await keyword. This definition of the method yields a so-called promise (some languages use this terminology) that represents the result of a task that will be completed in the future. Why do we need this? Because this makes the execution of the controller method asynchronous. When the execution arrives at an await keyword, the thread that processes the request, will stop further processing of this query and will start processing a new query instead. Ok, but again, why? Because we know that the operation \"behind\" the await will take time: it has to go to the database and fetch data from there. If the thread stopped here to wait for the result, it would be wasting resources. Instead of having the thread wait for the result, the task is handed off to a system in the background (the operating system and the .NET asynchronous I/O subsystem - will not go into details here), and we request notification when the results are available. Once this happens (the results from the database are, in fact, ready), the processing of the query will continue. In other words, the threads used by our application will always do useful work instead of waiting (or being suspended due to waiting). Consequently, this means that serving the HTTP requests need fewer operating system threads, therefore making better use of available computational resources. The previous method can be simplified in syntax by getting rid of the local variable and returning the Task directly. Functionally, this implementation works (almost) identically, but the one above makes the explanation easier. [HttpGet] public Task < ActionResult < IEnumerable < Carts >>> GetCarts () { return _context . Carts . ToListAsync (); // no await and the method declaration has no async } The ***Async methods The methods to fetch data from the database (e.g., ToList , First , All , Find , etc.) all have their ...Async pairs. These methods provide the basis for asynchronous execution. We will not discuss the execution in more details. What we need to remember, is that in order for our controller method to be asynchronous, there must be an asynchronous operation \"underneath\" (here: in Entity Framework). Let us also see a complex example: gather all data of the cart: [HttpGet(\"{id}\")] public async Task < ActionResult < UserCart >> GetCart ( int id ) { // asynchronous query to find the cart var cartRecord = await _context . Carts . FindAsync ( id ); if ( cartRecord == null ) return NotFound (); // build the query var productsquery = from p1 in _context . Products join m1 in _context . Manufacturers on p1 . ManufacturerID equals m1 . ID select new Product ( p1 . ID , m1 . Name , p1 . Name , p1 . Price ); // create the Product DTO // asynchronous evaluation var products = await productsquery . ToListAsync (). ConfigureAwait ( false ); // asynchronous request to get order items var orderitemsquery = from oi in _context . OrderItems where oi . CartID == cartRecord . ID select oi ; var orderitems = await orderitemsquery . ToListAsync (). ConfigureAwait ( false ); // further operations are synchronous, as every result in in memory already // Find the products in the cart // match them to the order items and crate a CartItem DTO var cartitems = products . Join ( orderitems , p => p . ID , oi => oi . ProductID , ( p , v ) => new CartItem ( p , v . Pieces )). ToList (); // Finally, the result is a UserCart DTO return new UserCart () { CartPieces = cartitems , NumberOfItems = cartitems . Count () } } Note, how all asynchronous method calls are await -ed! But once we have all the data from the database, we can continue in a synchronous fashion. The ConfigureAwait method The ConfigureAwait(false) gives us further options regarding performance optimization. With this option we signal that the await -ed result set can be processed by any available thread, not just the one that started the processing originally. In server-side applications this is usually the correct behavior, however, this is not true for all asynchronous operations (e.g., UI threads are usually special and in that case not any thread can continue). For more details, see: https://devblogs.microsoft.com/dotnet/configureawait-faq/ . Finally, let us see an example using FirstOrDefaultAsync to process a POST query that alters the contents of the cart (adds or removed items): // DTO describing the inputs of the operation namespace WebshopApi.Models { public class PostCartItemArgs { public int CartId { get ; set ; } public int ProductId { get ; set ; } public int Amount { get ; set ; } } } ////////////////////////////////////////////////////////////////////////////// // The HTTP handler in the controller [HttpPost] public async Task < IActionResult > PostCartItem ([ FromBody ] PostCartItemArgs data ) { // find the cart by ID var cart = await _context . Carts . FindAsync ( data . CartId ). ConfigureAwait ( false ); if ( cart == null ) return NotFound (); // find the order items in this cart matching the provided product var orderitemquery = from oi in _context . OrderItems where ( oi . CartID == data . Id && oi . ProductID == data . ProductId ) select oi ; // FirstOrDefault so that if there is no match, the result is null var orderitem = await orderitemquery . FirstOrDefaultAsync (). ConfigureAwait ( false ); if ( orderitem == null ) { // If there was no such item in the cart, add a new OrderItem _context . OrderItems . Add ( new OrderItems { CartID = data . Id , Amount = data . Amount , ProductID = data . ProductId }); } else { // If there is an item in the cart orderitem . Amount += data . Amount ; // If the amount is zero, it means, removed from the cart if ( orderitem . Amount == 0 ) _context . OrderItems . Remove ( orderitem ); } await _context . SaveChangesAsync (); // await here too, since this will need to go to the database return NoContent (); } Who waits for the Task result of the controller? Every async method has to be await -ed somewhere. When it comes to a WebApi controller, it will be the ASP.NET Core framework that invokes this method, and it will \"wait\" for the result before serializing it to JSON to send to the client. Sample code \u00b6 The source of the sample application is available here: https://github.com/mzergi/WebshopApi/","title":"Asynchronous queries and DTOs (sample WebAPI application)"},{"location":"lecture-notes/async/#asynchronous-queries-and-dtos-sample-webapi-application","text":"This topic discusses server-side asynchronous queries and the use of DTOs (Data Transfer Objects) through an example application. The web application is an ASP.NET Core WebApi server using Entity Framework data access. The functionality discussed here is the management of a webshop cart. Author The original author of this lecture note in Hungarian is M\u00e1t\u00e9 ZERGI.","title":"Asynchronous queries and DTOs (sample WebAPI application)"},{"location":"lecture-notes/async/#asynchronous-execution","text":"Most of our web applications use a database. When communicating with the database, we have to be aware that: the database might not always be available, the connection might not be stable and fast, and the database might be slow to respond. Therefore, we need to prepare to wait for the results queried from the database in our application. Using asynchronous techniques in the web application, we can make sure that the resources, such as the web server's threads, are used efficiently even while waiting for the database. Asynchronous execution vs. concurrent execution Asynchronous execution is not the same as concurrent execution. A web server always processes incoming requests concurrently (i.e., multiple requests are in the system at all times). On the other hand, asynchronous execution is about handling a single request efficiently by not blocking any thread for waiting to complete an I/O operation (such as database access, file access, or network communication).","title":"Asynchronous execution"},{"location":"lecture-notes/async/#the-database-of-the-sample-application","text":"The sample application presented here uses a simplified database structure as follows. For simplicity, the UserId of the carts is not a foreign key to a Users table, but a fixed constant of 1. Obviously, in a real-life example, UserId would be a foreign key. The Products stores the things the webshop sells; the Manufactureres contain the producers of these products; finally, OrderItems stores the content of the cart.","title":"The database of the sample application"},{"location":"lecture-notes/async/#server-application","text":"We would like to create a REST-compatible service for managing the webshop cart using ASP.NET Core WebApi and Entity Framework. Let us follow these steps: Create the C# model of the database tables, Create the database context, Create Data Transfer Objects that represent the information queried by the client in a format convenient for the client, Create WebApi controllers We will go through each of these steps.","title":"Server application"},{"location":"lecture-notes/async/#create-the-c-model-of-the-database-tables","text":"The C# classes that map the database tables are usually placed into a folder often called Models in ASP.NET Core. The following is the class for the Products table. namespace WebshopApi.Models { public class Product { public string Name { get ; set ; } public int ManufacturerID { get ; set ; } public int Price { get ; set ; } public int ID { get ; set ; } } } The following is the class for the Manufacturers table. namespace WebshopApi.Models { public class Manufacturer { public string Name { get ; set ; } public int ID { get ; set ; } } } The following is the class for the OrderItems table. namespace WebshopApi.Models { public class OrderItem { public int ID { get ; set ; } public int ProductID { get ; set ; } public int CartID { get ; set ; } public int Pieces { get ; set ; } } } The following is the class for the Carts table. namespace WebshopApi.Models { public class Cart { public int ID { get ; set ; } public int UserID { get ; set ; } } } Note, that the sole purpose of these classes is to map the data exactly as in the database.","title":"Create the C# model of the database tables"},{"location":"lecture-notes/async/#create-the-database-context","text":"After mapping the tables, we can now create the class that will represent our database: the DbContext class. This class must inherit from the Entity Framework Core DbContext class. namespace WebshopApi.Models { public class WebshopContext : DbContext { public WebshopContext ( DbContextOptions < WebshopContext > options ) : base ( options ) { } public DbSet < Product > Products { get ; set ; } public DbSet < Manufacturer > Manufacturers { get ; set ; } public DbSet < Cart > Carts { get ; set ; } public DbSet < OrderItem > OrderItems { get ; set ; } } } Each table in the database corresponds to a DbSet property as defined above. Each DbSet specified the type of the entity it stores; e.g., DbSet<Products> will store entities of type Product . The DbContextOptions configures the access to the database, such as the connection string. This is usually configured in the Startup class: public class Startup { // ... // This method is called by the runtime to populate the services of the DI container public void ConfigureServices ( IServiceCollection services ) { services . AddDbContext < WebshopContext >( opt => opt . UseSqlServer ( @\"Data Source=(localdb)\\mssqllocaldb;Initial Catalog=Webshop;Integrated Security=True\" )); // ... } }","title":"Create the database context"},{"location":"lecture-notes/async/#defining-data-transfer-objects","text":"We have the direct mapping of the database into C# classes. Let us now consider how does the cart of the webshop usually look like: it may contain multiple items. While the OrderItem class can represent a single item, our cart is a list of items. This list of items is what we shall describe with a so-called Data Transfer Object : it is a class that gathers data for the client . Definition: Data Transfer Object A container object that transfers data between application (here: between the client and the server). With the use of DTOs, we can pack all necessary information into one object, making it not only more convenient for the client, but also better in terms of performance: We only send information to the client that it really needs. Furthermore, a DTO can gather various information and send them all in one go. Let us consider, what information do we need to display the cart in the client: the products, the amount in the cart for each, and the total number of items. Class OrderItem has superfluous data that the client does not need: CartID and ID . Removing these properties we can arrive at a class very similar to OrderItem ; but it is still just one item of the cart. The class we can create this way is called CartItem . This CartItem has a Product that also stores some unnecessary data, and some properties that might need adding . For example, the manufacturer of the Product should contain the name of the manufacturer and not the ManufacturerID . Let us, therefore, create a new Product class, and our CartItem should store this class instead. These CartItem objects are gathered in a list , and let us add the total number of items in the cart. This will give us our last DTO, the UserCart . An instance of this UserCart is what that the client will receive. The DTO classes are usually separated from the database entities. Let us put these classes in a DTOs folder. CartItem class contains the data from an OrderItem without the unnecessary properties. namespace WebshopApi.DTOs { public class CartItem { public Product Product { get ; set ; } // This is the product that no longer has the ID of the manufacturer, but the name instead public int Amount { get ; set ; } // The amount in the cart } } The the matching Product DTO: namespace WebshopApi.DTOs { public class Product { public string ProductName { get ; set ; } // The product name, e.g., AB123 Full HD TV public string Manufacturer { get ; set ; } // A !!name!! of the manufacturer, e.g., BMETV public int Price { get ; set ; } // Price of the product public int ID { get ; set ; } // ID of the product } } Why is there an ID here? We might be curious why there is an ID here. An item in the cart is identified by the product itself. E.g., further details of the product in the cart can be queried by knowing this ID. We could create a new identifier for the item in the cart; but the product's ID is sufficient. UserCart collects all items, and adds a total number. namespace WebshopApi.DTOs { public class UserCart { public List < CartItem > CartPieces { get ; set ; } public int NumberOfItems { get ; set ; } } } By storing the CartItem s in a list, we make the job of the client easier. When rendering the contents of the cart the client code only needs to iterate through the array contents. This UserCart is created by gathering the required information, such as the product details, then adding up the number of items.","title":"Defining Data Transfer Objects"},{"location":"lecture-notes/async/#creating-the-webapi-controller","text":"The controllers are usually placed into the Controllers folder. Here, we have a single controller that uses the WebshopDbContext directly and handles the HTTP queries. This is where we can introduce asynchronous queries. Let us see an example right away: The following is a GET query to fetch all carts. [HttpGet] public async Task < ActionResult < IEnumerable < Cart >>> GetCarts () { var carts = await _context . Carts . ToListAsync (); return carts ; } Let us note the async keyword in the declaration and the Task return type, along with the await instruction in the body. Together, these are called async-await . Let us make sense of all these: The method returns a list of Cart instances, that is, IEnumerable<Cart> ; Which, according to WebApi controller conventions, is wrapped in an ActionResult ; And this whole thing is wrapped in a Task . This one is due to the asynchronous behavior. Although this seems complicated, every part of this is for a different reason. Let us examine the asynchronous behavior: the Task type, and the await keyword. This definition of the method yields a so-called promise (some languages use this terminology) that represents the result of a task that will be completed in the future. Why do we need this? Because this makes the execution of the controller method asynchronous. When the execution arrives at an await keyword, the thread that processes the request, will stop further processing of this query and will start processing a new query instead. Ok, but again, why? Because we know that the operation \"behind\" the await will take time: it has to go to the database and fetch data from there. If the thread stopped here to wait for the result, it would be wasting resources. Instead of having the thread wait for the result, the task is handed off to a system in the background (the operating system and the .NET asynchronous I/O subsystem - will not go into details here), and we request notification when the results are available. Once this happens (the results from the database are, in fact, ready), the processing of the query will continue. In other words, the threads used by our application will always do useful work instead of waiting (or being suspended due to waiting). Consequently, this means that serving the HTTP requests need fewer operating system threads, therefore making better use of available computational resources. The previous method can be simplified in syntax by getting rid of the local variable and returning the Task directly. Functionally, this implementation works (almost) identically, but the one above makes the explanation easier. [HttpGet] public Task < ActionResult < IEnumerable < Carts >>> GetCarts () { return _context . Carts . ToListAsync (); // no await and the method declaration has no async } The ***Async methods The methods to fetch data from the database (e.g., ToList , First , All , Find , etc.) all have their ...Async pairs. These methods provide the basis for asynchronous execution. We will not discuss the execution in more details. What we need to remember, is that in order for our controller method to be asynchronous, there must be an asynchronous operation \"underneath\" (here: in Entity Framework). Let us also see a complex example: gather all data of the cart: [HttpGet(\"{id}\")] public async Task < ActionResult < UserCart >> GetCart ( int id ) { // asynchronous query to find the cart var cartRecord = await _context . Carts . FindAsync ( id ); if ( cartRecord == null ) return NotFound (); // build the query var productsquery = from p1 in _context . Products join m1 in _context . Manufacturers on p1 . ManufacturerID equals m1 . ID select new Product ( p1 . ID , m1 . Name , p1 . Name , p1 . Price ); // create the Product DTO // asynchronous evaluation var products = await productsquery . ToListAsync (). ConfigureAwait ( false ); // asynchronous request to get order items var orderitemsquery = from oi in _context . OrderItems where oi . CartID == cartRecord . ID select oi ; var orderitems = await orderitemsquery . ToListAsync (). ConfigureAwait ( false ); // further operations are synchronous, as every result in in memory already // Find the products in the cart // match them to the order items and crate a CartItem DTO var cartitems = products . Join ( orderitems , p => p . ID , oi => oi . ProductID , ( p , v ) => new CartItem ( p , v . Pieces )). ToList (); // Finally, the result is a UserCart DTO return new UserCart () { CartPieces = cartitems , NumberOfItems = cartitems . Count () } } Note, how all asynchronous method calls are await -ed! But once we have all the data from the database, we can continue in a synchronous fashion. The ConfigureAwait method The ConfigureAwait(false) gives us further options regarding performance optimization. With this option we signal that the await -ed result set can be processed by any available thread, not just the one that started the processing originally. In server-side applications this is usually the correct behavior, however, this is not true for all asynchronous operations (e.g., UI threads are usually special and in that case not any thread can continue). For more details, see: https://devblogs.microsoft.com/dotnet/configureawait-faq/ . Finally, let us see an example using FirstOrDefaultAsync to process a POST query that alters the contents of the cart (adds or removed items): // DTO describing the inputs of the operation namespace WebshopApi.Models { public class PostCartItemArgs { public int CartId { get ; set ; } public int ProductId { get ; set ; } public int Amount { get ; set ; } } } ////////////////////////////////////////////////////////////////////////////// // The HTTP handler in the controller [HttpPost] public async Task < IActionResult > PostCartItem ([ FromBody ] PostCartItemArgs data ) { // find the cart by ID var cart = await _context . Carts . FindAsync ( data . CartId ). ConfigureAwait ( false ); if ( cart == null ) return NotFound (); // find the order items in this cart matching the provided product var orderitemquery = from oi in _context . OrderItems where ( oi . CartID == data . Id && oi . ProductID == data . ProductId ) select oi ; // FirstOrDefault so that if there is no match, the result is null var orderitem = await orderitemquery . FirstOrDefaultAsync (). ConfigureAwait ( false ); if ( orderitem == null ) { // If there was no such item in the cart, add a new OrderItem _context . OrderItems . Add ( new OrderItems { CartID = data . Id , Amount = data . Amount , ProductID = data . ProductId }); } else { // If there is an item in the cart orderitem . Amount += data . Amount ; // If the amount is zero, it means, removed from the cart if ( orderitem . Amount == 0 ) _context . OrderItems . Remove ( orderitem ); } await _context . SaveChangesAsync (); // await here too, since this will need to go to the database return NoContent (); } Who waits for the Task result of the controller? Every async method has to be await -ed somewhere. When it comes to a WebApi controller, it will be the ASP.NET Core framework that invokes this method, and it will \"wait\" for the result before serializing it to JSON to send to the client.","title":"Creating the WebApi controller"},{"location":"lecture-notes/async/#sample-code","text":"The source of the sample application is available here: https://github.com/mzergi/WebshopApi/","title":"Sample code"},{"location":"lecture-notes/di/","text":"Dependency Injection ASP.NET Core environment \u00b6 Definition Dependency Injection (DI) is programming technique that makes a class independent of its dependencies. It's a key enabler for decomposing an application into loosely coupled components. More precisely: Dependency Injection is a mechanism to decouple the creation of dependency graphs for a class from its class definition . Of course, the above definition is very abstract, and based on the short definition it's hard to understand what problems DI is trying to solve, and how DI is trying to solve them. In the following chapters, we will use an example to put DI into context and to learn the basics of the DI related services build into ASP.NET Core. Goals of DI Facilitated extensibility and maintainability Improved unit testability Facilitated code reuse Sample application The sample C# code is available here: https://github.com/bmeviauac01/todoapi-di-sample Example phase 1 - service class with wired in dependencies \u00b6 In this example, based on code snippets we look at parts of a to-do list (TODO) application that sends to-do item related email notifications. Note: The code is minimalistic for succinctness. The \"entry point\" of our example is the SendReminderIfNeeded operation of the ToDoService class. // Class for managing todo items public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // It checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { NotificationService notificationService = new NotificationService ( smtpAddress ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } // ... } // Entity class, encapsulates information about a todo task public class TodoItem { // Database key public long Id { get ; set ; } // Name/description of the task public string Name { get ; set ; } // Indicates if the task has been completed public bool IsComplete { get ; set ; } // It's possible to assign a contact person to a task: -1 indicated no contact // person is assigned, otherwise the id of the contact person public int LinkedContactId { get ; set ; } = - 1 ; } In the code above ( ToDoService.SendReminderIfNeeded ) we see that the essential logic of sending an e-mail is to be found in the NotificationService class. Indeed, this class is at the center of our investigation. The following code snippet describes the code for the NotificationService class and its dependencies: // Class for sending notifications class NotificationService { // Dependencies of the class EMailSender _emailSender ; Logger _logger ; ContactRepository _contactRepository ; public NotificationService ( string smtpAddress ) { _logger = new Logger (); _emailSender = new EMailSender ( _logger , smtpAddress ); _contactRepository = new ContactRepository (); } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } // Class supporting loggin public class Logger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail notifications public class EMailSender { Logger _logger ; string _smtpAddress ; public EMailSender ( Logger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } A few general thoughts: The NotificationService class has several dependencies ( EMailSender , Logger , ContactRepository classes) and it implements its services based on these dependency classes. Dependency classes may have additional dependencies: EMailSender is a great example of this, it's dependent on the Logger class. Note: NotificationService , EMailSender , Logger , ContactRepository classes are considered service classes because they contain business logic, not just encapsulate data, such as TodoItem . As we could see the SendEmailReminder operation is actually served by an object graph, where NotificationService is the root object, it has three dependencies, and its dependencies have further dependencies. The following figure illustrates this object graph: Note One may ask why we considered NotificationService , and not ToDoService as the root object. Actually it just depends on our viewpoint: for simplicity we considered ToDoService as an entry point (a \"client\") for fulfilling a request, so that we have less classes to put under scrutiny. In a real life application we probably would consider ToDoService as part of the dependency graph as well. Let's review the key features of this solution: The class instantiates its dependencies itself Class depends on the specific type of its dependencies (and not on interfaces, \"abstractions\") This approach has a couple of significant and rather painful drawbacks: Rigidity, lack of extensibility . NotificationService (without modification) cannot work with other mailing, logging and contact repository implementations (but only with the the wired in EMailSender , Logger and ContactRepository classes). That is, e.g. we can't use it with any other logging component, or e.g. use it with a contact repository that operates via a different data source/storage mechanism. Lack of unit testability . The NotificationService (without modification) cannot be unit tested. This would require replacing the EMailSender , Logger and ContactRepository dependencies with variants that provide fixed/expected responses for a given input. Keep in mind that unit testing is about testing the behavior of a class independently from its dependencies. In our example, instead of using the database base ContactRepository, we would need a ContactRepository implementation that could serve requests very quickly from memory with values supporting the specific test cases. There is one more subtle inconvenience that is hard to notice at first sight. In our example we had to provide the smtpAddress parameter to the NotificationService constructor, so that it can forward it to its EMailSender dependency. However, smtpAddress is a parameter completely meaningless for NotificationService , it has nothing to do with this piece of information. Unfortunately, we are forced to pass smtpAddress thorough NotificationService , as NotificationService is the class instantiating the EMailSender object. We could eliminate this by somehow instantiating EMailSender independently of NotificationService . In the next steps, we redesign our solution so that we can eliminate most of the downsides of the current rigid approach. Example phase 2 - service class with manual dependency injection \u00b6 Redesign our former solution the functional requirements are unchanged. The most important principles of transformation are the following: Dependencies will be based on abstractions/interfaces Classes will no longer instantiate their dependencies themselves Let's jump right into the code of the improved solution and analyze the differences: public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // Checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { var logger = new Logger (); var emailSender = new EMailSender ( logger , smtpAddress ); var contactRepository = new ContactRepository (); NotificationService notificationService = new NotificationService ( logger , emailSender , contactRepository ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } } // Class for sending notifications class NotificationService { // Dependencies of the class IEMailSender _emailSender ; ILogger _logger ; IContactRepository _contactRepository ; public NotificationService ( ILogger logger , IEMailSender emailSender , IContactRepository contactRepository ) { _logger = logger ; _emailSender = emailSender ; _contactRepository = contactRepository ; } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } #region Contracts (abstractions) // Interface for logging public interface ILogger { void LogInformation ( string text ); void LogError ( string text ); } // Interface for sending e-mail public interface IEMailSender { void SendMail ( string to , string subject , string message ); } // Interface for Contact entity persistence public interface IContactRepository { string GetContactEMailAddress ( int contactId ); } #endregion #region Implementations // Class for logging public class Logger : ILogger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail public class EMailSender : IEMailSender { ILogger _logger ; string _smtpAddress ; public EMailSender ( ILogger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository : IContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } #endregion We improved out previous solution in the following points: The NotificationService class no longer instantiates its dependencies itself, but receives them in constructor parameters. Interfaces (abstractions) have been introduced to manage dependencies The NotificationService class gets its dependencies in the form of interfaces. When a class receives its dependencies externally (e.g. via constructor parameters), it is called DEPENDENCY INJECTION (DI). In our case, the classes get their class dependencies in constructor parameters: this specific form of DI is called CONSTRUCTOR INJECTION . This is the most common - and most recommended - way to inject dependency. (Alternatively, for example, we could use property injection, which is based on a public property setter to set a specific dependency of a class). In our current solution, NotificationService dependencies are instantiated by the (direct) USER of the class (which is the ToDoService class). Primarily this is the reason why we are still facing with a few problems: The user of NotificationService objects, which is the ToDoService class, is still dependent on the implementation types (since it has to instantiate the Logger , EMailSender and ContactRepository classes). If we use the Logger , EMailSender and ContactRepository classes at multiple places in your application, we must instantiate them explicitly. In other words: at each and every place where have to create an ILogger , IEMailSender or IContactRepository implementation class, we have to make a decision which implementation to choose. This is essentially a special case of code duplication, the decision should appear only once in our code. Our goal, in contrast, would be to determine at a single central location what type implementation to use for an abstraction (interface type) everywhere in the application (e.g. for ILogger create an Logger instance everywhere, for IMailSender create an EMailSender everywhere). This would allow us to easily review our abstraction-to-implementation mappings at one place. Moreover, if we want to change one of the mappings (e.g. using AdvancedLogger instead of Logger for ILogger ) we could achieve that by making a single change at a central location. Example phase 3 - dependency injection based on .NET Core Dependency Injection \u00b6 We need some extra help from our framework to solve the two problems we concluded the previous chapter with: an Inversion of Control (IoC) container. Dependency Injection container is a widely used alternative name for the same tool/technique. In an IoC container we can store abstraction type -> implementation type mappings, such as ILogger->Logger, IMailSender->EMailSender, etc. This is called the REGISTER step. And then based on these mappings create an implementation type for a specific abstraction type (e.g. Logger for an ILogger ). This is called the RESOLVE step. In more detail: REGISTER : Register dependency mappings (e.g. ILogger -> Logger , IMailSender -> EMailSender ) into an IoC container, once, at a centralized location, at application startup. This is the REGISTER step of the DI process. Note: This solves \"problem 2\" pointed out at the end of the previous chapter: the mappings are centralized, and not scattered all over the application code base. RESOLVE : When we need an implementation object at runtime in our application, we ask the container for an implementation by specifying the abstraction (interface) type (e.g., by providing ILogger as a key, the container returns an object of class Logger ). The resolve step is typically done at the \" entry point \" of the application (e.g. in case of WebApi on the receival of web requests, we will look into this later). The resolve step is performed only for the ROOT OBJECT (e.g. for the appropriate Controller class in case of WebApi). The container creates and returns a root object and all its dependencies and all its indirect dependencies: an entire object graph is generated. This process is called AUTOWIRING . Note: In case of Web API calls, the Resolve step is executed by the Asp.Net framework and is mostly hidden from the developer: all we see is that our controller class is automatically instantiated and all constructor parameters are automatically populated (with the help of the IoC container based on the mappings of the REGISTER step). Fortunately, .NET Core has a built in IoC container based dependency injection service. Now we elucidate and illustrate the complete mechanism (register and resolve steps) using our enhanced e-mail notification solution as an example. 1) REGISTER step (registering dependencies) \u00b6 In an Asp.Net Core environment, dependencies are registered by the ConfigureServices (IServiceCollection services) function of our Startup class, namely by the AddSingleton , AddTransient and AddScoped operations of IServiceCollection . First, let's focus on the most exciting parts of ConfigureServices : public class Startup { public void ConfigureServices ( IServiceCollection services ) { // ... services . AddSingleton < ILogger , Logger >(); services . AddTransient < INotificationService , NotificationService >(); services . AddScoped < IContactRepository , ContactRepository >(); services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); // ... } Startup.ConfigureServices is called by the framework at application startup. We receive an IServiceCollection services object as parameter: this represents the IoC container created and initialized by the framework. We can register our own dependency mappings into this container. The services . AddSingleton < ILogger , Logger >(); line registers an ILogger -> Logger type mapping, and the Logger is registered as a singleton, as we used the AddSingleton operation for registration. This means that if we later ask the container for an ILogger object (provide ILogger as key at the resolve step), we will get a Logger object from the container, and always the same instance . The services . AddTransient < INotificationService , NotificationService >(); line registers an INotificationService -> NotificationService transient type mapping, as we used the AddTransient operation for registration. This means that if we later ask the container for an INotificationService object (provide INotificationService as key at the resolve step), we will get a separate newly created instance of NotificationService object from the container, for each query/resolve. services . AddScoped < IContactRepository , ContactRepository >(); line registers an IContactRepository -> ContactRepository scoped type mapping, as we used the AddScoped operation for registration. This means that if we later ask the container for an IContactRepository object (provide IContactRepository as key at the resolve step), we will get a NotificationService object, which will be the same instance for the same scope , and a different instance for different scopes. For a Web API based application one web request is handled within one scope. Consequently, we receive the same instance of a class turning to the container multiple times within the same web request, but different ones when the web requests are different. We can see additional registrations in the sample application's Startup.ConfigureServices method, which we will return to later. 2) RESOLVE step (resolving dependencies) \u00b6 The basics \u00b6 Let's sum up where we are now: we have our abstraction to implementation type mappings registered into the ASP.NET Core IoC container at application startup. Our mappings are the following: ILogger -> Logger as singleton INotificationService -> NotificationService as transient IContactRepository -> ContactRepository as scoped IEMailSender -> EMailSender as singleton From now on, whenever we need an instance of an implementation type for an abstraction, we can ask the container for it using the abstraction type as the key. How do we specifically do it in a .NET Core application? .NET Core provides an IServiceProvider reference to us, and we can use different forms of the GetService operation of this interface. E.g.: void SimpleResolve ( IServiceProvider sp ) { // Returns an instance of the Logger class, as we have // registered the Logger implementation type for our ILogger abstraction. var logger1 = sp . GetService ( typeof ( ILogger )); // Same as the previous example. The difference is that we have provided // the type as a generic parameter. This is a more convenient approach. // To use this we have to import the Microsoft.Extensions.DependencyInjection // namespace via the using statement. // Returns an instance of the Logger class, see explanation above. var logger2 = sp . GetService < ILogger >(); // GetService returns null if no type mapping is found for the specific type (ILogger) // GetRequiredService throws an exception instead. var logger3 = sp . GetRequiredService < ILogger >(); // ... } In the above example, code comments explain the behavior in detail. In each case, an abstraction type is to be provided for the GetService / GetRequiredService operation (either via the typeof operator, or via a generic parameter), and the operation returns with an instance of an implementation type based on the type mappings registered in the container. Object graph resolution, autowiring \u00b6 In the previous example, the container was able to instantiate the Logger class at the resolve step without any major 'headaches', since it has no additional dependencies: it has a single default constructor. Now consider the resolution of INotificationService : public void ObjectGraphResolve ( IServiceProvider sp ) { var notifService = sp . GetService < INotificationService >(); // ... } At the resolve step (GetService call), the container must create a NotificationService object. In doing so, it has to provide valid values for its constructor parameters, which actually means that has to resolve the class's direct and indirect dependencies, recursively: The NotificationService class has a three-parameter constructor (that is, it has three dependencies): NotificationService (ILogger logger, IEMailSender emailSender, IContactRepository contactRepository) . The GetService resolves constructor parameters one by one based on IoC container mapping registrations: ILogger logger: a Logger object is provided by the container, always the same instance (as ILogger->Logger mapping is registered as singleton) IEMailSender emailSender: an EMailSender object is provided by the container, a different instance in each case (as mapping is registered as transient) The EMailSender constructor has an ILogger logger parameter, that has to be resolved as well: a Logger object is provided by the container, always the same instance (as registered as singleton) IContactRepository contactRepository: a ContactRepository object is provided by the container, a different instance for different scopes (Web API e.g. for different Web API calls), as mapping is registered as scoped. Summing up: the GetService<INotificationService>() call above creates a fully parameterized NotificationService object with all of its direct and indirect dependencies, the call returns an object graph for us: As we have seen in this example, IoC containers/DI frameworks are capable of determining the dependency requirements of objects (by examining at their constructor parameters), and then creating entire object graphs based on upfront abstraction->implementation container type mappings. This process is called autowiring . Dependency resolution for ASP.NET Web API classes \u00b6 Besides making our solution IoC container based, we make a few further changes to our todo app. We eliminate our ToDoService class, and move its functionality in a slightly different form into an Asp.Net Core based ControllerBase derived class. This controller class will serve as our entry point and also as a root object, bringing our solution very close to a real life example (let it be a Web API, Web MVC app or a Web Razor Pages app). We could also have kept ToDoService in the middle of our call/dependency chain, but we try to keep things as simple as possible for our demonstration purposes. Furthermore, we also introduce an Entity Framework DbContext derived class called TodoContext to be able to demonstrate how it can be injected into repository classes in a typical application. Our new object graph will look like this: In the previous two chapters, we have assumed that a IServiceProvider object is available to call GetService . If we create a container ourselves, then this assumption is valid. However, only in the rarest cases do we create a container directly. In a typical ASP.NET Web API application, the container is created by the framework and is not directly accessible to us. Consequently, access to `IServiceProvider ', with the exception of a few startup and configuration points, is not available. The good news is that actually we don't need access to the container. The core concept of DI is that we perform dependency resolution only at the application entry point for the \"root object\". In case of Web API apps, the entry point is a call to an operation of a Controller class serving the specific API request. When a request is received, the framework determines and creates the Controller / ControllerBase child class based on the Url and rooting rules. If the controller class has dependencies (has constructor parameters), they are also resolved based on the container registration mappings, including indirect dependencies. The complete object graph is created, the root object is the controller class . Let's take a look at this in practice by refining our previous example with the addition of a TodoController class: [Route(\"api/[controller] \")] [ApiController] public class TodoController : ControllerBase { // Dependencies of the TodoController class private readonly TodoContext _context ; // this is a DbContext private readonly INotificationService _notificationService ; // Dependencies are received as constructor parameters public TodoController ( TodoContext context , INotificationService notificationService ) { _context = context ; _notificationService = notificationService ; // Fill wit some initial data if ( _context . TodoItems . Count () == 0 ) { _context . TodoItems . Add ( new TodoItem { Name = \"Item1\" }); _context . TodoItems . Add ( new TodoItem { Name = \"Item2\" , LinkedContactId = 2 }); _context . SaveChanges (); } } // API call handling function for sending an e-mail notification // Example for use: a http post request to this url (e.g. via using PostMan): // http://localhost:58922/api/todo/2/reminder // , which sends an e-mail notif to the e-mail address appointed of the // contact person referenced by the todo item. [HttpPost(\"{id}/reminder\")] public IActionResult ReminderMessageToLinkedContact ( long id ) { // Look up todo item var item = _context . TodoItems . Find ( id ); if ( item == null ) return NotFound (); // Rend reminder e-mail _notificationService . SendEmailReminder ( item . LinkedContactId , item . Name ); // Actually we don't create anything here, simply return an OK return Ok (); } // ... further operations } Requests under the http://<base_address>/api/todo url are routed to the TodoController class based on the routing rules. The mail sending request ( http://<base_address>/api/todo/<todo-id>/reminder ) is routed to its TodoController.ReminderMessageToLinkedContact operation. A TodoController object is instantiated by the framework, creating a new instance for each request. The TodoController class has two dependencies provided as constructor parameters. The first is a TodoContext object, which is a DbContext derived class. The other is an INotificationService , (which we already covered in our previous example). As we saw in the previous section, the DI framework can create these objects based on the container registered mappings (with all their indirect dependencies), and then pass them to the TodoController as constructor parameter, where they are stored in member variables. The entire object graph is created, with TodoController as the root object. This object graph is to serve the specific web API request. Note The resolution of TodoContext is only possible if it's pre-registered in the IoC container. We will discuss this in the next chapter. Entity Framework DbContext container registration and resolution \u00b6 In applications, especially in Asp.Net Core based ones, there are two ways to use DbContext: Each time it is needed, we create and dispose it with the help of a using block. This can result in the creation of multiple DbContext instances serving an incoming request (which is absolutely OK). We create one DbContext for a specific incoming request and share it for the classes involved in serving the request. In this case, we think of the DbContext instance as a unit of work serving the request. To accomplish this latter approach, ASP.NET Core provides a handy built-in DI based solution: when we configure our container with the type mappings at startup, we also register our DbContext class, which is then later automatically injected for our Controller and other (typically repository) dependencies. Let's see how our TodoContext ( DbContext derived) class is registered in our example. The place of the registration is the usual Startup.ConfigureServices : public void ConfigureServices ( IServiceCollection services ) { // ... services . AddDbContext < TodoContext >( opt => opt . UseInMemoryDatabase ( \"TodoList\" )); // ... } AddDbContext is an extension method defined by the framework for the IServiceCollection interface. This allows convenient registration of our DbContext class. We do not see into the implementation of AddDbContext , but actually it simply performs a scoped registration of our context type into the container: services . AddScoped < TodoContext , TodoContext >(); As shown in the example, TodoContext is not registered via an abstraction (no ITodoContext interface exists) , but via the TodoContext implementation type itself. DI frameworks / IoC containers support the key part of a mapping to be a specific type, e.g. the implementation type itself . Use this approach only when justified, e.g. when we don't need extensibility for the specific type, and introducing an abstraction (interface) would only complicate the solution. In an Asp.Net Core environment, we don't introduce an interface for our DbContext derived class: instead, we always register it with the type of its class to the IoC container (in our example TodoContext -> TodoContext mapping). DbContext itself can work with many persistent providers (e.g. MSSQL, Oracle, in-memory, etc.), so in many cases it does not make sense to put it behind further abstractions. In those cases when we need to abstract data access, we do not introduce an interface to access DbContext . Instead, we use the Repository design pattern, and we introduce interfaces for each repository implementations classes, and then register their mappings to the IoC container (e.g. ITodoRepository -> TodoRepository ). The repository classes either instantiate the DbContext objects themselves or the DbContext is injected as constructor parameter). Note This document does not intend to make a standpoint over the often disputed question, whether it makes or does not make sense introducing a repository layer in an Entity Framework based application. For illustration purposes, our TodoApi application uses a mixed solution in this sense: controller/service classes use DbContext directly to persist TodoItem objects, and use the Repository pattern to handle Contacts. Don't mix the two approaches in a real-life application. The example above also shows that you can also provide a lambda expression when registering DbContext (in case TodoContext ) using AddDbContext : opt => opt . UseInMemoryDatabase ( \"TodoList\" ) This lambda expression is called by the container later at the resolve step - that is, every time when a TodoContext is instantiated. An option object is provided as a parameter (in this example, the opt argument): this allows us to configurate the instance created by the container. In our example, calling the UseInMemoryDatabase operation creates an in-memory based database called \"TodoList\". Advanced dependency injection registration example \u00b6 Not compulsory material. Let's cover code parts of Startup.ConfigureServices we skipped previously. The registration of EMailSender looks quite tricky: services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); Let's take a look at the constructor of EMailSender to be able to better understand the situation: ```csharp public EMailSender(ILogger logger, string smtpAddress) { _logger = logger; _smtpAddress = smtpAddress; } `EMailSender` will need to be instantiated by the container when resolving `IEMailSender`, and the constructor parameters must be specified appropriately. The logger parameter is completely \"OK\", and the container can resolve it based on the ILogger-> Logger container mapping registration. However, there is no way to find out the value of the `smtpAddress` parameter. To solve this problem, ASP.NET Core proposes an \"options\" mechanism for the framework, which allows us to retrieve the value from some configuration. Covering the \"options\" topic would be a far-reaching thread for us, so for simplification we applied another approach. The `AddSingleton` (and other Add ... operations) have an overload in which we can specify a lambda expression. This lambda is called by the container later at the resolve step (that is, when we ask the container for an `IEMailSender` implementation) for each instance. With the help of this lambda we manually create the `EMailSender` object, so we have the chance to provide the necessary constructor parameters. In fact, the container is really \"helpful\" with us: it provides an `IServiceCollection` object as the lambda parameter for us (in this example it's called `sp`), and based on container registrations we can conveniently resolve types with the help of the already covered `GetRequiredService` and `GetService` calls. ## Further topics ### Dependency Injection/IoC containers in general The particularities of the DI container built in ASP.NET Core: * It provides basic services required by most applications (e.g., does not support property injection). * If you need more DI related functionality, you can use another IoC container Asp.Net Core can work with. * Several Dependecy Injection / IoC container class libraries exist that can be used with .NET Core, with .NET Framework, or with both. A few examples: AutoFac, DryIoc, LightInject, Castle Windsor, Ninject, StructureMap, SimpleInjector, MEF, ... * It's implemented in the __Microsoft.Extensions.DependencyInjection__ NuGet package. * For Asp.Net Core applications, it is automatically installed when the Asp.Net project is created. In fact, as we have seen, Asp.Net Core middleware heavily relies on it, it's a key pillar of runtime configuration and extensibility. * For other .NET Core applications (e.g. a simple .NET Core based console app), you need to add it manually by installing the Microsoft.Extensions.DependencyInjection NuGet package for the project. * Note: the NuGet package can be used with the (full) .NET Framework as well as it supports .NET Standard. ### The Service Locator antipattern Dependency injection is not the only way of using an IoC container. Another technique called __Service Locator__ exists. Dependency Injection is based on the mechanism of passing the dependencies of a class as constructor parameters. Service Locator uses another approach: the classes directly access the IoC container in their methods to resolve their dependencies. Keep in mind that this approach is considered an __anti-pattern__. The reason is simple: every time time a class needs a dependency, it has to turn to a container, so much of our code will depend on the container itself! In contrast, when dependency injection is used, dependency resolution is performed \"once\" at the application entry point for \"root objects\" (e.g. for the controller class in case of a Web API call), the rest of our code is completely independent of the container. Note that in our previous example, in our TodoController, NotificationService, EMailSender, Logger, and ContactRepository classes, we did not refer the container (neither via an IServiceProvider, nor by any other means). ### Asp.Net Core framework services Asp.Net Core has several built in services. E.g. it has support for Web API, and support for Razor Pages or MVC based web applications. These all rely on the DI services of Asp.Net Core. The `Startup.ConfigureServices` method of an Asp.Net Web API application has to have this piece of code: ```csharp services.AddMvc() .SetCompatibilityVersion(CompatibilityVersion.Version_2_1); AddMvc is a built in extension method for the IServiceProvider interface, which registers numerous (far more than 100!) service and configuration classes into the container required by the internals of the Web API middleware/pipeline. Starting from .NET Core 3.0 the situation is different: instead of calling AddMvc() we typically call AddControllers(), which is a more lightweight option, resulting in significantly less container registrations. Disposing service objects \u00b6 The container calls Dispose for the objects it creates if the object implements the IDisposable interface. Resources \u00b6 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection https://stackify.com/net-core-dependency-injection/amp https://medium.com/volosoft/asp-net-core-dependency-injection-best-practices-tips-tricks-c6e9c67f9d96","title":"Dependency Injection ASP.NET Core environment"},{"location":"lecture-notes/di/#dependency-injection-aspnet-core-environment","text":"Definition Dependency Injection (DI) is programming technique that makes a class independent of its dependencies. It's a key enabler for decomposing an application into loosely coupled components. More precisely: Dependency Injection is a mechanism to decouple the creation of dependency graphs for a class from its class definition . Of course, the above definition is very abstract, and based on the short definition it's hard to understand what problems DI is trying to solve, and how DI is trying to solve them. In the following chapters, we will use an example to put DI into context and to learn the basics of the DI related services build into ASP.NET Core. Goals of DI Facilitated extensibility and maintainability Improved unit testability Facilitated code reuse Sample application The sample C# code is available here: https://github.com/bmeviauac01/todoapi-di-sample","title":"Dependency Injection ASP.NET Core environment"},{"location":"lecture-notes/di/#example-phase-1-service-class-with-wired-in-dependencies","text":"In this example, based on code snippets we look at parts of a to-do list (TODO) application that sends to-do item related email notifications. Note: The code is minimalistic for succinctness. The \"entry point\" of our example is the SendReminderIfNeeded operation of the ToDoService class. // Class for managing todo items public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // It checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { NotificationService notificationService = new NotificationService ( smtpAddress ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } // ... } // Entity class, encapsulates information about a todo task public class TodoItem { // Database key public long Id { get ; set ; } // Name/description of the task public string Name { get ; set ; } // Indicates if the task has been completed public bool IsComplete { get ; set ; } // It's possible to assign a contact person to a task: -1 indicated no contact // person is assigned, otherwise the id of the contact person public int LinkedContactId { get ; set ; } = - 1 ; } In the code above ( ToDoService.SendReminderIfNeeded ) we see that the essential logic of sending an e-mail is to be found in the NotificationService class. Indeed, this class is at the center of our investigation. The following code snippet describes the code for the NotificationService class and its dependencies: // Class for sending notifications class NotificationService { // Dependencies of the class EMailSender _emailSender ; Logger _logger ; ContactRepository _contactRepository ; public NotificationService ( string smtpAddress ) { _logger = new Logger (); _emailSender = new EMailSender ( _logger , smtpAddress ); _contactRepository = new ContactRepository (); } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } // Class supporting loggin public class Logger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail notifications public class EMailSender { Logger _logger ; string _smtpAddress ; public EMailSender ( Logger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } A few general thoughts: The NotificationService class has several dependencies ( EMailSender , Logger , ContactRepository classes) and it implements its services based on these dependency classes. Dependency classes may have additional dependencies: EMailSender is a great example of this, it's dependent on the Logger class. Note: NotificationService , EMailSender , Logger , ContactRepository classes are considered service classes because they contain business logic, not just encapsulate data, such as TodoItem . As we could see the SendEmailReminder operation is actually served by an object graph, where NotificationService is the root object, it has three dependencies, and its dependencies have further dependencies. The following figure illustrates this object graph: Note One may ask why we considered NotificationService , and not ToDoService as the root object. Actually it just depends on our viewpoint: for simplicity we considered ToDoService as an entry point (a \"client\") for fulfilling a request, so that we have less classes to put under scrutiny. In a real life application we probably would consider ToDoService as part of the dependency graph as well. Let's review the key features of this solution: The class instantiates its dependencies itself Class depends on the specific type of its dependencies (and not on interfaces, \"abstractions\") This approach has a couple of significant and rather painful drawbacks: Rigidity, lack of extensibility . NotificationService (without modification) cannot work with other mailing, logging and contact repository implementations (but only with the the wired in EMailSender , Logger and ContactRepository classes). That is, e.g. we can't use it with any other logging component, or e.g. use it with a contact repository that operates via a different data source/storage mechanism. Lack of unit testability . The NotificationService (without modification) cannot be unit tested. This would require replacing the EMailSender , Logger and ContactRepository dependencies with variants that provide fixed/expected responses for a given input. Keep in mind that unit testing is about testing the behavior of a class independently from its dependencies. In our example, instead of using the database base ContactRepository, we would need a ContactRepository implementation that could serve requests very quickly from memory with values supporting the specific test cases. There is one more subtle inconvenience that is hard to notice at first sight. In our example we had to provide the smtpAddress parameter to the NotificationService constructor, so that it can forward it to its EMailSender dependency. However, smtpAddress is a parameter completely meaningless for NotificationService , it has nothing to do with this piece of information. Unfortunately, we are forced to pass smtpAddress thorough NotificationService , as NotificationService is the class instantiating the EMailSender object. We could eliminate this by somehow instantiating EMailSender independently of NotificationService . In the next steps, we redesign our solution so that we can eliminate most of the downsides of the current rigid approach.","title":"Example phase 1 - service class with wired in dependencies"},{"location":"lecture-notes/di/#example-phase-2-service-class-with-manual-dependency-injection","text":"Redesign our former solution the functional requirements are unchanged. The most important principles of transformation are the following: Dependencies will be based on abstractions/interfaces Classes will no longer instantiate their dependencies themselves Let's jump right into the code of the improved solution and analyze the differences: public class ToDoService { const string smtpAddress = \"smtp.myserver.com\" ; // Checks the todoItem object received as a parameter and sends an e-mail // notification about the to-do item to the contact person specified by the // todo item. public void SendReminderIfNeeded ( TodoItem todoItem ) { if ( checkIfTodoReminderIsToBeSent ( todoItem )) { var logger = new Logger (); var emailSender = new EMailSender ( logger , smtpAddress ); var contactRepository = new ContactRepository (); NotificationService notificationService = new NotificationService ( logger , emailSender , contactRepository ); notificationService . SendEmailReminder ( todoItem . LinkedContactId , todoItem . Name ); } } bool checkIfTodoReminderIsToBeSent ( TodoItem todoItem ) { bool send = true ; /* ... */ return send ; } } // Class for sending notifications class NotificationService { // Dependencies of the class IEMailSender _emailSender ; ILogger _logger ; IContactRepository _contactRepository ; public NotificationService ( ILogger logger , IEMailSender emailSender , IContactRepository contactRepository ) { _logger = logger ; _emailSender = emailSender ; _contactRepository = contactRepository ; } // Sends an email notification to the contact with the given ID // (contactId is a key in the Contacts table) public void SendEmailReminder ( int contactId , string todoMessage ) { string emailTo = _contactRepository . GetContactEMailAddress ( contactId ); string emailSubject = \"TODO reminder\" ; string emailMessage = \"Reminder about the following todo item: \" + todoMessage ; _emailSender . SendMail ( emailTo , emailSubject , emailMessage ); } } #region Contracts (abstractions) // Interface for logging public interface ILogger { void LogInformation ( string text ); void LogError ( string text ); } // Interface for sending e-mail public interface IEMailSender { void SendMail ( string to , string subject , string message ); } // Interface for Contact entity persistence public interface IContactRepository { string GetContactEMailAddress ( int contactId ); } #endregion #region Implementations // Class for logging public class Logger : ILogger { public void LogInformation ( string text ) { /* ...*/ } public void LogError ( string text ) { /* ...*/ } } // Class for sending e-mail public class EMailSender : IEMailSender { ILogger _logger ; string _smtpAddress ; public EMailSender ( ILogger logger , string smtpAddress ) { _logger = logger ; _smtpAddress = smtpAddress ; } public void SendMail ( string to , string subject , string message ) { _logger . LogInformation ( $\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\" ); // ... } } // Class for Contact entity persistence public class ContactRepository : IContactRepository { public string GetContactEMailAddress ( int contactId ) { // ... } // ... } #endregion We improved out previous solution in the following points: The NotificationService class no longer instantiates its dependencies itself, but receives them in constructor parameters. Interfaces (abstractions) have been introduced to manage dependencies The NotificationService class gets its dependencies in the form of interfaces. When a class receives its dependencies externally (e.g. via constructor parameters), it is called DEPENDENCY INJECTION (DI). In our case, the classes get their class dependencies in constructor parameters: this specific form of DI is called CONSTRUCTOR INJECTION . This is the most common - and most recommended - way to inject dependency. (Alternatively, for example, we could use property injection, which is based on a public property setter to set a specific dependency of a class). In our current solution, NotificationService dependencies are instantiated by the (direct) USER of the class (which is the ToDoService class). Primarily this is the reason why we are still facing with a few problems: The user of NotificationService objects, which is the ToDoService class, is still dependent on the implementation types (since it has to instantiate the Logger , EMailSender and ContactRepository classes). If we use the Logger , EMailSender and ContactRepository classes at multiple places in your application, we must instantiate them explicitly. In other words: at each and every place where have to create an ILogger , IEMailSender or IContactRepository implementation class, we have to make a decision which implementation to choose. This is essentially a special case of code duplication, the decision should appear only once in our code. Our goal, in contrast, would be to determine at a single central location what type implementation to use for an abstraction (interface type) everywhere in the application (e.g. for ILogger create an Logger instance everywhere, for IMailSender create an EMailSender everywhere). This would allow us to easily review our abstraction-to-implementation mappings at one place. Moreover, if we want to change one of the mappings (e.g. using AdvancedLogger instead of Logger for ILogger ) we could achieve that by making a single change at a central location.","title":"Example phase 2 - service class with manual dependency injection"},{"location":"lecture-notes/di/#example-phase-3-dependency-injection-based-on-net-core-dependency-injection","text":"We need some extra help from our framework to solve the two problems we concluded the previous chapter with: an Inversion of Control (IoC) container. Dependency Injection container is a widely used alternative name for the same tool/technique. In an IoC container we can store abstraction type -> implementation type mappings, such as ILogger->Logger, IMailSender->EMailSender, etc. This is called the REGISTER step. And then based on these mappings create an implementation type for a specific abstraction type (e.g. Logger for an ILogger ). This is called the RESOLVE step. In more detail: REGISTER : Register dependency mappings (e.g. ILogger -> Logger , IMailSender -> EMailSender ) into an IoC container, once, at a centralized location, at application startup. This is the REGISTER step of the DI process. Note: This solves \"problem 2\" pointed out at the end of the previous chapter: the mappings are centralized, and not scattered all over the application code base. RESOLVE : When we need an implementation object at runtime in our application, we ask the container for an implementation by specifying the abstraction (interface) type (e.g., by providing ILogger as a key, the container returns an object of class Logger ). The resolve step is typically done at the \" entry point \" of the application (e.g. in case of WebApi on the receival of web requests, we will look into this later). The resolve step is performed only for the ROOT OBJECT (e.g. for the appropriate Controller class in case of WebApi). The container creates and returns a root object and all its dependencies and all its indirect dependencies: an entire object graph is generated. This process is called AUTOWIRING . Note: In case of Web API calls, the Resolve step is executed by the Asp.Net framework and is mostly hidden from the developer: all we see is that our controller class is automatically instantiated and all constructor parameters are automatically populated (with the help of the IoC container based on the mappings of the REGISTER step). Fortunately, .NET Core has a built in IoC container based dependency injection service. Now we elucidate and illustrate the complete mechanism (register and resolve steps) using our enhanced e-mail notification solution as an example.","title":"Example phase 3 - dependency injection based on .NET Core Dependency Injection"},{"location":"lecture-notes/di/#1-register-step-registering-dependencies","text":"In an Asp.Net Core environment, dependencies are registered by the ConfigureServices (IServiceCollection services) function of our Startup class, namely by the AddSingleton , AddTransient and AddScoped operations of IServiceCollection . First, let's focus on the most exciting parts of ConfigureServices : public class Startup { public void ConfigureServices ( IServiceCollection services ) { // ... services . AddSingleton < ILogger , Logger >(); services . AddTransient < INotificationService , NotificationService >(); services . AddScoped < IContactRepository , ContactRepository >(); services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); // ... } Startup.ConfigureServices is called by the framework at application startup. We receive an IServiceCollection services object as parameter: this represents the IoC container created and initialized by the framework. We can register our own dependency mappings into this container. The services . AddSingleton < ILogger , Logger >(); line registers an ILogger -> Logger type mapping, and the Logger is registered as a singleton, as we used the AddSingleton operation for registration. This means that if we later ask the container for an ILogger object (provide ILogger as key at the resolve step), we will get a Logger object from the container, and always the same instance . The services . AddTransient < INotificationService , NotificationService >(); line registers an INotificationService -> NotificationService transient type mapping, as we used the AddTransient operation for registration. This means that if we later ask the container for an INotificationService object (provide INotificationService as key at the resolve step), we will get a separate newly created instance of NotificationService object from the container, for each query/resolve. services . AddScoped < IContactRepository , ContactRepository >(); line registers an IContactRepository -> ContactRepository scoped type mapping, as we used the AddScoped operation for registration. This means that if we later ask the container for an IContactRepository object (provide IContactRepository as key at the resolve step), we will get a NotificationService object, which will be the same instance for the same scope , and a different instance for different scopes. For a Web API based application one web request is handled within one scope. Consequently, we receive the same instance of a class turning to the container multiple times within the same web request, but different ones when the web requests are different. We can see additional registrations in the sample application's Startup.ConfigureServices method, which we will return to later.","title":"1) REGISTER step (registering dependencies)"},{"location":"lecture-notes/di/#2-resolve-step-resolving-dependencies","text":"","title":"2) RESOLVE step (resolving dependencies)"},{"location":"lecture-notes/di/#the-basics","text":"Let's sum up where we are now: we have our abstraction to implementation type mappings registered into the ASP.NET Core IoC container at application startup. Our mappings are the following: ILogger -> Logger as singleton INotificationService -> NotificationService as transient IContactRepository -> ContactRepository as scoped IEMailSender -> EMailSender as singleton From now on, whenever we need an instance of an implementation type for an abstraction, we can ask the container for it using the abstraction type as the key. How do we specifically do it in a .NET Core application? .NET Core provides an IServiceProvider reference to us, and we can use different forms of the GetService operation of this interface. E.g.: void SimpleResolve ( IServiceProvider sp ) { // Returns an instance of the Logger class, as we have // registered the Logger implementation type for our ILogger abstraction. var logger1 = sp . GetService ( typeof ( ILogger )); // Same as the previous example. The difference is that we have provided // the type as a generic parameter. This is a more convenient approach. // To use this we have to import the Microsoft.Extensions.DependencyInjection // namespace via the using statement. // Returns an instance of the Logger class, see explanation above. var logger2 = sp . GetService < ILogger >(); // GetService returns null if no type mapping is found for the specific type (ILogger) // GetRequiredService throws an exception instead. var logger3 = sp . GetRequiredService < ILogger >(); // ... } In the above example, code comments explain the behavior in detail. In each case, an abstraction type is to be provided for the GetService / GetRequiredService operation (either via the typeof operator, or via a generic parameter), and the operation returns with an instance of an implementation type based on the type mappings registered in the container.","title":"The basics"},{"location":"lecture-notes/di/#object-graph-resolution-autowiring","text":"In the previous example, the container was able to instantiate the Logger class at the resolve step without any major 'headaches', since it has no additional dependencies: it has a single default constructor. Now consider the resolution of INotificationService : public void ObjectGraphResolve ( IServiceProvider sp ) { var notifService = sp . GetService < INotificationService >(); // ... } At the resolve step (GetService call), the container must create a NotificationService object. In doing so, it has to provide valid values for its constructor parameters, which actually means that has to resolve the class's direct and indirect dependencies, recursively: The NotificationService class has a three-parameter constructor (that is, it has three dependencies): NotificationService (ILogger logger, IEMailSender emailSender, IContactRepository contactRepository) . The GetService resolves constructor parameters one by one based on IoC container mapping registrations: ILogger logger: a Logger object is provided by the container, always the same instance (as ILogger->Logger mapping is registered as singleton) IEMailSender emailSender: an EMailSender object is provided by the container, a different instance in each case (as mapping is registered as transient) The EMailSender constructor has an ILogger logger parameter, that has to be resolved as well: a Logger object is provided by the container, always the same instance (as registered as singleton) IContactRepository contactRepository: a ContactRepository object is provided by the container, a different instance for different scopes (Web API e.g. for different Web API calls), as mapping is registered as scoped. Summing up: the GetService<INotificationService>() call above creates a fully parameterized NotificationService object with all of its direct and indirect dependencies, the call returns an object graph for us: As we have seen in this example, IoC containers/DI frameworks are capable of determining the dependency requirements of objects (by examining at their constructor parameters), and then creating entire object graphs based on upfront abstraction->implementation container type mappings. This process is called autowiring .","title":"Object graph resolution, autowiring"},{"location":"lecture-notes/di/#dependency-resolution-for-aspnet-web-api-classes","text":"Besides making our solution IoC container based, we make a few further changes to our todo app. We eliminate our ToDoService class, and move its functionality in a slightly different form into an Asp.Net Core based ControllerBase derived class. This controller class will serve as our entry point and also as a root object, bringing our solution very close to a real life example (let it be a Web API, Web MVC app or a Web Razor Pages app). We could also have kept ToDoService in the middle of our call/dependency chain, but we try to keep things as simple as possible for our demonstration purposes. Furthermore, we also introduce an Entity Framework DbContext derived class called TodoContext to be able to demonstrate how it can be injected into repository classes in a typical application. Our new object graph will look like this: In the previous two chapters, we have assumed that a IServiceProvider object is available to call GetService . If we create a container ourselves, then this assumption is valid. However, only in the rarest cases do we create a container directly. In a typical ASP.NET Web API application, the container is created by the framework and is not directly accessible to us. Consequently, access to `IServiceProvider ', with the exception of a few startup and configuration points, is not available. The good news is that actually we don't need access to the container. The core concept of DI is that we perform dependency resolution only at the application entry point for the \"root object\". In case of Web API apps, the entry point is a call to an operation of a Controller class serving the specific API request. When a request is received, the framework determines and creates the Controller / ControllerBase child class based on the Url and rooting rules. If the controller class has dependencies (has constructor parameters), they are also resolved based on the container registration mappings, including indirect dependencies. The complete object graph is created, the root object is the controller class . Let's take a look at this in practice by refining our previous example with the addition of a TodoController class: [Route(\"api/[controller] \")] [ApiController] public class TodoController : ControllerBase { // Dependencies of the TodoController class private readonly TodoContext _context ; // this is a DbContext private readonly INotificationService _notificationService ; // Dependencies are received as constructor parameters public TodoController ( TodoContext context , INotificationService notificationService ) { _context = context ; _notificationService = notificationService ; // Fill wit some initial data if ( _context . TodoItems . Count () == 0 ) { _context . TodoItems . Add ( new TodoItem { Name = \"Item1\" }); _context . TodoItems . Add ( new TodoItem { Name = \"Item2\" , LinkedContactId = 2 }); _context . SaveChanges (); } } // API call handling function for sending an e-mail notification // Example for use: a http post request to this url (e.g. via using PostMan): // http://localhost:58922/api/todo/2/reminder // , which sends an e-mail notif to the e-mail address appointed of the // contact person referenced by the todo item. [HttpPost(\"{id}/reminder\")] public IActionResult ReminderMessageToLinkedContact ( long id ) { // Look up todo item var item = _context . TodoItems . Find ( id ); if ( item == null ) return NotFound (); // Rend reminder e-mail _notificationService . SendEmailReminder ( item . LinkedContactId , item . Name ); // Actually we don't create anything here, simply return an OK return Ok (); } // ... further operations } Requests under the http://<base_address>/api/todo url are routed to the TodoController class based on the routing rules. The mail sending request ( http://<base_address>/api/todo/<todo-id>/reminder ) is routed to its TodoController.ReminderMessageToLinkedContact operation. A TodoController object is instantiated by the framework, creating a new instance for each request. The TodoController class has two dependencies provided as constructor parameters. The first is a TodoContext object, which is a DbContext derived class. The other is an INotificationService , (which we already covered in our previous example). As we saw in the previous section, the DI framework can create these objects based on the container registered mappings (with all their indirect dependencies), and then pass them to the TodoController as constructor parameter, where they are stored in member variables. The entire object graph is created, with TodoController as the root object. This object graph is to serve the specific web API request. Note The resolution of TodoContext is only possible if it's pre-registered in the IoC container. We will discuss this in the next chapter.","title":"Dependency resolution  for ASP.NET Web API classes"},{"location":"lecture-notes/di/#entity-framework-dbcontext-container-registration-and-resolution","text":"In applications, especially in Asp.Net Core based ones, there are two ways to use DbContext: Each time it is needed, we create and dispose it with the help of a using block. This can result in the creation of multiple DbContext instances serving an incoming request (which is absolutely OK). We create one DbContext for a specific incoming request and share it for the classes involved in serving the request. In this case, we think of the DbContext instance as a unit of work serving the request. To accomplish this latter approach, ASP.NET Core provides a handy built-in DI based solution: when we configure our container with the type mappings at startup, we also register our DbContext class, which is then later automatically injected for our Controller and other (typically repository) dependencies. Let's see how our TodoContext ( DbContext derived) class is registered in our example. The place of the registration is the usual Startup.ConfigureServices : public void ConfigureServices ( IServiceCollection services ) { // ... services . AddDbContext < TodoContext >( opt => opt . UseInMemoryDatabase ( \"TodoList\" )); // ... } AddDbContext is an extension method defined by the framework for the IServiceCollection interface. This allows convenient registration of our DbContext class. We do not see into the implementation of AddDbContext , but actually it simply performs a scoped registration of our context type into the container: services . AddScoped < TodoContext , TodoContext >(); As shown in the example, TodoContext is not registered via an abstraction (no ITodoContext interface exists) , but via the TodoContext implementation type itself. DI frameworks / IoC containers support the key part of a mapping to be a specific type, e.g. the implementation type itself . Use this approach only when justified, e.g. when we don't need extensibility for the specific type, and introducing an abstraction (interface) would only complicate the solution. In an Asp.Net Core environment, we don't introduce an interface for our DbContext derived class: instead, we always register it with the type of its class to the IoC container (in our example TodoContext -> TodoContext mapping). DbContext itself can work with many persistent providers (e.g. MSSQL, Oracle, in-memory, etc.), so in many cases it does not make sense to put it behind further abstractions. In those cases when we need to abstract data access, we do not introduce an interface to access DbContext . Instead, we use the Repository design pattern, and we introduce interfaces for each repository implementations classes, and then register their mappings to the IoC container (e.g. ITodoRepository -> TodoRepository ). The repository classes either instantiate the DbContext objects themselves or the DbContext is injected as constructor parameter). Note This document does not intend to make a standpoint over the often disputed question, whether it makes or does not make sense introducing a repository layer in an Entity Framework based application. For illustration purposes, our TodoApi application uses a mixed solution in this sense: controller/service classes use DbContext directly to persist TodoItem objects, and use the Repository pattern to handle Contacts. Don't mix the two approaches in a real-life application. The example above also shows that you can also provide a lambda expression when registering DbContext (in case TodoContext ) using AddDbContext : opt => opt . UseInMemoryDatabase ( \"TodoList\" ) This lambda expression is called by the container later at the resolve step - that is, every time when a TodoContext is instantiated. An option object is provided as a parameter (in this example, the opt argument): this allows us to configurate the instance created by the container. In our example, calling the UseInMemoryDatabase operation creates an in-memory based database called \"TodoList\".","title":"Entity Framework DbContext container registration and resolution"},{"location":"lecture-notes/di/#advanced-dependency-injection-registration-example","text":"Not compulsory material. Let's cover code parts of Startup.ConfigureServices we skipped previously. The registration of EMailSender looks quite tricky: services . AddSingleton < IEMailSender , EMailSender >( sp => new EMailSender ( sp . GetRequiredService < ILogger >(), \"smtp.myserver.com\" ) ); Let's take a look at the constructor of EMailSender to be able to better understand the situation: ```csharp public EMailSender(ILogger logger, string smtpAddress) { _logger = logger; _smtpAddress = smtpAddress; } `EMailSender` will need to be instantiated by the container when resolving `IEMailSender`, and the constructor parameters must be specified appropriately. The logger parameter is completely \"OK\", and the container can resolve it based on the ILogger-> Logger container mapping registration. However, there is no way to find out the value of the `smtpAddress` parameter. To solve this problem, ASP.NET Core proposes an \"options\" mechanism for the framework, which allows us to retrieve the value from some configuration. Covering the \"options\" topic would be a far-reaching thread for us, so for simplification we applied another approach. The `AddSingleton` (and other Add ... operations) have an overload in which we can specify a lambda expression. This lambda is called by the container later at the resolve step (that is, when we ask the container for an `IEMailSender` implementation) for each instance. With the help of this lambda we manually create the `EMailSender` object, so we have the chance to provide the necessary constructor parameters. In fact, the container is really \"helpful\" with us: it provides an `IServiceCollection` object as the lambda parameter for us (in this example it's called `sp`), and based on container registrations we can conveniently resolve types with the help of the already covered `GetRequiredService` and `GetService` calls. ## Further topics ### Dependency Injection/IoC containers in general The particularities of the DI container built in ASP.NET Core: * It provides basic services required by most applications (e.g., does not support property injection). * If you need more DI related functionality, you can use another IoC container Asp.Net Core can work with. * Several Dependecy Injection / IoC container class libraries exist that can be used with .NET Core, with .NET Framework, or with both. A few examples: AutoFac, DryIoc, LightInject, Castle Windsor, Ninject, StructureMap, SimpleInjector, MEF, ... * It's implemented in the __Microsoft.Extensions.DependencyInjection__ NuGet package. * For Asp.Net Core applications, it is automatically installed when the Asp.Net project is created. In fact, as we have seen, Asp.Net Core middleware heavily relies on it, it's a key pillar of runtime configuration and extensibility. * For other .NET Core applications (e.g. a simple .NET Core based console app), you need to add it manually by installing the Microsoft.Extensions.DependencyInjection NuGet package for the project. * Note: the NuGet package can be used with the (full) .NET Framework as well as it supports .NET Standard. ### The Service Locator antipattern Dependency injection is not the only way of using an IoC container. Another technique called __Service Locator__ exists. Dependency Injection is based on the mechanism of passing the dependencies of a class as constructor parameters. Service Locator uses another approach: the classes directly access the IoC container in their methods to resolve their dependencies. Keep in mind that this approach is considered an __anti-pattern__. The reason is simple: every time time a class needs a dependency, it has to turn to a container, so much of our code will depend on the container itself! In contrast, when dependency injection is used, dependency resolution is performed \"once\" at the application entry point for \"root objects\" (e.g. for the controller class in case of a Web API call), the rest of our code is completely independent of the container. Note that in our previous example, in our TodoController, NotificationService, EMailSender, Logger, and ContactRepository classes, we did not refer the container (neither via an IServiceProvider, nor by any other means). ### Asp.Net Core framework services Asp.Net Core has several built in services. E.g. it has support for Web API, and support for Razor Pages or MVC based web applications. These all rely on the DI services of Asp.Net Core. The `Startup.ConfigureServices` method of an Asp.Net Web API application has to have this piece of code: ```csharp services.AddMvc() .SetCompatibilityVersion(CompatibilityVersion.Version_2_1); AddMvc is a built in extension method for the IServiceProvider interface, which registers numerous (far more than 100!) service and configuration classes into the container required by the internals of the Web API middleware/pipeline. Starting from .NET Core 3.0 the situation is different: instead of calling AddMvc() we typically call AddControllers(), which is a more lightweight option, resulting in significantly less container registrations.","title":"Advanced dependency injection registration example"},{"location":"lecture-notes/di/#disposing-service-objects","text":"The container calls Dispose for the objects it creates if the object implements the IDisposable interface.","title":"Disposing service objects"},{"location":"lecture-notes/di/#resources","text":"https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection https://stackify.com/net-core-dependency-injection/amp https://medium.com/volosoft/asp-net-core-dependency-injection-best-practices-tips-tricks-c6e9c67f9d96","title":"Resources"},{"location":"lecture-notes/linq/","text":"LINQ: Language Integrated Query \u00b6 Consider the following classes and lists of such instances. class Product { public int ID ; public string Name ; public int Price ; public int VATID ; } class VAT { public int ID ; public int Percentage ; } List < Product > products = ... List < VAT > vat = ... LINQ expressions and IQueryable \u00b6 Let us consider a simple expression: products.Where(p => p.Price < 1000) . This expression is not yet a result set as it has not yet been evaluated . The result of a LINQ queries are represented as an IQueryable<T> generic interface, which does not hold the result, only the descriptor of the query. This is called deferred execution , as the execution will only happen when the result is effectively used: when the result set is iterated (e.g. foreach ), when a specific item is accessed (see later, e.g. .First() ), when we as for a list instead ( .ToList() ). This operation is useful, because this allows us to chain LINQ operations after each other, such as: var l = products . Where ( p => p . Price < 1000 ) . Where ( p => p . Name . Contains ( 's' )) . OrderBy ( p => p . Name ) . Select ( p => p . Name ) ... // variable l does not contain the result foreach ( var x in l ) // this is when the execution will happen { ... } Force evaluation If we want to force the execution at any given moment, we usually use .ToList() . But this has to be considered first and only used when necessary. LINQ operations \u00b6 The examples below, when available, show both syntaxes. Filtering \u00b6 products . Where ( p => p . Price < 1000 ) from p in products where p . Price < 1000 Projection \u00b6 products . Select ( p => p . Name ) from p in products select p . Name Join \u00b6 from p in products join v in vat on p . VATID equals v . Id select p . Price * v . Percentage products . Join ( vat , p => p . VATID , v => v . Id , ( p , v ) => p . Price * v . Percentage ) Sorting \u00b6 products . OrderBy [ Descending ]( p => p . Name ) . ThenBy [ Descending ]( p => p . Price ) from p in products orderby p . Name , p . Price [ descending ] Set operations \u00b6 products . Select ( p => p . Name ). Distinct () products . Where ( p => p . Price < 1000 ) . Union ( products . Where ( p => p . Price > 100000 ) ) // similarly Except, Intersect Aggregation \u00b6 products . Count () products . Select ( p => p . Price ). Average () // similarly Sum, Min, Max First, last \u00b6 products . First () products . Last () products . Where ( p => p . Id == 12 ). FirstOrDefault () products . Where ( p => p . Id == 12 ). SingleOrDefault () Paging \u00b6 products . Take ( 10 ) products . Skip ( 10 ). Take ( 10 ) Contains (exists) \u00b6 products . Any ( p => p . Price == 1234 ) products . Where ( p => p . Price == 1234 ). Any () Grouping \u00b6 from p in products group p by p . VATID products . GroupBy ( p => p . VATID ) Advanced projections \u00b6 During projection we can transform the results into various formats. Whole entity \u00b6 from p in products ... select p The result set is of type IQueryable<Product> , so we get Product instances. Specified field \u00b6 from p in products ... select p . Name The result set is of type IQueryable<string> , so we only get the names. Named types \u00b6 from p in products ... select new MyType ( p . Name , p . Price ) The result set is of type IQueryable<MyType> , when MyType is a class we have to define and has a matching constructor. Anonym types \u00b6 from p in products where p . Price > 1000 select new { ID = p . ID , Name = p . Name }; Anonym types can be instantiated using the syntax new { } . The compiler will effectively create a class definition with the properties we specified. This is generally used when we only need two or three properties, and we have no need for the entire entity. A similar use case for anonym types is when we calculate a property inside the query, such as the name of the product and the full price: from p in products join v in vat on p . VATID equals v . Id select new { Name = p . Name , FullPrice = p . Price * v . Percentage } More information and further examples \u00b6 Lambda expressions: https://www.tutorialsteacher.com/linq/linq-lambda-expression Linq: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/","title":"LINQ: Language Integrated Query"},{"location":"lecture-notes/linq/#linq-language-integrated-query","text":"Consider the following classes and lists of such instances. class Product { public int ID ; public string Name ; public int Price ; public int VATID ; } class VAT { public int ID ; public int Percentage ; } List < Product > products = ... List < VAT > vat = ...","title":"LINQ: Language Integrated Query"},{"location":"lecture-notes/linq/#linq-expressions-and-iqueryable","text":"Let us consider a simple expression: products.Where(p => p.Price < 1000) . This expression is not yet a result set as it has not yet been evaluated . The result of a LINQ queries are represented as an IQueryable<T> generic interface, which does not hold the result, only the descriptor of the query. This is called deferred execution , as the execution will only happen when the result is effectively used: when the result set is iterated (e.g. foreach ), when a specific item is accessed (see later, e.g. .First() ), when we as for a list instead ( .ToList() ). This operation is useful, because this allows us to chain LINQ operations after each other, such as: var l = products . Where ( p => p . Price < 1000 ) . Where ( p => p . Name . Contains ( 's' )) . OrderBy ( p => p . Name ) . Select ( p => p . Name ) ... // variable l does not contain the result foreach ( var x in l ) // this is when the execution will happen { ... } Force evaluation If we want to force the execution at any given moment, we usually use .ToList() . But this has to be considered first and only used when necessary.","title":"LINQ expressions and IQueryable"},{"location":"lecture-notes/linq/#linq-operations","text":"The examples below, when available, show both syntaxes.","title":"LINQ operations"},{"location":"lecture-notes/linq/#filtering","text":"products . Where ( p => p . Price < 1000 ) from p in products where p . Price < 1000","title":"Filtering"},{"location":"lecture-notes/linq/#projection","text":"products . Select ( p => p . Name ) from p in products select p . Name","title":"Projection"},{"location":"lecture-notes/linq/#join","text":"from p in products join v in vat on p . VATID equals v . Id select p . Price * v . Percentage products . Join ( vat , p => p . VATID , v => v . Id , ( p , v ) => p . Price * v . Percentage )","title":"Join"},{"location":"lecture-notes/linq/#sorting","text":"products . OrderBy [ Descending ]( p => p . Name ) . ThenBy [ Descending ]( p => p . Price ) from p in products orderby p . Name , p . Price [ descending ]","title":"Sorting"},{"location":"lecture-notes/linq/#set-operations","text":"products . Select ( p => p . Name ). Distinct () products . Where ( p => p . Price < 1000 ) . Union ( products . Where ( p => p . Price > 100000 ) ) // similarly Except, Intersect","title":"Set operations"},{"location":"lecture-notes/linq/#aggregation","text":"products . Count () products . Select ( p => p . Price ). Average () // similarly Sum, Min, Max","title":"Aggregation"},{"location":"lecture-notes/linq/#first-last","text":"products . First () products . Last () products . Where ( p => p . Id == 12 ). FirstOrDefault () products . Where ( p => p . Id == 12 ). SingleOrDefault ()","title":"First, last"},{"location":"lecture-notes/linq/#paging","text":"products . Take ( 10 ) products . Skip ( 10 ). Take ( 10 )","title":"Paging"},{"location":"lecture-notes/linq/#contains-exists","text":"products . Any ( p => p . Price == 1234 ) products . Where ( p => p . Price == 1234 ). Any ()","title":"Contains (exists)"},{"location":"lecture-notes/linq/#grouping","text":"from p in products group p by p . VATID products . GroupBy ( p => p . VATID )","title":"Grouping"},{"location":"lecture-notes/linq/#advanced-projections","text":"During projection we can transform the results into various formats.","title":"Advanced projections"},{"location":"lecture-notes/linq/#whole-entity","text":"from p in products ... select p The result set is of type IQueryable<Product> , so we get Product instances.","title":"Whole entity"},{"location":"lecture-notes/linq/#specified-field","text":"from p in products ... select p . Name The result set is of type IQueryable<string> , so we only get the names.","title":"Specified field"},{"location":"lecture-notes/linq/#named-types","text":"from p in products ... select new MyType ( p . Name , p . Price ) The result set is of type IQueryable<MyType> , when MyType is a class we have to define and has a matching constructor.","title":"Named types"},{"location":"lecture-notes/linq/#anonym-types","text":"from p in products where p . Price > 1000 select new { ID = p . ID , Name = p . Name }; Anonym types can be instantiated using the syntax new { } . The compiler will effectively create a class definition with the properties we specified. This is generally used when we only need two or three properties, and we have no need for the entire entity. A similar use case for anonym types is when we calculate a property inside the query, such as the name of the product and the full price: from p in products join v in vat on p . VATID equals v . Id select new { Name = p . Name , FullPrice = p . Price * v . Percentage }","title":"Anonym types"},{"location":"lecture-notes/linq/#more-information-and-further-examples","text":"Lambda expressions: https://www.tutorialsteacher.com/linq/linq-lambda-expression Linq: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/","title":"More information and further examples"},{"location":"lecture-notes/mongodb/","text":"MongoDB basics, operations, and the MongoDB .NET Driver \u00b6 NoSQL databases \u00b6 NoSQL databases are data management systems that do not work with the relational data model. The NoSQL name can be a little misleading, as the concept has little to do with the SQL language - the main difference is rather the representation of the data. Why do we need such new databases when we already have the relational model and relational databases? A small database with a simple schema can be easily described in the relational model. But our applications evolve: new functionalities are added, making the schema more complex. More and more data is added to the database making the maintenance inefficient. Relational databases need constant schema changes and updates, which can be cumbersome. Constant data migrations can be a pain. Furthermore, relational databases can be bottlenecks from the scalability perspective - but we will not discuss this aspect in more detail. NoSQL databases offer a solution to these problems. Instead of the rigid schema , NoSQL databases have a flexible schema . In other words, we will require less consistency regarding the data. Basic concepts of MongoDB \u00b6 MongoDB is a client-server database system that has a non-relational schema. The mongod (Mongo daemon) process on the right is the database server. The other side is our application, where a client connects to the database using a network connection. This connection uses the so-called wire protocol , which is a MongoDB proprietary communication protocol. The protocol transmits data and queries in JSON format represented in a binary fashion as BSON. Logical structure \u00b6 The top layer of a MongoDB database system is the so-called cluster . The servers are organized into these clusters. We will not discuss clusters here; these are tools for enabling scalability. The databases are the mongod processes, which host the databases. A MongoDB server/cluster stores multiple databases. And the databases contain collections . If we want to map these concepts to a relational model, then the collections correspond to the tables, and the rows/records in a table correspond to the documents of the collection. Let us investigate these further. Document \u00b6 The document is the unit of storage in MongoDB. A document is a JSON (-like) file: it contains key-value pairs. MongoDB itself stores it as a BSON in binary format. { name : \"sue\" , age : 26 , status : \"A\" , groups : [ \"news\" , \"sports\" ] } The keys can have arbitrary names with a few limitations, such that they have to be unique and cannot begin with the $ character. The names are case sensitive. Values can be string, number, date, binary, embedded document, null , or even as the groups in the example shows, an array - a relational database cannot represent an array so simple. Mapping to the object-oriented world, a document is an object. MongoDB documents have a maximum size of 16MB and this is not a configurable parameter. Collection \u00b6 Collections are analogous to relational database tables, but without a schema. Collections need no definition; the system creates them upon first use. Collections are a place for \"similar\" documents. Although there is no schema, indexes can still be defined on the collections to support fast searching. Since there is no schema, there are no domain integrity requirements enforced either. Database \u00b6 The database has the same purpose as in the relational model. It gathers all data of our application. Access management is also configured on the database level. The name of databases is case sensitive and lowercase by convention. Key \u00b6 The _id field is the unambiguous identifier of each document. Other keys cannot be defined. This field does not need to be specified during insert; the client driver or the server can generate a new value (a 12 byte ObjectId by default). Uniqueness can be guaranteed with the use of indices. We can define an index and mark it as unique to create a key-like field. These unique indices can contain multiple fields too. There are no references to keys in MongoDB. A document can reference another document by copying it's key, but the system has no consistency guarantees for these (e.g., the referenced document can be deleted). MongoDB operations and the MongoDB .NET Driver \u00b6 The following code snippets use the official Nuget package MongoDB.Driver . Establishing a connection \u00b6 To access the MongoDB database, you first need a connection. A MongoClient class represents the connection. We need the server address to establish a connection (see https://docs.mongodb.com/manual/reference/connection-string/ for details on the connection string). var client = new MongoClient ( \"mongodb://localhost:27017\" ); The connection should be treated as a singleton and not disposed of. Connection lifetime The connection is typically stored in a global static variable, or an IoC (Inversion of Control) / DI (Dependency Injection) store. Although the database name may be in the connection string (e.g. mongodb://localhost:27017/datadriven ), it is used only for authentication. Thus, after establishing the connection, we need to specify what database we will use. var db = client . GetDatabase ( \"datadriven\" ); The database does not need to exist in advance. The above call will automatically create if the database does not already exist. Managing collections \u00b6 Unlike a relational database, in MongoDB, our operations are always performed on a single collection , so the selection of a collection is not part of the issued command (like where in SQL), but a prerequisite for the operation. You can get a specific collection by calling GetCollection ; its generic parameter is the C# class implementing the document type. var collection = db . GetCollection < BsonDocument >( \"products\" ); The basic concept of the .NET MongoDB driver is to map every document to a .NET object. This is also called ODM (Object Document Mapping) . ODM is the equivalent of ORM in the NoSQL database world. \"Raw\" json In other languages and platforms, MongoDB drivers do not always map to objects. Sample codes found on the Internet often show communication via \"raw\" JSON documents. Let's try to avoid this, as we learned in ORM, that object-oriented mapping is more convenient and secure. In the previous example, a document of the type \"BsonDocument\" is used. BsonDocument is a generic document representation in which we can store key-value pairs. It is uncomfortable and unsafe to use; thus we usually do try to avoid it. See the suggested solution soon. You can run queries on the variable representing the collection, such as inserting a document and then listing the contents of the collection. The collection will be created automatically the first time you use it, so you don't have to define it. collection . InsertOne ( new BsonDocument () { { \"name\" , \"Apple\" }, { \"categoryName\" , \"Apple\" }, { \"price\" , 123 } }); // listing all documents: a search criteria is needed // this is an empty criteria matching all documents var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var l in list ) Console . WriteLine ( l ); Naming convention Field names in the document start with lowercase letters like price or categoryName (this is the so-called camel case spelling). This is a convention of the MongoDB world for historical reasons. Unless there is a good reason, do not deviate from it. Mapping documents to C# objects \u00b6 As with relational databases, we can work with objects and classes in MongoDB. The .NET driver for MongoDB offers this conveniently. The first step is to define the C# class(es) to map the contents of the database. Since there is no schema for the database and table, we cannot generate C# code based on the schema (as we did with the Entity Framework). So in this world, we tend to follow the Code First approach, which is to write C# code and have the system translate it to database collections. Let us define the following classes to represent Products . public class Product { public ObjectId Id { get ; set ; } // this will be the identifier with name _id public string Name { get ; set ; } public float Price { get ; set ; } public int Stock { get ; set ; } public string [] Categories { get ; set ; } // array field public VAT VAT { get ; set ; } // embedded document } public class VAT // this class is only ever embedded, hence needs to id { public string VATCategoryName { get ; set ; } public float Percentage { get ; set ; } } Note that the name of the field was price before, but in C# it starts with a capital letter, according to Pascal Case : Price . The MongoDB .NET driver integrates with the C# language and the .NET environment and respects its conventions so that the names in the class definition and the field names in the MongoDB documents will be mapped automatically: the Price class property will be price in the document. Customizing the mapping \u00b6 The C# class - MongoDB document mapping is automatic, but it can also be customized. There are several ways to deviate from the conventions. The easiest way is to use custom attributes in the class definition: public class Product { // maps to field _id [BsonId] public ObjectId Identifier { get ; set ; } // can specify the name explicitly [BsonElement(\"price\")] public string TotalPrice { get ; set ; } // properties can be ignores [BsonIgnore] public string DoNotSace { get ; set ; } } Our other option is to register so-called convention packs at a higher level. The convention pack describes the rules of mapping. (A set of conventions also defines the default behavior.) For example, you can specify the following to map the field names to camel case and exclude data members with a default value (defined in the C# language) from the document. // defince convention pack var pack = new ConventionPack (); pack . Add ( new CamelCaseElementNameConvention ()); pack . Add ( new IgnoreIfDefaultConvention ( true )); // register the convention pack // the first parameter is a name to reference this pack // the last argument is a fitlering criteria when to use this convention ConventionRegistry . Register ( \"datadriven\" , pack , t => true ); We also have more sophisticated customizations, such as defining conversion logic for translation between a C# representation and a MongoDB representation, and specifying how to save inheritance hierarchies. For more details, see the official documentation: https://mongodb.github.io/mongo-csharp-driver/2.8/reference/bson/serialization/ . Queries \u00b6 We will use the collection from now on by mapping it to the Product class. This is the recommended solution; the BsonDocument based solution is used only when necessary. The simplest query we have already seen is to list all the documents: var collection = db . GetCollection < Product >( \"products\" ); var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var p in list ) Console . WriteLine ( $\"Id: {p.Id}, Name: {p.Name}\" ); Listing is done using the Find method. The name illustrates MongoDB's philosophy: listing an entire collection is not practical, so there is no simple syntax for it. Find requires a search criteria, which is an empty condition here to matches everything. There are several ways to describe search criteria. With BsonDocument based filtering, the filtering condition must be written according to the MongoDB syntax. We generally will avoid this because the MongoDB .NET driver provides a more convenient solution for us. In most cases, we can use Lambda expressions to describe the filtering. collection . Find ( x => x . Price < 123 ); In this case, the Lambda expression is a delegate of type Predicate <T> , that is, expects a Product and returns bool . Thus in the example above, the x variable represents a Products instance. Of course, this search also works for more complex cases. collection . Find ( x => x . Price < 123 && x . Name . Contains ( \"red\" )); The filtering described by the Lambda expressions hides what search syntax we actually have in MongoDB. For example, the above Contains search condition will actually mean a search with a regular expression. In MongoDB's own language, the previous filter looks like this: { \"price\" : { \"$lt\" : 123.0 }, \"name\" : \"/red/s\" } Note that this description is itself a document. If we wanted to write the filter condition ourselves, we would have to create this descriptor in a BsonDocument . The document keys describe the fields used for filtering, and the values are the filter criteria. In some cases, the condition is a scalar value such as a regular expression (or if we filter for equality); in other cases, the condition is an embedded document, as with the < condition. Here, the $lt key is a special key that denotes the less than operator and the value to the right of the operator is 123.0. The regular expression should be specified according to JavaScript RegExp Syntax . The conditions listed in this way are automatically evaluated in and and fashion. Instead of the Lambda expression, we can create a similar description without having to compile a filter condition in \"text\" form. The .NET driver for MongoDB gives us the ability to use a so-called builders . collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Lt ( x => x . Price , 123 ), Builders < Product >. Filter . Regex ( x => x . Name , \"/red/s\" ), ) ); The above syntax is a bit more eloquent than the Lambda expression, but it is closer to the MongoDB philosophy, and better describes what we want. We can view this syntax as SQL, a declarative, goal-oriented, but platform-specific description. However, it is also type-safe. The Builders<T> generic class is an auxiliary class that we can use to build filtering and other MongoDB specific definitions. Builders<Product>.Filter can be used to define filtering conditions that match the Product C# class. First, we create an and connection, within which we have two filtering conditions. The operators are the less than and regular expressions seen before. We pass two parameters to these functions: the field to be filtered and the operand. Note that no string-based field names were used here or in the Lambda expressions. We can refer to the class fields with the C# Expression syntax. This is practical because we avoid typing field names. Note that all ways of describing the search criteria are identical. The MongoDB driver maps each syntax to its internal representation. Lambda expression-based requires fewer characters and fits better into C#, while the builder approach is used to express MongoDB features better. You can use either one. Using query results \u00b6 The result of the collection.Find(...) function is not yet the result set, but only a descriptor to execute the query. There are generally three ways to retrieve and process the result. Listing \u00b6 Get the complete result set as a list: collection.Find(...).ToList() . Get first/single item \u00b6 If you only need the first item, or know that there will be only one item, you can use collection.Find(...).First() , .FirstOrDefault() , or .Single() , .SingleOrDefault() functions. Cursor \u00b6 If the result set contains multiple documents, it is advisable to iterate it using a cursor. MongoDB limits the size of the response to a query, so if we query too many records, we may get an error instead of a result. To overcome this, we use the cursors where we always get only a subset of the documents. var cur = collection . Find (...). ToCursor (); while ( cur . MoveNext ()) // cursor stepping { foreach ( var t in cur . Current ) // the value of the cursor is not a single document, but a list in itself { ... } } Operators for filtering \u00b6 The filter criteria apply to the fields in the document, and the filter criteria are always constant. Thus it is not possible, for example, to compare two fields , and we cannot refer to other collections. There is a so-called MongoDB aggregation pipeline, which allows you to formulate more complex queries, but for now, let us focus on simple queries. The filter condition compares a field in the document to a constant we specify. The following options are most commonly used. Comparison operators \u00b6 collection . Find ( x => x . Price == 123 ); collection . Find ( Builders < Product >. Filter . Eq ( x => x . Price , 123 )); //Eq, as in equals collection . Find ( x => x . Price != 123 ); collection . Find ( Builders < Product >. Filter . Ne ( x => x . Price , 123 )); // Ne, as in not equals collection . Find ( x => x . Price >= 123 ); collection . Find ( Builders < Product >. Filter . Gte ( x => x . Price , 123 )); // Gte, as in greater than or equal to collection . Find ( x => x . Price < 123 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . Price , 123 )); // Lt, as in less than Boolean operators \u00b6 collection . Find ( x => x . Price > 500 && x . Price < 1000 ); collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Gt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Price , 1000 ) ) ); collection . Find ( x => x . Price < 500 || x . Stock < 10 ); collection . Find ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ); collection . Find ( x => !( x . Price < 500 || x . Stock < 10 )); collection . Find ( Builders < Product >. Filter . Not ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ) ); Value is one of multiple alternatives \u00b6 collection . Find ( x => x . Id == ... || x . Id = ...); collection . Find ( Builders < Product >. Filter . In ( x => x . Id , new [] { ... })); // similarly Nin, as in not in oper\u00e1tor Value exists (not null) \u00b6 collection . Find ( x => x . VAT != null ); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT )); Exists filtering Does exist, that is, non-null filtering is special because there are two ways to have a null value in MongoDB: if the key exists in the document and it has a value of null; or if the key does not exist at all. Filtering fields of embedded document \u00b6 Embedded documents can be used for filtering in the same way. The following are all valid, and it does not matter if the embedded document ( VAT ) does not exist: collection . Find ( x => x . VAT . Percentage < 27 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . VAT . Percentage , 27 )); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT . Percentage , exists : false )); // does not exists, that is, in C#, equals null Filtering based on an array field \u00b6 Any field in the document can be an array value, as in the example string [] Categories . In MongoDB, we can define filtering based on an array field using the Any* criterion. // products of this category collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); // products that are assigned to at least one category not listed by name collection . Find ( Builders < Product >. Filter . AnyNin ( x => x . Categories , new [] { \"Balls\" , \"Rackets\" })); Any... The Any* conditions look at every element of an array but match only once with respect to the document. So, if multiple elements of an array match a condition, we only get the document once in the result set. Query execution pipeline \u00b6 MongoDB queries executed through a pipeline. We won't go into details about this, but in addition to simple filtering, we'll see a few examples frequently used in queries. Paging, sorting \u00b6 For paging, we specify the maximum number of matching documents we request: collection . Find (...). Limit ( 100 ); And for the items on the following page, we skip the items already seen on the first page: collection . Find (...). Skip ( 100 ). Limit ( 100 ); Skip and Limit are meaningless in this form because without sorting, the \"first 100 elements\" query is not deterministic. So for these types of queries, it is necessary to provide an appropriate sorting requirement. Sorting is defined using Builders<T> . collection . Find (...) . Sort ( Builders < Product >. Sort . Ascending ( x => x . Name )) . Skip ( 100 ). Limit ( 100 ); Paging issue The above paging mechanism is still not entirely correct. For example, if a product is deleted in between the query of the first and second pages, the products will shift by one, and there may be a product that will be skipped. This is, in fact, not a problem just with MongoDB. Consider how you would solve this problem. Number documents \u00b6 There are two ways to query the number of documents that match a query: collection . CountDocuments ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )). CountDocuments (); Grouping \u00b6 Grouping is a syntactically complex operation. For grouping, we need to define an aggregation pipeline. We will not discuss this in more detail, but the following example shows its use. // products in the \"Balls\" category grouped by VAT percentage foreach ( var g in collection . Aggregate () . Match ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )) // filtering . Group ( x => x . VAT . Percentage , x => x ) // grouping . ToList ()) { Console . WriteLine ( $\"VAT percentage: {g.Key}\" ); foreach ( var p in g ) Console . WriteLine ( $\"\\tProduct: {p.Name}\" ); } Insert, Modify, Delete \u00b6 After queries, let's get to know data modification constructs. Inserting a new document \u00b6 To insert a new document, you need the object representing the new document. We can add this to the collection. var newProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruits\" } }; collection . InsertOne ( newProduct ); Console . WriteLine ( $\"Inserted record id id: {newProduct.Id}\" ); // after insert the ID of the document will be available in the C# instance Note that the Id field is not assigned. This will be set by the client driver. If we want, we can give it a value, but it is not customary. Remember, there is no schema in MongoDB, so the inserted document may be completely different from the rest of the items in the collection. Note that not all fields are assigned values. Because there are no integrity criteria, any insertion will be successful, but there may be problems with queries (for example, assuming that the Stock field is always set). You can use the InsertMany function to insert multiple documents, but remember that there are no transactions, so adding multiple documents is an independent operation. If, for any reason, an error occurs during the insertion, the successfully inserted documents will remain in the database. However, each document is saved atomically, so no \"half\" document can be added to the database in the event of an error. Delete documents \u00b6 To delete, you need to define a filter condition and execute it with the DeleteOne or DeleteMany functions. The difference is that DeleteOne only deletes the first matching document, while DeleteMany deletes all. If you know that only one document can match this condition (for example, deleting it by ID), you should use DeleteOne as the database does not have to perform an exhaustive search. The deletion condition can be described by the syntax familiar to the search. Deletion is different from Entity Framework. Here, the entity does not have to be loaded; instead, we specify a filtering condition. var deleteResult = collection . DeleteOne ( x => x . Id == new ObjectId ( \"...\" )); Console . WriteLine ( $\"Deleted: {deleteResult.DeletedCount} records\" ); If you want to retrieve the deleted element, you can use FindOneAndDelete , which returns the deleted entity. Updating documents \u00b6 Perhaps the most interesting feature of MongoDB is the update of documents. While the functionalities showed before (queries, inserts, deletions) are similar to most databases (either relational or NoSQL), MongoDB supports a much broader range of modification operations. There are two ways to change a document: replace the entire document with a new one or update its parts. Complete document replacement \u00b6 To replace a document completely, we need a filtering condition to specify which document we want to replace; and we need a new document. var replacementProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruit\" } }; var replaceResult = collection . ReplaceOne ( x => x . Id == new ObjectId ( \"...\" ), replacementProduct ); Console . WriteLine ( $\"Updated: {replaceResult.ModifiedCount}\" ); A single document is matched and replaces it with another document. The operation itself is atomic, that is, if it is interrupted, no half document is saved. You can use the FindOneAndReplace method to get the pre-swap document. Interesting It is also possible to change the document ID during update (the replacement document can have a different ID). Document update operators \u00b6 Document update operators can change the value of a document's fields atomically without replacing the entire document. We use the help of the Builder<T> to describe the modifying operations. Set your stock to a constant value: collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 )); The first parameter of the UpdateOne function is the filter condition. You can use any of the syntax described before. The second parameter is the descriptor of the update operation, which you can build with Builders<T> . In the example code above, the argument names are specified ( filter: and update: ) to make it clear what the parameter represents. This is optional, but it increases readability (at the expense of code length). The operation can update multiple fields at the same time. collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 ) . CurrentDate ( x => x . StockUpdated ) . Unset ( x => x . NeedsUpdate ) ); Typical modifier operators are: Set : Set the value of the field; SetOnInsert : like Set but executed only when a new document is inserted (see upsert below); Unset : delete field (remove key and value from document); CurrentDate : set the current date; Inc : increment value; Min , Max : change the value of a field if the value entered is smaller / larger than the current value of the field; Mul : value multiplication; PopFirst , PopLast : remove first / last element from an array; Pull : remove value from an array; Push : add value to an array at the end (further options in the same operator: array sorting, keeping the first n element of an array); AddToSet : add a value to an array if it does not already exist. The above operations are meaningful even if the specified field does not exist. Depending on the type of operator, the database will make changes to a default value. For example, for Inc and Mul , the field will be set to 0 and then modified. For array operations, an empty array is modified. For other operations, you can look up the behavior in the documentation . Multiple documents can be modified at the same time using this method. The requested update operations are performed on all documents that match the filter criteria. For example: in view of the summer season, put all balls on sale with a 25% discount. collection . UpdateMany ( filter : Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" ), update : Builders < Product >. Update . Mul ( x => x . Price , 0.75 ) . AddToSet ( x => x . Categories , \"On sale\" )); Update operators change the documents atomically. Using them can eliminate some of the problems caused by concurrent data access. Upsert : replacing a non-existent document \u00b6 During update operations, we have the option upsert (update/insert) . This means that either an insertion or an update is made, depending on whether the item was in the database. The default behavior is not to upsert, we must request it explicitly. collection . ReplaceOne ( filter : x => x . Id == new ObjectId ( \"...\" ), replacement : replacementObject , options : new UpdateOptions () { IsUpsert = true }); We can also do upsert with update operators. As we have seen, modifier operators are not concerned about missing fields. Likewise, it does not matter if the document does not exist; this is equivalent to performing a modifying operation on a completely blank document. collection . UpdateOne ( filter : ..., update : ..., options : new UpdateOptions () { IsUpsert = true }); The upsert operation can be a workaround for managing concurrency in the absence of a transaction. Because we do not have a transaction, we cannot verify before insertion that a particular record does not yet exist. Instead, we can use the upsert method, which allows atomic querying and insertion/modification. merge Note: In SQL, the merge command provides a similar solution.","title":"MongoDB basics, operations, and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#mongodb-basics-operations-and-the-mongodb-net-driver","text":"","title":"MongoDB basics, operations, and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#nosql-databases","text":"NoSQL databases are data management systems that do not work with the relational data model. The NoSQL name can be a little misleading, as the concept has little to do with the SQL language - the main difference is rather the representation of the data. Why do we need such new databases when we already have the relational model and relational databases? A small database with a simple schema can be easily described in the relational model. But our applications evolve: new functionalities are added, making the schema more complex. More and more data is added to the database making the maintenance inefficient. Relational databases need constant schema changes and updates, which can be cumbersome. Constant data migrations can be a pain. Furthermore, relational databases can be bottlenecks from the scalability perspective - but we will not discuss this aspect in more detail. NoSQL databases offer a solution to these problems. Instead of the rigid schema , NoSQL databases have a flexible schema . In other words, we will require less consistency regarding the data.","title":"NoSQL databases"},{"location":"lecture-notes/mongodb/#basic-concepts-of-mongodb","text":"MongoDB is a client-server database system that has a non-relational schema. The mongod (Mongo daemon) process on the right is the database server. The other side is our application, where a client connects to the database using a network connection. This connection uses the so-called wire protocol , which is a MongoDB proprietary communication protocol. The protocol transmits data and queries in JSON format represented in a binary fashion as BSON.","title":"Basic concepts of MongoDB"},{"location":"lecture-notes/mongodb/#logical-structure","text":"The top layer of a MongoDB database system is the so-called cluster . The servers are organized into these clusters. We will not discuss clusters here; these are tools for enabling scalability. The databases are the mongod processes, which host the databases. A MongoDB server/cluster stores multiple databases. And the databases contain collections . If we want to map these concepts to a relational model, then the collections correspond to the tables, and the rows/records in a table correspond to the documents of the collection. Let us investigate these further.","title":"Logical structure"},{"location":"lecture-notes/mongodb/#document","text":"The document is the unit of storage in MongoDB. A document is a JSON (-like) file: it contains key-value pairs. MongoDB itself stores it as a BSON in binary format. { name : \"sue\" , age : 26 , status : \"A\" , groups : [ \"news\" , \"sports\" ] } The keys can have arbitrary names with a few limitations, such that they have to be unique and cannot begin with the $ character. The names are case sensitive. Values can be string, number, date, binary, embedded document, null , or even as the groups in the example shows, an array - a relational database cannot represent an array so simple. Mapping to the object-oriented world, a document is an object. MongoDB documents have a maximum size of 16MB and this is not a configurable parameter.","title":"Document"},{"location":"lecture-notes/mongodb/#collection","text":"Collections are analogous to relational database tables, but without a schema. Collections need no definition; the system creates them upon first use. Collections are a place for \"similar\" documents. Although there is no schema, indexes can still be defined on the collections to support fast searching. Since there is no schema, there are no domain integrity requirements enforced either.","title":"Collection"},{"location":"lecture-notes/mongodb/#database","text":"The database has the same purpose as in the relational model. It gathers all data of our application. Access management is also configured on the database level. The name of databases is case sensitive and lowercase by convention.","title":"Database"},{"location":"lecture-notes/mongodb/#key","text":"The _id field is the unambiguous identifier of each document. Other keys cannot be defined. This field does not need to be specified during insert; the client driver or the server can generate a new value (a 12 byte ObjectId by default). Uniqueness can be guaranteed with the use of indices. We can define an index and mark it as unique to create a key-like field. These unique indices can contain multiple fields too. There are no references to keys in MongoDB. A document can reference another document by copying it's key, but the system has no consistency guarantees for these (e.g., the referenced document can be deleted).","title":"Key"},{"location":"lecture-notes/mongodb/#mongodb-operations-and-the-mongodb-net-driver","text":"The following code snippets use the official Nuget package MongoDB.Driver .","title":"MongoDB operations and the MongoDB .NET Driver"},{"location":"lecture-notes/mongodb/#establishing-a-connection","text":"To access the MongoDB database, you first need a connection. A MongoClient class represents the connection. We need the server address to establish a connection (see https://docs.mongodb.com/manual/reference/connection-string/ for details on the connection string). var client = new MongoClient ( \"mongodb://localhost:27017\" ); The connection should be treated as a singleton and not disposed of. Connection lifetime The connection is typically stored in a global static variable, or an IoC (Inversion of Control) / DI (Dependency Injection) store. Although the database name may be in the connection string (e.g. mongodb://localhost:27017/datadriven ), it is used only for authentication. Thus, after establishing the connection, we need to specify what database we will use. var db = client . GetDatabase ( \"datadriven\" ); The database does not need to exist in advance. The above call will automatically create if the database does not already exist.","title":"Establishing a connection"},{"location":"lecture-notes/mongodb/#managing-collections","text":"Unlike a relational database, in MongoDB, our operations are always performed on a single collection , so the selection of a collection is not part of the issued command (like where in SQL), but a prerequisite for the operation. You can get a specific collection by calling GetCollection ; its generic parameter is the C# class implementing the document type. var collection = db . GetCollection < BsonDocument >( \"products\" ); The basic concept of the .NET MongoDB driver is to map every document to a .NET object. This is also called ODM (Object Document Mapping) . ODM is the equivalent of ORM in the NoSQL database world. \"Raw\" json In other languages and platforms, MongoDB drivers do not always map to objects. Sample codes found on the Internet often show communication via \"raw\" JSON documents. Let's try to avoid this, as we learned in ORM, that object-oriented mapping is more convenient and secure. In the previous example, a document of the type \"BsonDocument\" is used. BsonDocument is a generic document representation in which we can store key-value pairs. It is uncomfortable and unsafe to use; thus we usually do try to avoid it. See the suggested solution soon. You can run queries on the variable representing the collection, such as inserting a document and then listing the contents of the collection. The collection will be created automatically the first time you use it, so you don't have to define it. collection . InsertOne ( new BsonDocument () { { \"name\" , \"Apple\" }, { \"categoryName\" , \"Apple\" }, { \"price\" , 123 } }); // listing all documents: a search criteria is needed // this is an empty criteria matching all documents var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var l in list ) Console . WriteLine ( l ); Naming convention Field names in the document start with lowercase letters like price or categoryName (this is the so-called camel case spelling). This is a convention of the MongoDB world for historical reasons. Unless there is a good reason, do not deviate from it.","title":"Managing collections"},{"location":"lecture-notes/mongodb/#mapping-documents-to-c-objects","text":"As with relational databases, we can work with objects and classes in MongoDB. The .NET driver for MongoDB offers this conveniently. The first step is to define the C# class(es) to map the contents of the database. Since there is no schema for the database and table, we cannot generate C# code based on the schema (as we did with the Entity Framework). So in this world, we tend to follow the Code First approach, which is to write C# code and have the system translate it to database collections. Let us define the following classes to represent Products . public class Product { public ObjectId Id { get ; set ; } // this will be the identifier with name _id public string Name { get ; set ; } public float Price { get ; set ; } public int Stock { get ; set ; } public string [] Categories { get ; set ; } // array field public VAT VAT { get ; set ; } // embedded document } public class VAT // this class is only ever embedded, hence needs to id { public string VATCategoryName { get ; set ; } public float Percentage { get ; set ; } } Note that the name of the field was price before, but in C# it starts with a capital letter, according to Pascal Case : Price . The MongoDB .NET driver integrates with the C# language and the .NET environment and respects its conventions so that the names in the class definition and the field names in the MongoDB documents will be mapped automatically: the Price class property will be price in the document.","title":"Mapping documents to C# objects"},{"location":"lecture-notes/mongodb/#customizing-the-mapping","text":"The C# class - MongoDB document mapping is automatic, but it can also be customized. There are several ways to deviate from the conventions. The easiest way is to use custom attributes in the class definition: public class Product { // maps to field _id [BsonId] public ObjectId Identifier { get ; set ; } // can specify the name explicitly [BsonElement(\"price\")] public string TotalPrice { get ; set ; } // properties can be ignores [BsonIgnore] public string DoNotSace { get ; set ; } } Our other option is to register so-called convention packs at a higher level. The convention pack describes the rules of mapping. (A set of conventions also defines the default behavior.) For example, you can specify the following to map the field names to camel case and exclude data members with a default value (defined in the C# language) from the document. // defince convention pack var pack = new ConventionPack (); pack . Add ( new CamelCaseElementNameConvention ()); pack . Add ( new IgnoreIfDefaultConvention ( true )); // register the convention pack // the first parameter is a name to reference this pack // the last argument is a fitlering criteria when to use this convention ConventionRegistry . Register ( \"datadriven\" , pack , t => true ); We also have more sophisticated customizations, such as defining conversion logic for translation between a C# representation and a MongoDB representation, and specifying how to save inheritance hierarchies. For more details, see the official documentation: https://mongodb.github.io/mongo-csharp-driver/2.8/reference/bson/serialization/ .","title":"Customizing the mapping"},{"location":"lecture-notes/mongodb/#queries","text":"We will use the collection from now on by mapping it to the Product class. This is the recommended solution; the BsonDocument based solution is used only when necessary. The simplest query we have already seen is to list all the documents: var collection = db . GetCollection < Product >( \"products\" ); var list = collection . Find ( new BsonDocument ()). ToList (); foreach ( var p in list ) Console . WriteLine ( $\"Id: {p.Id}, Name: {p.Name}\" ); Listing is done using the Find method. The name illustrates MongoDB's philosophy: listing an entire collection is not practical, so there is no simple syntax for it. Find requires a search criteria, which is an empty condition here to matches everything. There are several ways to describe search criteria. With BsonDocument based filtering, the filtering condition must be written according to the MongoDB syntax. We generally will avoid this because the MongoDB .NET driver provides a more convenient solution for us. In most cases, we can use Lambda expressions to describe the filtering. collection . Find ( x => x . Price < 123 ); In this case, the Lambda expression is a delegate of type Predicate <T> , that is, expects a Product and returns bool . Thus in the example above, the x variable represents a Products instance. Of course, this search also works for more complex cases. collection . Find ( x => x . Price < 123 && x . Name . Contains ( \"red\" )); The filtering described by the Lambda expressions hides what search syntax we actually have in MongoDB. For example, the above Contains search condition will actually mean a search with a regular expression. In MongoDB's own language, the previous filter looks like this: { \"price\" : { \"$lt\" : 123.0 }, \"name\" : \"/red/s\" } Note that this description is itself a document. If we wanted to write the filter condition ourselves, we would have to create this descriptor in a BsonDocument . The document keys describe the fields used for filtering, and the values are the filter criteria. In some cases, the condition is a scalar value such as a regular expression (or if we filter for equality); in other cases, the condition is an embedded document, as with the < condition. Here, the $lt key is a special key that denotes the less than operator and the value to the right of the operator is 123.0. The regular expression should be specified according to JavaScript RegExp Syntax . The conditions listed in this way are automatically evaluated in and and fashion. Instead of the Lambda expression, we can create a similar description without having to compile a filter condition in \"text\" form. The .NET driver for MongoDB gives us the ability to use a so-called builders . collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Lt ( x => x . Price , 123 ), Builders < Product >. Filter . Regex ( x => x . Name , \"/red/s\" ), ) ); The above syntax is a bit more eloquent than the Lambda expression, but it is closer to the MongoDB philosophy, and better describes what we want. We can view this syntax as SQL, a declarative, goal-oriented, but platform-specific description. However, it is also type-safe. The Builders<T> generic class is an auxiliary class that we can use to build filtering and other MongoDB specific definitions. Builders<Product>.Filter can be used to define filtering conditions that match the Product C# class. First, we create an and connection, within which we have two filtering conditions. The operators are the less than and regular expressions seen before. We pass two parameters to these functions: the field to be filtered and the operand. Note that no string-based field names were used here or in the Lambda expressions. We can refer to the class fields with the C# Expression syntax. This is practical because we avoid typing field names. Note that all ways of describing the search criteria are identical. The MongoDB driver maps each syntax to its internal representation. Lambda expression-based requires fewer characters and fits better into C#, while the builder approach is used to express MongoDB features better. You can use either one.","title":"Queries"},{"location":"lecture-notes/mongodb/#using-query-results","text":"The result of the collection.Find(...) function is not yet the result set, but only a descriptor to execute the query. There are generally three ways to retrieve and process the result.","title":"Using query results"},{"location":"lecture-notes/mongodb/#listing","text":"Get the complete result set as a list: collection.Find(...).ToList() .","title":"Listing"},{"location":"lecture-notes/mongodb/#get-firstsingle-item","text":"If you only need the first item, or know that there will be only one item, you can use collection.Find(...).First() , .FirstOrDefault() , or .Single() , .SingleOrDefault() functions.","title":"Get first/single item"},{"location":"lecture-notes/mongodb/#cursor","text":"If the result set contains multiple documents, it is advisable to iterate it using a cursor. MongoDB limits the size of the response to a query, so if we query too many records, we may get an error instead of a result. To overcome this, we use the cursors where we always get only a subset of the documents. var cur = collection . Find (...). ToCursor (); while ( cur . MoveNext ()) // cursor stepping { foreach ( var t in cur . Current ) // the value of the cursor is not a single document, but a list in itself { ... } }","title":"Cursor"},{"location":"lecture-notes/mongodb/#operators-for-filtering","text":"The filter criteria apply to the fields in the document, and the filter criteria are always constant. Thus it is not possible, for example, to compare two fields , and we cannot refer to other collections. There is a so-called MongoDB aggregation pipeline, which allows you to formulate more complex queries, but for now, let us focus on simple queries. The filter condition compares a field in the document to a constant we specify. The following options are most commonly used.","title":"Operators for filtering"},{"location":"lecture-notes/mongodb/#comparison-operators","text":"collection . Find ( x => x . Price == 123 ); collection . Find ( Builders < Product >. Filter . Eq ( x => x . Price , 123 )); //Eq, as in equals collection . Find ( x => x . Price != 123 ); collection . Find ( Builders < Product >. Filter . Ne ( x => x . Price , 123 )); // Ne, as in not equals collection . Find ( x => x . Price >= 123 ); collection . Find ( Builders < Product >. Filter . Gte ( x => x . Price , 123 )); // Gte, as in greater than or equal to collection . Find ( x => x . Price < 123 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . Price , 123 )); // Lt, as in less than","title":"Comparison operators"},{"location":"lecture-notes/mongodb/#boolean-operators","text":"collection . Find ( x => x . Price > 500 && x . Price < 1000 ); collection . Find ( Builders < Product >. Filter . And ( Builders < Product >. Filter . Gt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Price , 1000 ) ) ); collection . Find ( x => x . Price < 500 || x . Stock < 10 ); collection . Find ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ); collection . Find ( x => !( x . Price < 500 || x . Stock < 10 )); collection . Find ( Builders < Product >. Filter . Not ( Builders < Product >. Filter . Or ( Builders < Product >. Filter . Lt ( x => x . Price , 500 ), Builders < Product >. Filter . Lt ( x => x . Stock , 10 ) ) ) );","title":"Boolean operators"},{"location":"lecture-notes/mongodb/#value-is-one-of-multiple-alternatives","text":"collection . Find ( x => x . Id == ... || x . Id = ...); collection . Find ( Builders < Product >. Filter . In ( x => x . Id , new [] { ... })); // similarly Nin, as in not in oper\u00e1tor","title":"Value is one of multiple alternatives"},{"location":"lecture-notes/mongodb/#value-exists-not-null","text":"collection . Find ( x => x . VAT != null ); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT )); Exists filtering Does exist, that is, non-null filtering is special because there are two ways to have a null value in MongoDB: if the key exists in the document and it has a value of null; or if the key does not exist at all.","title":"Value exists (not null)"},{"location":"lecture-notes/mongodb/#filtering-fields-of-embedded-document","text":"Embedded documents can be used for filtering in the same way. The following are all valid, and it does not matter if the embedded document ( VAT ) does not exist: collection . Find ( x => x . VAT . Percentage < 27 ); collection . Find ( Builders < Product >. Filter . Lt ( x => x . VAT . Percentage , 27 )); collection . Find ( Builders < Product >. Filter . Exists ( x => x . VAT . Percentage , exists : false )); // does not exists, that is, in C#, equals null","title":"Filtering fields of embedded document"},{"location":"lecture-notes/mongodb/#filtering-based-on-an-array-field","text":"Any field in the document can be an array value, as in the example string [] Categories . In MongoDB, we can define filtering based on an array field using the Any* criterion. // products of this category collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); // products that are assigned to at least one category not listed by name collection . Find ( Builders < Product >. Filter . AnyNin ( x => x . Categories , new [] { \"Balls\" , \"Rackets\" })); Any... The Any* conditions look at every element of an array but match only once with respect to the document. So, if multiple elements of an array match a condition, we only get the document once in the result set.","title":"Filtering based on an array field"},{"location":"lecture-notes/mongodb/#query-execution-pipeline","text":"MongoDB queries executed through a pipeline. We won't go into details about this, but in addition to simple filtering, we'll see a few examples frequently used in queries.","title":"Query execution pipeline"},{"location":"lecture-notes/mongodb/#paging-sorting","text":"For paging, we specify the maximum number of matching documents we request: collection . Find (...). Limit ( 100 ); And for the items on the following page, we skip the items already seen on the first page: collection . Find (...). Skip ( 100 ). Limit ( 100 ); Skip and Limit are meaningless in this form because without sorting, the \"first 100 elements\" query is not deterministic. So for these types of queries, it is necessary to provide an appropriate sorting requirement. Sorting is defined using Builders<T> . collection . Find (...) . Sort ( Builders < Product >. Sort . Ascending ( x => x . Name )) . Skip ( 100 ). Limit ( 100 ); Paging issue The above paging mechanism is still not entirely correct. For example, if a product is deleted in between the query of the first and second pages, the products will shift by one, and there may be a product that will be skipped. This is, in fact, not a problem just with MongoDB. Consider how you would solve this problem.","title":"Paging, sorting"},{"location":"lecture-notes/mongodb/#number-documents","text":"There are two ways to query the number of documents that match a query: collection . CountDocuments ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )); collection . Find ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )). CountDocuments ();","title":"Number documents"},{"location":"lecture-notes/mongodb/#grouping","text":"Grouping is a syntactically complex operation. For grouping, we need to define an aggregation pipeline. We will not discuss this in more detail, but the following example shows its use. // products in the \"Balls\" category grouped by VAT percentage foreach ( var g in collection . Aggregate () . Match ( Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" )) // filtering . Group ( x => x . VAT . Percentage , x => x ) // grouping . ToList ()) { Console . WriteLine ( $\"VAT percentage: {g.Key}\" ); foreach ( var p in g ) Console . WriteLine ( $\"\\tProduct: {p.Name}\" ); }","title":"Grouping"},{"location":"lecture-notes/mongodb/#insert-modify-delete","text":"After queries, let's get to know data modification constructs.","title":"Insert, Modify, Delete"},{"location":"lecture-notes/mongodb/#inserting-a-new-document","text":"To insert a new document, you need the object representing the new document. We can add this to the collection. var newProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruits\" } }; collection . InsertOne ( newProduct ); Console . WriteLine ( $\"Inserted record id id: {newProduct.Id}\" ); // after insert the ID of the document will be available in the C# instance Note that the Id field is not assigned. This will be set by the client driver. If we want, we can give it a value, but it is not customary. Remember, there is no schema in MongoDB, so the inserted document may be completely different from the rest of the items in the collection. Note that not all fields are assigned values. Because there are no integrity criteria, any insertion will be successful, but there may be problems with queries (for example, assuming that the Stock field is always set). You can use the InsertMany function to insert multiple documents, but remember that there are no transactions, so adding multiple documents is an independent operation. If, for any reason, an error occurs during the insertion, the successfully inserted documents will remain in the database. However, each document is saved atomically, so no \"half\" document can be added to the database in the event of an error.","title":"Inserting a new document"},{"location":"lecture-notes/mongodb/#delete-documents","text":"To delete, you need to define a filter condition and execute it with the DeleteOne or DeleteMany functions. The difference is that DeleteOne only deletes the first matching document, while DeleteMany deletes all. If you know that only one document can match this condition (for example, deleting it by ID), you should use DeleteOne as the database does not have to perform an exhaustive search. The deletion condition can be described by the syntax familiar to the search. Deletion is different from Entity Framework. Here, the entity does not have to be loaded; instead, we specify a filtering condition. var deleteResult = collection . DeleteOne ( x => x . Id == new ObjectId ( \"...\" )); Console . WriteLine ( $\"Deleted: {deleteResult.DeletedCount} records\" ); If you want to retrieve the deleted element, you can use FindOneAndDelete , which returns the deleted entity.","title":"Delete documents"},{"location":"lecture-notes/mongodb/#updating-documents","text":"Perhaps the most interesting feature of MongoDB is the update of documents. While the functionalities showed before (queries, inserts, deletions) are similar to most databases (either relational or NoSQL), MongoDB supports a much broader range of modification operations. There are two ways to change a document: replace the entire document with a new one or update its parts.","title":"Updating documents"},{"location":"lecture-notes/mongodb/#complete-document-replacement","text":"To replace a document completely, we need a filtering condition to specify which document we want to replace; and we need a new document. var replacementProduct = new Product { Name = \"Apple\" , Price = 890 , Categories = new [] { \"Fruit\" } }; var replaceResult = collection . ReplaceOne ( x => x . Id == new ObjectId ( \"...\" ), replacementProduct ); Console . WriteLine ( $\"Updated: {replaceResult.ModifiedCount}\" ); A single document is matched and replaces it with another document. The operation itself is atomic, that is, if it is interrupted, no half document is saved. You can use the FindOneAndReplace method to get the pre-swap document. Interesting It is also possible to change the document ID during update (the replacement document can have a different ID).","title":"Complete document replacement"},{"location":"lecture-notes/mongodb/#document-update-operators","text":"Document update operators can change the value of a document's fields atomically without replacing the entire document. We use the help of the Builder<T> to describe the modifying operations. Set your stock to a constant value: collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 )); The first parameter of the UpdateOne function is the filter condition. You can use any of the syntax described before. The second parameter is the descriptor of the update operation, which you can build with Builders<T> . In the example code above, the argument names are specified ( filter: and update: ) to make it clear what the parameter represents. This is optional, but it increases readability (at the expense of code length). The operation can update multiple fields at the same time. collection . UpdateOne ( filter : x => x . Id == new ObjectId ( \"...\" ), update : Builders < Product >. Update . Set ( x => x . Stock , 5 ) . CurrentDate ( x => x . StockUpdated ) . Unset ( x => x . NeedsUpdate ) ); Typical modifier operators are: Set : Set the value of the field; SetOnInsert : like Set but executed only when a new document is inserted (see upsert below); Unset : delete field (remove key and value from document); CurrentDate : set the current date; Inc : increment value; Min , Max : change the value of a field if the value entered is smaller / larger than the current value of the field; Mul : value multiplication; PopFirst , PopLast : remove first / last element from an array; Pull : remove value from an array; Push : add value to an array at the end (further options in the same operator: array sorting, keeping the first n element of an array); AddToSet : add a value to an array if it does not already exist. The above operations are meaningful even if the specified field does not exist. Depending on the type of operator, the database will make changes to a default value. For example, for Inc and Mul , the field will be set to 0 and then modified. For array operations, an empty array is modified. For other operations, you can look up the behavior in the documentation . Multiple documents can be modified at the same time using this method. The requested update operations are performed on all documents that match the filter criteria. For example: in view of the summer season, put all balls on sale with a 25% discount. collection . UpdateMany ( filter : Builders < Product >. Filter . AnyEq ( x => x . Categories , \"Balls\" ), update : Builders < Product >. Update . Mul ( x => x . Price , 0.75 ) . AddToSet ( x => x . Categories , \"On sale\" )); Update operators change the documents atomically. Using them can eliminate some of the problems caused by concurrent data access.","title":"Document update operators"},{"location":"lecture-notes/mongodb/#upsert-replacing-a-non-existent-document","text":"During update operations, we have the option upsert (update/insert) . This means that either an insertion or an update is made, depending on whether the item was in the database. The default behavior is not to upsert, we must request it explicitly. collection . ReplaceOne ( filter : x => x . Id == new ObjectId ( \"...\" ), replacement : replacementObject , options : new UpdateOptions () { IsUpsert = true }); We can also do upsert with update operators. As we have seen, modifier operators are not concerned about missing fields. Likewise, it does not matter if the document does not exist; this is equivalent to performing a modifying operation on a completely blank document. collection . UpdateOne ( filter : ..., update : ..., options : new UpdateOptions () { IsUpsert = true }); The upsert operation can be a workaround for managing concurrency in the absence of a transaction. Because we do not have a transaction, we cannot verify before insertion that a particular record does not yet exist. Instead, we can use the upsert method, which allows atomic querying and insertion/modification. merge Note: In SQL, the merge command provides a similar solution.","title":"Upsert: replacing a non-existent document"},{"location":"lecture-notes/mssql/server-side-programming/","text":"Microsoft SQL Server programming \u00b6 The code below can be run on the sample database . Variables \u00b6 Variable declaration: DECLARE @ num int SELECT @ num -- NULL Assigning a value with SET instruction or part of the declaration: DECLARE @ num int = 5 SELECT @ num -- 5 SET @ num = 3 SELECT @ num -- 3 The variable is not bound to the instruction block. The variable is accessible within the so-called batch or stored procedure: BEGIN DECLARE @ num int SET @ num = 3 END SELECT @ num -- This works, the variable is accessible even outside the declaring instruction block. -- 3 GO -- begins a new batch SELECT @ num -- Error: Must declare the scalar variable \"@num\". Assigning value through query result: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 1 SELECT @ name If the query yields multiple results, the last value will be assigned: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer -- will match multiple rows SELECT @ name -- last value of the SELECT will be assinged Ig the query yields no results, the variable is not changed: DECLARE @ name nvarchar ( max ) SET @ name = 'aaa' SELECT @ name = Name FROM Customer WHERE ID = 99999999 -- no matches SELECT @ name -- aaa Instruction blocks and control flow statements \u00b6 Instruction block: BEGIN DECLARE @ num int SET @ num = 3 END Condition (if the customer exists we update the email address): DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 123 IF @ name IS NOT NULL BEGIN PRINT 'Updating email' UPDATE Customer SET Email = 'agh*******@gmail.com' WHERE ID = 123 END ELSE BEGIN PRINT 'No such customer' END Cycle (generating new products, e.g. for testing): WHILE ( SELECT COUNT ( * ) FROM Product ) < 1000 BEGIN INSERT INTO Product ( Name , Price , Stock , VATID , CategoryID ) VALUES ( 'Abc' , 1 , 1 , 3 , 13 ) END Stored procedure \u00b6 Inserting a new VAT percentage into table VAT if it does not exist: create or alter procedure InsertNewVAT -- create procedure with this name @ Percentage int -- parameters of the procedure as begin tran -- protection against nonrepeatable read set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values ( @ Percentage ) else print 'error' ; commit Invoking the stored procedure: exec InsertNewVAT 27 Deleting the stored procedure: drop procedure InsertNewVAT Before SQL Server 2016 there was no create or alter only the two separately. Stored functions \u00b6 Query VAT percentages over a threshold: CREATE FUNCTION VATPercentages ( @ min int ) RETURNS TABLE AS RETURN ( SELECT ID , Percentage FROM VAT WHERE Percentage > @ min ); Usage: SELECT * FROM VATPercentages ( 20 ) Error handling \u00b6 Inserting a new VAT percentage into table VAT ; if it already exist, raise and error: create or alter procedure InsertNewVAT @ Percentage int as begin tran set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values ( @ Percentage ) else throw 51000 , 'error' , 1 ; commit Error handling: begin try exec InsertNewVAT 27 end try begin catch SELECT ERROR_NUMBER () AS ErrorNumber , ERROR_SEVERITY () AS ErrorSeverity , ERROR_STATE () AS ErrorState , ERROR_PROCEDURE () AS ErrorProcedure , ERROR_LINE () AS ErrorLine , ERROR_MESSAGE () AS ErrorMessage ; end catch Trigger \u00b6 Let us log the deletion of products into an audit log table: -- Creating the audit log table create table AuditLog ([ Description ] [ nvarchar ]( max ) NULL ) go -- Logging trigger create or alter trigger ProductDeleteLog on Product for delete as insert into AuditLog ( Description ) select 'Product deleted: ' + convert ( nvarchar , d . Name ) from deleted d Let us suppose that customers have two email addresses: one for the login and another one for the newsletter, which is not necessarily specified. Let us always know the effective email address: -- Add the new email addresses alter table Customer add [ NotificationEmail ] nvarchar ( max ), [ EffectiveEmail ] nvarchar ( max ) go -- The trigger to keep the effective address up to date create or alter trigger CustomerEmailUpdate on Customer for insert , update as update Customer set EffectiveEmail = ISNULL ( i . NotificationEmail , i . Email ) from Customer c join inserted i on c . ID = i . ID A total sum column on the order table should be updated to reflect the total of all items in the order: create or alter trigger OrderTotalUpdateTrigger on OrderItem for insert , update , delete as update Order set Total = isnull ( Total , 0 ) + TotalChange from Order inner join ( select i . OrderID , sum ( Amount * Price ) as TotalChange from inserted i group by i . OrderID ) OrderChange on Order . ID = OrderChange . OrderID update Order set Total = isnull ( Total , 0 ) \u2013 TotalChange from Order inner join ( select d . OrderID , sum ( Amount * Price ) as TotalChange from deleted d group by d . OrderID ) OrderChange on Order . ID = OrderChange . OrderID Cursor \u00b6 Let us find the products of which there are only a few in stock, and if the last time we sold one of them has been more than a year ago, then let us put the product on sale: DECLARE @ ProductName nvarchar ( max ) DECLARE @ ProductID int DECLARE @ LastOrder datetime DECLARE products_cur CURSOR FAST_FORWARD READ_ONLY FOR SELECT Id , Name FROM Product WHERE Stock < 3 OPEN products_cur FETCH FROM products_cur INTO @ ProductID , @ ProductName WHILE @@ FETCH_STATUS = 0 BEGIN SELECT @ LastOrder = MAX ([ Order ]. Date ) FROM [ Order ] JOIN OrderItem ON [ Order ]. Id = OrderItem . OrderId WHERE OrderItem . ProductID = @ ProductId PRINT CONCAT ( 'ProductID: ' , convert ( nvarchar , @ ProductID ), ' Last order: ' , ISNULL ( convert ( nvarchar , @ LastOrder ), 'No last order' )) IF @ LastOrder IS NULL OR @ LastOrder < DATEADD ( year , - 1 , GETDATE ()) BEGIN UPDATE Product SET Price = Price * 0 . 75 WHERE Id = @ ProductID END FETCH FROM products_cur INTO @ ProductID , @ ProductName END CLOSE products_cur DEALLOCATE products_cur","title":"Microsoft SQL Server programming"},{"location":"lecture-notes/mssql/server-side-programming/#microsoft-sql-server-programming","text":"The code below can be run on the sample database .","title":"Microsoft SQL Server programming"},{"location":"lecture-notes/mssql/server-side-programming/#variables","text":"Variable declaration: DECLARE @ num int SELECT @ num -- NULL Assigning a value with SET instruction or part of the declaration: DECLARE @ num int = 5 SELECT @ num -- 5 SET @ num = 3 SELECT @ num -- 3 The variable is not bound to the instruction block. The variable is accessible within the so-called batch or stored procedure: BEGIN DECLARE @ num int SET @ num = 3 END SELECT @ num -- This works, the variable is accessible even outside the declaring instruction block. -- 3 GO -- begins a new batch SELECT @ num -- Error: Must declare the scalar variable \"@num\". Assigning value through query result: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 1 SELECT @ name If the query yields multiple results, the last value will be assigned: DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer -- will match multiple rows SELECT @ name -- last value of the SELECT will be assinged Ig the query yields no results, the variable is not changed: DECLARE @ name nvarchar ( max ) SET @ name = 'aaa' SELECT @ name = Name FROM Customer WHERE ID = 99999999 -- no matches SELECT @ name -- aaa","title":"Variables"},{"location":"lecture-notes/mssql/server-side-programming/#instruction-blocks-and-control-flow-statements","text":"Instruction block: BEGIN DECLARE @ num int SET @ num = 3 END Condition (if the customer exists we update the email address): DECLARE @ name nvarchar ( max ) SELECT @ name = Name FROM Customer WHERE ID = 123 IF @ name IS NOT NULL BEGIN PRINT 'Updating email' UPDATE Customer SET Email = 'agh*******@gmail.com' WHERE ID = 123 END ELSE BEGIN PRINT 'No such customer' END Cycle (generating new products, e.g. for testing): WHILE ( SELECT COUNT ( * ) FROM Product ) < 1000 BEGIN INSERT INTO Product ( Name , Price , Stock , VATID , CategoryID ) VALUES ( 'Abc' , 1 , 1 , 3 , 13 ) END","title":"Instruction blocks and control flow statements"},{"location":"lecture-notes/mssql/server-side-programming/#stored-procedure","text":"Inserting a new VAT percentage into table VAT if it does not exist: create or alter procedure InsertNewVAT -- create procedure with this name @ Percentage int -- parameters of the procedure as begin tran -- protection against nonrepeatable read set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values ( @ Percentage ) else print 'error' ; commit Invoking the stored procedure: exec InsertNewVAT 27 Deleting the stored procedure: drop procedure InsertNewVAT Before SQL Server 2016 there was no create or alter only the two separately.","title":"Stored procedure"},{"location":"lecture-notes/mssql/server-side-programming/#stored-functions","text":"Query VAT percentages over a threshold: CREATE FUNCTION VATPercentages ( @ min int ) RETURNS TABLE AS RETURN ( SELECT ID , Percentage FROM VAT WHERE Percentage > @ min ); Usage: SELECT * FROM VATPercentages ( 20 )","title":"Stored functions"},{"location":"lecture-notes/mssql/server-side-programming/#error-handling","text":"Inserting a new VAT percentage into table VAT ; if it already exist, raise and error: create or alter procedure InsertNewVAT @ Percentage int as begin tran set transaction isolation level repeatable read declare @ Count int select @ Count = count ( * ) from VAT where Percentage = @ Percentage if @ Count = 0 insert into VAT values ( @ Percentage ) else throw 51000 , 'error' , 1 ; commit Error handling: begin try exec InsertNewVAT 27 end try begin catch SELECT ERROR_NUMBER () AS ErrorNumber , ERROR_SEVERITY () AS ErrorSeverity , ERROR_STATE () AS ErrorState , ERROR_PROCEDURE () AS ErrorProcedure , ERROR_LINE () AS ErrorLine , ERROR_MESSAGE () AS ErrorMessage ; end catch","title":"Error handling"},{"location":"lecture-notes/mssql/server-side-programming/#trigger","text":"Let us log the deletion of products into an audit log table: -- Creating the audit log table create table AuditLog ([ Description ] [ nvarchar ]( max ) NULL ) go -- Logging trigger create or alter trigger ProductDeleteLog on Product for delete as insert into AuditLog ( Description ) select 'Product deleted: ' + convert ( nvarchar , d . Name ) from deleted d Let us suppose that customers have two email addresses: one for the login and another one for the newsletter, which is not necessarily specified. Let us always know the effective email address: -- Add the new email addresses alter table Customer add [ NotificationEmail ] nvarchar ( max ), [ EffectiveEmail ] nvarchar ( max ) go -- The trigger to keep the effective address up to date create or alter trigger CustomerEmailUpdate on Customer for insert , update as update Customer set EffectiveEmail = ISNULL ( i . NotificationEmail , i . Email ) from Customer c join inserted i on c . ID = i . ID A total sum column on the order table should be updated to reflect the total of all items in the order: create or alter trigger OrderTotalUpdateTrigger on OrderItem for insert , update , delete as update Order set Total = isnull ( Total , 0 ) + TotalChange from Order inner join ( select i . OrderID , sum ( Amount * Price ) as TotalChange from inserted i group by i . OrderID ) OrderChange on Order . ID = OrderChange . OrderID update Order set Total = isnull ( Total , 0 ) \u2013 TotalChange from Order inner join ( select d . OrderID , sum ( Amount * Price ) as TotalChange from deleted d group by d . OrderID ) OrderChange on Order . ID = OrderChange . OrderID","title":"Trigger"},{"location":"lecture-notes/mssql/server-side-programming/#cursor","text":"Let us find the products of which there are only a few in stock, and if the last time we sold one of them has been more than a year ago, then let us put the product on sale: DECLARE @ ProductName nvarchar ( max ) DECLARE @ ProductID int DECLARE @ LastOrder datetime DECLARE products_cur CURSOR FAST_FORWARD READ_ONLY FOR SELECT Id , Name FROM Product WHERE Stock < 3 OPEN products_cur FETCH FROM products_cur INTO @ ProductID , @ ProductName WHILE @@ FETCH_STATUS = 0 BEGIN SELECT @ LastOrder = MAX ([ Order ]. Date ) FROM [ Order ] JOIN OrderItem ON [ Order ]. Id = OrderItem . OrderId WHERE OrderItem . ProductID = @ ProductId PRINT CONCAT ( 'ProductID: ' , convert ( nvarchar , @ ProductID ), ' Last order: ' , ISNULL ( convert ( nvarchar , @ LastOrder ), 'No last order' )) IF @ LastOrder IS NULL OR @ LastOrder < DATEADD ( year , - 1 , GETDATE ()) BEGIN UPDATE Product SET Price = Price * 0 . 75 WHERE Id = @ ProductID END FETCH FROM products_cur INTO @ ProductID , @ ProductName END CLOSE products_cur DEALLOCATE products_cur","title":"Cursor"},{"location":"lecture-notes/mssql/sql/","text":"SQL language, MSSQL platform-specific SQL \u00b6 You can run these queries on the sample database . Simple queries \u00b6 Which product costs less than 2000 and have less than 50 in stock? select Name , Price , Stock from Product where Price < 2000 and Stock < 50 Which product has no description? select * from Product where Description is null Joining tables \u00b6 Customers with a main site in Budapest (the two alternatives are equivalent). select * from Customer c , CustomerSite s where c . MainCustomerSiteID = s . ID and City = 'Budapest' select * from Customer c inner join CustomerSite s on c . MainCustomerSiteID = s . ID where City = 'Budapest' List the products that start with letter M, the ordered amounts and deadlines. Include the products that have not been ordered yet. select p . Name , oi . Amount from Product p left outer join OrderItem oi on p . id = oi . ProductID left outer join [ Order ] o on o . ID = oi . OrderID where p . Name like 'M%' [Order] [Order] is in brackets, because this signals that this is a table name and not the beginning of the order by SQL language element. Sorting \u00b6 select * from Product order by Name Microsoft SQL Server specific: collation specifies the rules for sorting select * from Product order by Name collate SQL_Latin1_General_Cp1_CI_AI Sort by multiple fields select * from Product order by Stock desc , Price Subqueries \u00b6 List the order statuses, deadlines and dates select o . Date , o . Deadline , s . Name from [ Order ] o inner join Status s on o . StatusId = s . ID An alternative, but the two are not equivalent: the subquery is the equivalent of the left outer join and not the innter join! select o . Date , o . Deadline , ( select s . Name from Status s where o . StatusId = s . ID ) from [ Order ] o Filter duplicates \u00b6 Which products have been ordered in batches of more than 3? One product may have been ordered multiple times, but we want the name only once. select distinct p . Name from Product p inner join OrderItem oi on oi . ProductID = p . ID where oi . Amount > 3 Aggregate functions \u00b6 How much is the most expensive product? select max ( Price ) from Product Which are the most expensive products? select * from Product where Price = ( select max ( Price ) from Product ) What was the min, max and average selling price of each product with name containing Lego having an average selling price more than 10000 select p . Id , p . Name , min ( oi . Price ), max ( oi . Price ), avg ( oi . Price ) from Product p inner join OrderItem oi on p . ID = oi . ProductID Where p . Name like '%Lego%' group by p . Id , p . Name having avg ( oi . Price ) > 10000 order by 2 Inserting records \u00b6 Inserting a single record by assigning value to all columns (except identity ) insert into Product values ( 'aa' , 100 , 0 , 3 , 2 , null ) Set values of selected columns only insert into Product ( Name , Price ) values ( 'aa' , 100 ) Insert the result of a query insert into Product ( Name , Price ) select Name , Price from InvoiceItem where Amount > 2 MSSQL specific: identity column create table VAT ( ID int identity primary key , Percentage int ) insert into VAT ( Percentage ) values ( 27 ) select @@ identity MSSQL specific: setting the value of identity column set identity_insert VAT on insert into VAT ( ID , Percentage ) values ( 123 , 27 ) set identity_insert VAT off Updating records \u00b6 Raise the price of LEGOs by 10% and add 5 to stock update Product set Price = 1 . 1 * Price , Stock = Stock + 5 where Name like '%Lego%' Update based on filtering by referenced table content: raise the price by 10% for those products that are subject to 20% VAT, and have more then 10 pcs in stock update Product set Price = 1 . 1 * Price where Stock > 10 and VATID in ( select ID from VAT where Percentage = 20 ) MSSQL Server specific solution to the same task update Product set Price = 1 . 1 * Price from Product p inner join VAT v on p . VATID = v . ID where Stock > 10 and Percentage = 20 Deleting records \u00b6 delete from Product where ID > 10 Assigning ranks \u00b6 Assigning ranks by ordering select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p Ranking within groups select p . * , rank () over ( partition by CategoryID order by Name ) as r , dense_rank () over ( partition by CategoryID order by Name ) as dr from Product p CTE (Common Table Expression) \u00b6 Motivation: subqueries often make queries complex First three products sorted by name alphabetically select * from ( select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p ) a where a . dr <= 3 Same solution using CTE with q1 as ( select * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product ) select * from q1 where q1 . dr < 3 How many pieces have been sold from the second most expensive product? with q as ( select * , dense_rank () over ( order by Price desc ) dr from Product ) select q . ID , q . Name , sum ( Amount ) from q inner join OrderItem oi on oi . ProductID = q . ID where q . dr = 2 group by q . ID , q . Name Paging: list products alphabetically from 3. to 8. record with q as ( select * , rank () over ( order by Name ) r from Product ) select * from q where q . r between 3 and 8 Paging using MSSQL Server (2012+) specific syntax select * from Product order by Name offset 2 rows fetch next 6 rows only select top 3 * from Product order by Name","title":"SQL language, MSSQL platform-specific SQL"},{"location":"lecture-notes/mssql/sql/#sql-language-mssql-platform-specific-sql","text":"You can run these queries on the sample database .","title":"SQL language, MSSQL platform-specific SQL"},{"location":"lecture-notes/mssql/sql/#simple-queries","text":"Which product costs less than 2000 and have less than 50 in stock? select Name , Price , Stock from Product where Price < 2000 and Stock < 50 Which product has no description? select * from Product where Description is null","title":"Simple queries"},{"location":"lecture-notes/mssql/sql/#joining-tables","text":"Customers with a main site in Budapest (the two alternatives are equivalent). select * from Customer c , CustomerSite s where c . MainCustomerSiteID = s . ID and City = 'Budapest' select * from Customer c inner join CustomerSite s on c . MainCustomerSiteID = s . ID where City = 'Budapest' List the products that start with letter M, the ordered amounts and deadlines. Include the products that have not been ordered yet. select p . Name , oi . Amount from Product p left outer join OrderItem oi on p . id = oi . ProductID left outer join [ Order ] o on o . ID = oi . OrderID where p . Name like 'M%' [Order] [Order] is in brackets, because this signals that this is a table name and not the beginning of the order by SQL language element.","title":"Joining tables"},{"location":"lecture-notes/mssql/sql/#sorting","text":"select * from Product order by Name Microsoft SQL Server specific: collation specifies the rules for sorting select * from Product order by Name collate SQL_Latin1_General_Cp1_CI_AI Sort by multiple fields select * from Product order by Stock desc , Price","title":"Sorting"},{"location":"lecture-notes/mssql/sql/#subqueries","text":"List the order statuses, deadlines and dates select o . Date , o . Deadline , s . Name from [ Order ] o inner join Status s on o . StatusId = s . ID An alternative, but the two are not equivalent: the subquery is the equivalent of the left outer join and not the innter join! select o . Date , o . Deadline , ( select s . Name from Status s where o . StatusId = s . ID ) from [ Order ] o","title":"Subqueries"},{"location":"lecture-notes/mssql/sql/#filter-duplicates","text":"Which products have been ordered in batches of more than 3? One product may have been ordered multiple times, but we want the name only once. select distinct p . Name from Product p inner join OrderItem oi on oi . ProductID = p . ID where oi . Amount > 3","title":"Filter duplicates"},{"location":"lecture-notes/mssql/sql/#aggregate-functions","text":"How much is the most expensive product? select max ( Price ) from Product Which are the most expensive products? select * from Product where Price = ( select max ( Price ) from Product ) What was the min, max and average selling price of each product with name containing Lego having an average selling price more than 10000 select p . Id , p . Name , min ( oi . Price ), max ( oi . Price ), avg ( oi . Price ) from Product p inner join OrderItem oi on p . ID = oi . ProductID Where p . Name like '%Lego%' group by p . Id , p . Name having avg ( oi . Price ) > 10000 order by 2","title":"Aggregate functions"},{"location":"lecture-notes/mssql/sql/#inserting-records","text":"Inserting a single record by assigning value to all columns (except identity ) insert into Product values ( 'aa' , 100 , 0 , 3 , 2 , null ) Set values of selected columns only insert into Product ( Name , Price ) values ( 'aa' , 100 ) Insert the result of a query insert into Product ( Name , Price ) select Name , Price from InvoiceItem where Amount > 2 MSSQL specific: identity column create table VAT ( ID int identity primary key , Percentage int ) insert into VAT ( Percentage ) values ( 27 ) select @@ identity MSSQL specific: setting the value of identity column set identity_insert VAT on insert into VAT ( ID , Percentage ) values ( 123 , 27 ) set identity_insert VAT off","title":"Inserting records"},{"location":"lecture-notes/mssql/sql/#updating-records","text":"Raise the price of LEGOs by 10% and add 5 to stock update Product set Price = 1 . 1 * Price , Stock = Stock + 5 where Name like '%Lego%' Update based on filtering by referenced table content: raise the price by 10% for those products that are subject to 20% VAT, and have more then 10 pcs in stock update Product set Price = 1 . 1 * Price where Stock > 10 and VATID in ( select ID from VAT where Percentage = 20 ) MSSQL Server specific solution to the same task update Product set Price = 1 . 1 * Price from Product p inner join VAT v on p . VATID = v . ID where Stock > 10 and Percentage = 20","title":"Updating records"},{"location":"lecture-notes/mssql/sql/#deleting-records","text":"delete from Product where ID > 10","title":"Deleting records"},{"location":"lecture-notes/mssql/sql/#assigning-ranks","text":"Assigning ranks by ordering select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p Ranking within groups select p . * , rank () over ( partition by CategoryID order by Name ) as r , dense_rank () over ( partition by CategoryID order by Name ) as dr from Product p","title":"Assigning ranks"},{"location":"lecture-notes/mssql/sql/#cte-common-table-expression","text":"Motivation: subqueries often make queries complex First three products sorted by name alphabetically select * from ( select p . * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product p ) a where a . dr <= 3 Same solution using CTE with q1 as ( select * , rank () over ( order by Name ) as r , dense_rank () over ( order by Name ) as dr from Product ) select * from q1 where q1 . dr < 3 How many pieces have been sold from the second most expensive product? with q as ( select * , dense_rank () over ( order by Price desc ) dr from Product ) select q . ID , q . Name , sum ( Amount ) from q inner join OrderItem oi on oi . ProductID = q . ID where q . dr = 2 group by q . ID , q . Name Paging: list products alphabetically from 3. to 8. record with q as ( select * , rank () over ( order by Name ) r from Product ) select * from q where q . r between 3 and 8 Paging using MSSQL Server (2012+) specific syntax select * from Product order by Name offset 2 rows fetch next 6 rows only select top 3 * from Product order by Name","title":"CTE (Common Table Expression)"},{"location":"lecture-notes/transactions/","text":"Transactions in databases \u00b6 Context When we talk about transactions, we mean relational databases . The problem and the solutions, however, are generic and are not specific to relational databases. Concurrent data access \u00b6 Database management systems are based on a client-server architecture. The client (the software we write) connects to the database and executes queries. We should always remember that there is a single database, but multiple clients involved here. The purpose of the database system is to serve as many requests as possible; consequently, it executes the queries concurrently . In such a concurrent system, data access can overlap in the following ways. If the concurrent data access (either read or write) concerns independent data, there is no problem, and the operations may proceed concurrently. If all operations only read data, there is no issue either; multiple readers can access the same data. However, if the same data is accessed simultaneously and there is at least one writer , a concurrency problem may manifest itself. This concurrency issue is analogous to the mutual exclusion problem known in operating systems and the various programming languages and frameworks. Concurrent data access in these scenarios usually involve mutual access to shared memory space, and the solution is ensuring mutual exclusion using some kind of guard. In database management systems, concurrency is related to the records (rows) of database tables, and the guards are transactions. Transactions \u00b6 Definition A transaction is a logical unit of a process, a series of operations that only make sense together. A transaction combines operations into one unit, and the system guarantees the following properties: atomic execution, consistency, isolation from each other, and durability. Let us examine these basic properties to understand how concurrent data access issues are resolved with their help. A transaction is just a tool A transaction, similarly to mutexes provides by an operating system of programming framework, is just a tool provided to the software developer. The proper usage is the responsibility of the developer. Transactions basic properties \u00b6 Atomicity \u00b6 Atomic execution means that we have a sequence of operations, and this sequence is meaningful only when all of it is executed. In other words, partial execution must be prohibited. In database systems, we often need multiple statements to achieve our goal, hence the sequence of steps. Let us imaging the checkout process in a webshop: The order is recorded in the database with the provided data The amount of stock is decreased by one since one piece was sold These steps only make sense together. Given that an order has been recorded, the amount of stock must be compensated; otherwise, the data becomes invalid, and we sell more products than we have. Thus, we must not abort the sequence of steps in the middle. This is what atomicity guarantees: if executing a sequence of steps has begun, all steps have to complete successfully or the initial state before the modification must be restored . Consistency \u00b6 The database's consistency rules are described by the integrity requirements, such as the record referenced by a foreign key must exist. There are other types of consistency requirements; e.g., there cannot be more students registered for an exam than the limit in the Neptun system. Transactions ensure that our database is always in a consistent state. While a transaction is in progress, temporary inconsistencies may arise, similarly to the interim state between the two steps of the sequence of the operation above. However, at the end of the transaction, consistency must be restored. In other words: transactions enforce transition between consistent states . Durability \u00b6 Durability prescribes that the effect of a transaction is durable , that is, the results are not lost. Practically it means that the modifications performed by a transaction must be flushed to persistent storage (i.e., disk). There are two types of errors in database systems that can lead to data corruption: soft crash and hard crash. Soft crash means the database process terminates, and the content of memory is lost. Transactions offer protection from these kinds of crashes. A hard crash means that the disk is also affected. Only a backup can provide protection here. Isolation \u00b6 By isolation, we mean to isolate the effect of transactions from each other. That is, when writing our query, we do not need to concern ourselves with other concurrent transactions; the system will handle this aspect. The developer can write queries as if they were executed in the system alone, and the system will guarantee that it will prohibit those concurrency issues that we do not want to deal with . The system will still run transactions concurrently. However, it guarantees to schedule the transactions to not violate the rules of the isolation level requested by the transaction. Therefore, all transactions need to specify the requested isolation level . Isolation problems and isolation levels \u00b6 Before we can discuss the isolation levels, we need to first understand the types of problems that concurrency can cause. Problems \u00b6 Dirty read \u00b6 A dirty read means that a transaction accesses the uncommitted data of another transaction: A transaction modifies a record in the database but does not commit yet. Another transaction reads the same record (in its changed state). The first transaction is aborted, and the system restores the record to the state it was in before the change. The transaction that read the record in the second step is now working with invalid, non-existent data. It should not have read it. Source Source of images: https://vladmihalcea.com/2014/01/05/a-beginners-guide-to-acid-and-database-transactions/ Dirty read should almost always be avoided. Lost update \u00b6 During a lost update, two writes conflict: A transaction changes a record. Another transaction overwrites the same record. The database has the result of the second write as if the first did not even happen. Non-repeatable read \u00b6 A non-repeatable read means that the result of the query depends on the time it was issued: A transaction queries a record. A different transaction changes the same record. If the first transaction re-executes the same query as before, it gets a different result. Phantom records / phantom read \u00b6 We face the problem of phantom records when we work with recordsets: A transaction executes a query that yields multiple records as a result. Meanwhile, a different transaction deletes a record that is included in the previous result set. The first transaction starts processing its result set (e.g., iterates over the records one by one). Should the deleted record be processed now? We can imagine a similar scenario when a record is altered in the second step. Which state should the reader transaction in step three see? The one before, or the one after the modification? Isolation levels \u00b6 The problems discussed before can be avoided by using the right isolation level. We should consider, though, that the \"higher\" level of isolation we prescribe, the lower the throughput of the database system will be. Also, we might face deadlocks (see below). Our goal, thus, is a compromise between a suitable isolation level and performance. The ANSI/ISO SQL standard defines the following isolation levels: Read uncommitted: offers no protection. Read committed: no dirty read. Repeatable read: no dirty read and no non-repeatable read. Serializable: prohibits all issues. Read uncommitted is seldom used. Serializable , similarly, is avoided if possible. The default, usually, is read committed . Scheduling enforced with locks \u00b6 The database enforces isolation through locks: when a record is accessed (read or write), it is locked by the system. The lock is placed on the record when it is first accessed and is removed at the end of the transaction. The type of lock (e.g., shared lock or mutually exclusive) depends on the isolation level and the implementation of the database management system. These locks, in effect, enforce the scheduling of the transactions. When a lock is not available, because the record it used by another translation and concurrent access is not allowed by the isolation level, the transaction will wait. We know that when we use locks, deadlock can occur. This is no different in databases. A deadlock may occur when two transactions are competing for the same locks. See the figure below; a continuous line represents an owned lock, while the dashed ones represent a lock the transaction would like to acquire. Neither of these requests can be fulfilled, resulting in both transactions being unable to move forward. Deadlocks cannot be prevented in database management systems, but they can be recognized and dealt with. The system monitors locks, and when a deadlock is detected one of the transactions is aborted and all its modifications are rolled back. All applications using a database must be prepared to handle this. When a deadlock happens, there is usually no other resolution to retry the operation later (e.g., automatically, or manually requested by the end-user). Transaction boundaries \u00b6 A transaction combines a sequence of steps. It is, therefore, necessary to mark the beginning and the end of the transaction. The way transaction boundaries are signaled may depend on the platform, but generally: All operations are executed within the scope of a transaction. If the transaction boundary is not marked explicitly, each statement is a transaction in itself. Since all SQL statements run within a transaction scope, the transaction properties are automatically guaranteed for all statements. For example, a delete statement affecting multiple records cannot abort and delete only half of the records. The developer executes a begin transaction SQL statement to start a transaction, and completes it either with commit or rollback . Commit completes the translation and saves its changes, while rollback aborts the transaction and undoes its changes. Some database management systems enable nested transactions too. Completing transactions follow the nesting: each level needs to be committed. Transaction logging \u00b6 So far, we have covered what transactions are used for. Let us understand how they work internally. Transactional logging is the process used by the database management system to track the pending modifications of running transactions allowing rolling back these changes in case of abort or soft crash. To understand transactional logging, let us consider the following system model. This conceptual model includes the following operations: Begin T(x): Start of transaction Input(A): Read data from the durable database store (disk) Output(A): Write data to durable database store (disk) Read(A): Transaction reads the data from the memory buffer Write(A): Transaction writes the data to the memory buffer FLUSH_LOG: Write the transaction log to disk The process of transactional logging is demonstrated in the following example. In this example, a transaction modifies two data elements: A is decreases by 2, and B is increased by 2. Undo transaction log \u00b6 We begin with an empty memory buffer. Every data is on disk. The process starts by reading the data from disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Begin(T1) 10 20 - - Begin T1 Input(A) 10 20 10 - Input(B) 10 20 10 20 The transaction has all the necessary data in the memory buffer. The modification is performed, and the data is written back to the buffer. At the same time, the original values and written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20 The transaction completes, and it saves the changes. The transaction commits, which first flushes the transaction log to disk, then the changes are persisted to disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Output(B) 8 22 8 22 Commit T1 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. There is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, some data could already be written to disk. These need to be reverted. The transaction log is processed starting from the end, and for all transactions that have no commit mark in the log, the values must be restored to their original state. To summarize, when using undo logging: the database cannot be modified until the transaction log is flushed, and the commit mark must be placed into the log once the database writes are finished. The key is to flush the transaction log before the changes are persisted. The drawback of this method is that the transaction log is flushed twice, which is a performance issue due to the cost of disk access. Redo transaction log \u00b6 The process starts with reading the data from disk, followed by performing the modifications, but this time the final values are written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 22 To finalize the transaction, the log is flushed first to register the modified values - but no modification is made to the database files yet. Thus, the transaction log needs to be written to disk only once (compared to the undo logging scheme). Operation A (database) B (database) A (buffer) B (buffer) Transactional log Commit T1 Flush_LOG 10 20 8 22 After the transaction log is persisted, the changes are committed to the database files. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Output(A) 8 20 8 22 Output(B) 8 22 8 22 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. In that case, there is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, the commit mark is flushed to the log, but no changes were made to the database yet. Restoring from an aborted state at this stage is performed by processing the transaction log from the beginning and redoing all committed transactions. To summarize, when using redo logging: the database cannot be modified until the transaction log is flushed, commit mark must be placed into the transaction log before writing the database files. There are fewer transaction log flushes in this scheme compared to undo logging; however, the restore procedure is longer. Undo/redo logging \u00b6 As the name suggests, this is the combination of the two schemes. The process starts just like in the previous cases. The difference is in writing the transaction log: both the original and the modified values are written to the log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20, 22 The commit procedure is simpler. The order of writing the database files and writing the commit mark into the transaction log is no longer fixed - however, flushing the transaction log must still be performed first. The simplification, therefore, is that the place of the commit mark is not fixed. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Commit T1 Output(B) 8 22 8 22 Restore needs to combine the procedures discussed before: committed transactions are replayed (just like in redo logging), while aborted transactions are reverted (just like in undo logging). This solution has the following advantages: there is less synchronization during the commit procedure (with regards to writing the transaction log and the database files), the changes can be persisted in the database files sooner (no need to wait for writing the commit mark). Reducing the transaction log \u00b6 The transaction log needs to be emptied periodically. Transactions that are committed and persisted into the database files can be purged from the log. Similarly, aborted transactions that were reverted can also be removed. This is performed automatically by the system, but can also be triggered manually. Long-running transactions can significantly increase the size of the log. The larger the log is, the longer the purging process will take. Questions to test your knowledge \u00b6 What type of concurrent data access problems do you know? List the isolation levels. Which problems does each of the levels prohibit? What are the basic properties of transactions? Decide whether the following statements are true or false: The serializable isolation level executes the transactions one after the other. Deadlock can be prevented by using the right isolation level. The default isolation level is usually read committed . If we are not using explicit transactions, then we are protected from the issue of dirty read. The transaction log offers protection against all kinds of data losses. In the redo transaction logging scheme, the transaction log starts with the commit mark.","title":"Transactions in databases"},{"location":"lecture-notes/transactions/#transactions-in-databases","text":"Context When we talk about transactions, we mean relational databases . The problem and the solutions, however, are generic and are not specific to relational databases.","title":"Transactions in databases"},{"location":"lecture-notes/transactions/#concurrent-data-access","text":"Database management systems are based on a client-server architecture. The client (the software we write) connects to the database and executes queries. We should always remember that there is a single database, but multiple clients involved here. The purpose of the database system is to serve as many requests as possible; consequently, it executes the queries concurrently . In such a concurrent system, data access can overlap in the following ways. If the concurrent data access (either read or write) concerns independent data, there is no problem, and the operations may proceed concurrently. If all operations only read data, there is no issue either; multiple readers can access the same data. However, if the same data is accessed simultaneously and there is at least one writer , a concurrency problem may manifest itself. This concurrency issue is analogous to the mutual exclusion problem known in operating systems and the various programming languages and frameworks. Concurrent data access in these scenarios usually involve mutual access to shared memory space, and the solution is ensuring mutual exclusion using some kind of guard. In database management systems, concurrency is related to the records (rows) of database tables, and the guards are transactions.","title":"Concurrent data access"},{"location":"lecture-notes/transactions/#transactions","text":"Definition A transaction is a logical unit of a process, a series of operations that only make sense together. A transaction combines operations into one unit, and the system guarantees the following properties: atomic execution, consistency, isolation from each other, and durability. Let us examine these basic properties to understand how concurrent data access issues are resolved with their help. A transaction is just a tool A transaction, similarly to mutexes provides by an operating system of programming framework, is just a tool provided to the software developer. The proper usage is the responsibility of the developer.","title":"Transactions"},{"location":"lecture-notes/transactions/#transactions-basic-properties","text":"","title":"Transactions basic properties"},{"location":"lecture-notes/transactions/#atomicity","text":"Atomic execution means that we have a sequence of operations, and this sequence is meaningful only when all of it is executed. In other words, partial execution must be prohibited. In database systems, we often need multiple statements to achieve our goal, hence the sequence of steps. Let us imaging the checkout process in a webshop: The order is recorded in the database with the provided data The amount of stock is decreased by one since one piece was sold These steps only make sense together. Given that an order has been recorded, the amount of stock must be compensated; otherwise, the data becomes invalid, and we sell more products than we have. Thus, we must not abort the sequence of steps in the middle. This is what atomicity guarantees: if executing a sequence of steps has begun, all steps have to complete successfully or the initial state before the modification must be restored .","title":"Atomicity"},{"location":"lecture-notes/transactions/#consistency","text":"The database's consistency rules are described by the integrity requirements, such as the record referenced by a foreign key must exist. There are other types of consistency requirements; e.g., there cannot be more students registered for an exam than the limit in the Neptun system. Transactions ensure that our database is always in a consistent state. While a transaction is in progress, temporary inconsistencies may arise, similarly to the interim state between the two steps of the sequence of the operation above. However, at the end of the transaction, consistency must be restored. In other words: transactions enforce transition between consistent states .","title":"Consistency"},{"location":"lecture-notes/transactions/#durability","text":"Durability prescribes that the effect of a transaction is durable , that is, the results are not lost. Practically it means that the modifications performed by a transaction must be flushed to persistent storage (i.e., disk). There are two types of errors in database systems that can lead to data corruption: soft crash and hard crash. Soft crash means the database process terminates, and the content of memory is lost. Transactions offer protection from these kinds of crashes. A hard crash means that the disk is also affected. Only a backup can provide protection here.","title":"Durability"},{"location":"lecture-notes/transactions/#isolation","text":"By isolation, we mean to isolate the effect of transactions from each other. That is, when writing our query, we do not need to concern ourselves with other concurrent transactions; the system will handle this aspect. The developer can write queries as if they were executed in the system alone, and the system will guarantee that it will prohibit those concurrency issues that we do not want to deal with . The system will still run transactions concurrently. However, it guarantees to schedule the transactions to not violate the rules of the isolation level requested by the transaction. Therefore, all transactions need to specify the requested isolation level .","title":"Isolation"},{"location":"lecture-notes/transactions/#isolation-problems-and-isolation-levels","text":"Before we can discuss the isolation levels, we need to first understand the types of problems that concurrency can cause.","title":"Isolation problems and isolation levels"},{"location":"lecture-notes/transactions/#problems","text":"","title":"Problems"},{"location":"lecture-notes/transactions/#dirty-read","text":"A dirty read means that a transaction accesses the uncommitted data of another transaction: A transaction modifies a record in the database but does not commit yet. Another transaction reads the same record (in its changed state). The first transaction is aborted, and the system restores the record to the state it was in before the change. The transaction that read the record in the second step is now working with invalid, non-existent data. It should not have read it. Source Source of images: https://vladmihalcea.com/2014/01/05/a-beginners-guide-to-acid-and-database-transactions/ Dirty read should almost always be avoided.","title":"Dirty read"},{"location":"lecture-notes/transactions/#lost-update","text":"During a lost update, two writes conflict: A transaction changes a record. Another transaction overwrites the same record. The database has the result of the second write as if the first did not even happen.","title":"Lost update"},{"location":"lecture-notes/transactions/#non-repeatable-read","text":"A non-repeatable read means that the result of the query depends on the time it was issued: A transaction queries a record. A different transaction changes the same record. If the first transaction re-executes the same query as before, it gets a different result.","title":"Non-repeatable read"},{"location":"lecture-notes/transactions/#phantom-records-phantom-read","text":"We face the problem of phantom records when we work with recordsets: A transaction executes a query that yields multiple records as a result. Meanwhile, a different transaction deletes a record that is included in the previous result set. The first transaction starts processing its result set (e.g., iterates over the records one by one). Should the deleted record be processed now? We can imagine a similar scenario when a record is altered in the second step. Which state should the reader transaction in step three see? The one before, or the one after the modification?","title":"Phantom records / phantom read"},{"location":"lecture-notes/transactions/#isolation-levels","text":"The problems discussed before can be avoided by using the right isolation level. We should consider, though, that the \"higher\" level of isolation we prescribe, the lower the throughput of the database system will be. Also, we might face deadlocks (see below). Our goal, thus, is a compromise between a suitable isolation level and performance. The ANSI/ISO SQL standard defines the following isolation levels: Read uncommitted: offers no protection. Read committed: no dirty read. Repeatable read: no dirty read and no non-repeatable read. Serializable: prohibits all issues. Read uncommitted is seldom used. Serializable , similarly, is avoided if possible. The default, usually, is read committed .","title":"Isolation levels"},{"location":"lecture-notes/transactions/#scheduling-enforced-with-locks","text":"The database enforces isolation through locks: when a record is accessed (read or write), it is locked by the system. The lock is placed on the record when it is first accessed and is removed at the end of the transaction. The type of lock (e.g., shared lock or mutually exclusive) depends on the isolation level and the implementation of the database management system. These locks, in effect, enforce the scheduling of the transactions. When a lock is not available, because the record it used by another translation and concurrent access is not allowed by the isolation level, the transaction will wait. We know that when we use locks, deadlock can occur. This is no different in databases. A deadlock may occur when two transactions are competing for the same locks. See the figure below; a continuous line represents an owned lock, while the dashed ones represent a lock the transaction would like to acquire. Neither of these requests can be fulfilled, resulting in both transactions being unable to move forward. Deadlocks cannot be prevented in database management systems, but they can be recognized and dealt with. The system monitors locks, and when a deadlock is detected one of the transactions is aborted and all its modifications are rolled back. All applications using a database must be prepared to handle this. When a deadlock happens, there is usually no other resolution to retry the operation later (e.g., automatically, or manually requested by the end-user).","title":"Scheduling enforced with locks"},{"location":"lecture-notes/transactions/#transaction-boundaries","text":"A transaction combines a sequence of steps. It is, therefore, necessary to mark the beginning and the end of the transaction. The way transaction boundaries are signaled may depend on the platform, but generally: All operations are executed within the scope of a transaction. If the transaction boundary is not marked explicitly, each statement is a transaction in itself. Since all SQL statements run within a transaction scope, the transaction properties are automatically guaranteed for all statements. For example, a delete statement affecting multiple records cannot abort and delete only half of the records. The developer executes a begin transaction SQL statement to start a transaction, and completes it either with commit or rollback . Commit completes the translation and saves its changes, while rollback aborts the transaction and undoes its changes. Some database management systems enable nested transactions too. Completing transactions follow the nesting: each level needs to be committed.","title":"Transaction boundaries"},{"location":"lecture-notes/transactions/#transaction-logging","text":"So far, we have covered what transactions are used for. Let us understand how they work internally. Transactional logging is the process used by the database management system to track the pending modifications of running transactions allowing rolling back these changes in case of abort or soft crash. To understand transactional logging, let us consider the following system model. This conceptual model includes the following operations: Begin T(x): Start of transaction Input(A): Read data from the durable database store (disk) Output(A): Write data to durable database store (disk) Read(A): Transaction reads the data from the memory buffer Write(A): Transaction writes the data to the memory buffer FLUSH_LOG: Write the transaction log to disk The process of transactional logging is demonstrated in the following example. In this example, a transaction modifies two data elements: A is decreases by 2, and B is increased by 2.","title":"Transaction logging"},{"location":"lecture-notes/transactions/#undo-transaction-log","text":"We begin with an empty memory buffer. Every data is on disk. The process starts by reading the data from disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Begin(T1) 10 20 - - Begin T1 Input(A) 10 20 10 - Input(B) 10 20 10 20 The transaction has all the necessary data in the memory buffer. The modification is performed, and the data is written back to the buffer. At the same time, the original values and written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20 The transaction completes, and it saves the changes. The transaction commits, which first flushes the transaction log to disk, then the changes are persisted to disk. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Output(B) 8 22 8 22 Commit T1 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. There is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, some data could already be written to disk. These need to be reverted. The transaction log is processed starting from the end, and for all transactions that have no commit mark in the log, the values must be restored to their original state. To summarize, when using undo logging: the database cannot be modified until the transaction log is flushed, and the commit mark must be placed into the log once the database writes are finished. The key is to flush the transaction log before the changes are persisted. The drawback of this method is that the transaction log is flushed twice, which is a performance issue due to the cost of disk access.","title":"Undo transaction log"},{"location":"lecture-notes/transactions/#redo-transaction-log","text":"The process starts with reading the data from disk, followed by performing the modifications, but this time the final values are written to the transaction log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 22 To finalize the transaction, the log is flushed first to register the modified values - but no modification is made to the database files yet. Thus, the transaction log needs to be written to disk only once (compared to the undo logging scheme). Operation A (database) B (database) A (buffer) B (buffer) Transactional log Commit T1 Flush_LOG 10 20 8 22 After the transaction log is persisted, the changes are committed to the database files. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Output(A) 8 20 8 22 Output(B) 8 22 8 22 How can the consistent state be restored in case of a soft crash? Suppose the transaction is aborted before the commit. In that case, there is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash. If the transaction is in the middle of the commit procedure, the commit mark is flushed to the log, but no changes were made to the database yet. Restoring from an aborted state at this stage is performed by processing the transaction log from the beginning and redoing all committed transactions. To summarize, when using redo logging: the database cannot be modified until the transaction log is flushed, commit mark must be placed into the transaction log before writing the database files. There are fewer transaction log flushes in this scheme compared to undo logging; however, the restore procedure is longer.","title":"Redo transaction log"},{"location":"lecture-notes/transactions/#undoredo-logging","text":"As the name suggests, this is the combination of the two schemes. The process starts just like in the previous cases. The difference is in writing the transaction log: both the original and the modified values are written to the log. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20, 22 The commit procedure is simpler. The order of writing the database files and writing the commit mark into the transaction log is no longer fixed - however, flushing the transaction log must still be performed first. The simplification, therefore, is that the place of the commit mark is not fixed. Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Commit T1 Output(B) 8 22 8 22 Restore needs to combine the procedures discussed before: committed transactions are replayed (just like in redo logging), while aborted transactions are reverted (just like in undo logging). This solution has the following advantages: there is less synchronization during the commit procedure (with regards to writing the transaction log and the database files), the changes can be persisted in the database files sooner (no need to wait for writing the commit mark).","title":"Undo/redo logging"},{"location":"lecture-notes/transactions/#reducing-the-transaction-log","text":"The transaction log needs to be emptied periodically. Transactions that are committed and persisted into the database files can be purged from the log. Similarly, aborted transactions that were reverted can also be removed. This is performed automatically by the system, but can also be triggered manually. Long-running transactions can significantly increase the size of the log. The larger the log is, the longer the purging process will take.","title":"Reducing the transaction log"},{"location":"lecture-notes/transactions/#questions-to-test-your-knowledge","text":"What type of concurrent data access problems do you know? List the isolation levels. Which problems does each of the levels prohibit? What are the basic properties of transactions? Decide whether the following statements are true or false: The serializable isolation level executes the transactions one after the other. Deadlock can be prevented by using the right isolation level. The default isolation level is usually read committed . If we are not using explicit transactions, then we are protected from the issue of dirty read. The transaction log offers protection against all kinds of data losses. In the redo transaction logging scheme, the transaction log starts with the commit mark.","title":"Questions to test your knowledge"},{"location":"seminar/ef/","text":"Entity Framework \u00b6 The goal of the seminar is to practice writing Linq queries and working with Entity Framework. Entity Framework Core In this seminar, we are using Entity Framework available in the .NET Framework and not the cross-platform Core version. The usage of Linq is very similar, if not identical in both technologies. The visual code generation technology, however, is only available in the .NET Framework and Entity Framework. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2015/2017/2019 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: C# language Entity Framework and Linq How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: Create a project and map the database \u00b6 Let us create a new C# console application in Visual Studio. In VS 2019 search for \"console framework\"; this will yield the project template we are looking for. Do not create a .NET Core application because it does not support the visual Database First code generation we will use. Mind the project language: it shall be C#. Create a new project; you may work in directory c:\\work . Add a new ADO.NET Entity Data Model to the project. In Solution Explorer right-click the project / Add / New Item / Data / ADO.NET Entity Data Model . At the bottom of the dialog, use the name DataDrivenEntities . The model shall be created based on an existing database: in the wizard, choose the \"EF designer from database\" option. Specify the connection to the database. Create a new connection and save the connection string into the app config file. Data source : Microsoft SQL Server Server name : (localdb)\\mssqllocaldb Select or enter database name : let us chose our database Save connection settings in App.Config : yes (check) use the name DataDrivenEntities (this will be the name of the DbContext class) Use Entity Framework 6.0 mapping. Map all tables: make a check next to the Tables folder. Model namespace : e.g. DataDrivenEntitiesModel Wait for the code generation. If VS asks about running a \"template\" let us allow it. Locate the connection string in app.config and check its contents. app.config It is advisable to put configuration, such as the connection string here, because the database access information may be different for each installation of an application. If the connection string is hard-wired into the source code, the application needs to be recompiled to change it. However, the app.config file is part of the application next to the executable is an editable xml format. Open the EF data model (double-click in the Solution Explorer). Let us examine the entities and the connections. The model can be changed using the Entity Data Model Browser and Entity Data Model Mapping Details windows (if not visible open them through View / Other windows ). Let us make a few corrections in the generated names: Customer.CustomerSite1 -> .Sites CustomerSite.Customer1 -> .MainCustomer Order.OrderItem -> .OrderItem\u200b s Product.OrderItem -> .OrderItem\u200b s VAT.Product -> .Product\u200b s Category.Product -> .Product\u200b s Save the model using Ctrl-S. Let us examine the C# source code of the DbContext any one of the entity classes. In Solution Explorer expand the EDM model file, and the C# source files will be listed underneath. You should not make changes to these files, as these are generated by the EDM model. Any change we make is lost when the EDM is re-generated. We may note, however, that all classes are declared as partial ; thus we can extend them by creating new source files and using the same namespace and class name. Exercise 2: Queries \u00b6 In the following exercises, write C# code using Linq to Entities. The results should be printed to console. You can check the SQL query generated in runtime: hover over the IQueryable variable once the iteration of the list is underway; it will show you the generated command. List the names and the amount of stock of all products that we have more than 30 in stock! List the products that have been ordered at least twice! List the orders that have a total value of at least 30.000! For each order, print the customer name and list all items of the order (with the product name, amount, and price). Find the most expensive product! List all customer record pairs that have their main site of business in the same city. Each pair should only be listed once. Solution Console . WriteLine ( \"***** Exercise two *****\" ); using ( var db = new DataDrivenEntities ()) { // 2.1 Console . WriteLine ( \"\\t2.1:\" ); var qProductStock = from p in db . Product where p . Stock > 30 select p ; foreach ( var p in qProductStock ) Console . WriteLine ( \"\\t\\tName={0}\\tStock={1}\" , p . Name , p . Stock ); // 2.2 Console . WriteLine ( \"\\t2.2:\" ); var qProductOrder = from p in db . Product where p . OrderItems . Count >= 2 select p ; foreach ( var p in qProductOrder ) Console . WriteLine ( \"\\t\\tName={0}\" , p . Name ); // 2.3 Console . WriteLine ( \"\\t2.3:\" ); var qOrderTotal = from o in db . Order where o . OrderItems . Sum ( oi => oi . Amount * oi . Price ) > 30000 select o ; foreach ( var o in qOrderTotal ) { Console . WriteLine ( \"\\t\\tName={0}\" , o . CustomerSite . MainCustomer . Name ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( \"\\t\\t\\tProduct={0}\\tPrice={1}\\tAmount={2}\" , oi . Product . Name , oi . Price , oi . Amount ); } // 2.3 alternative solution // A new namespace import is needed; copy the following line to the top of this file! // using System.Data.Entity; // Generates a single query and populates the Navigation Properties too Console . WriteLine ( \"\\tc 2.3 alternative solution:\" ); var qOrderTotal2 = from o in db . Order . Include ( o => o . OrderItems ) // or .Include(\"OrderItem\") . Include ( o => o . OrderItems . Select ( oi => oi . Product )) // or .Include(\"OrderItem.Product\") . Include ( o => o . CustomerSite ) // or .Include(\"CustomerSite\") . Include ( o => o . CustomerSite . MainCustomer ) // or .Include(\"CustomerSite.Customer\") where o . OrderItems . Sum ( oi => oi . Amount * oi . Price ) > 30000 select o ; foreach ( var o in qOrderTotal2 ) { Console . WriteLine ( \"\\t\\tName={0}\" , o . CustomerSite . MainCustomer . Name ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( \"\\t\\t\\tProduct={0}\\tPrice={1}\\tAmount={2}\" , oi . Product . Name , oi . Price , oi . Amount ); } // 2.4 Console . WriteLine ( \"\\t2.4:\" ); var qPriceMax = from p in db . Product where p . Price == db . Product . Max ( a => a . Price ) select p ; foreach ( var t in qPriceMax ) Console . WriteLine ( \"\\t\\tName={0}\\tPrice={1}\" , t . Name , t . Price ); // 2.5 Console . WriteLine ( \"\\t2.5:\" ); var qJoin = from s1 in db . CustomerSite join s2 in db . CustomerSite on s1 . City equals s2 . City where s1 . CustomerID > s2 . CustomerID select new { c1 = s1 . MainCustomer , c2 = s2 . MainCustomer }; foreach ( var v in qJoin ) Console . WriteLine ( \"\\t\\tCustomer 1={0}\\tCustomer 2={1}\" , v . c1 . Name , v . c2 . Name ); } Exercise 3: Data modification \u00b6 The DbContext can also be used to modify the database. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Solution Console . WriteLine ( \"***** Exercise three *****\" ); using ( var db = new DataDrivenEntities ()) { // 3.1 Console . WriteLine ( \"\\t3.1:\" ); var qProductsLego = from p in db . Product where p . Category . Name == \"LEGO\" select p ; Console . WriteLine ( \"\\tBefore change:\" ); foreach ( var p in qProductsLego ) { Console . WriteLine ( \"\\t\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); p . Price = 1.1 * p . Price ; } db . SaveChanges (); qProductsLego = from p in db . Product where p . Category . Name == \"LEGO\" select p ; Console . WriteLine ( \"\\tAfter change:\" ); foreach ( var p in qProductsLego ) Console . WriteLine ( \"\\t\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); // 3.2 Console . WriteLine ( \"\\t3.2:\" ); Category categoryExpensiveToys = ( from c in db . Category where c . Name == \"Expensive toys\" select c ). SingleOrDefault (); if ( categoryExpensiveToys == null ) { categoryExpensiveToys = new Category { Name = \"Expensive toys\" }; // The following line is not always necessary. If there is any product assigned to this // new category, when saving the changes, a new category record will automatically be // created. If we add this explicitly though, it better reflects our way of thinking, // and if there are no assigned products, the new category record is created regardless. db . Category . Add ( categoryExpensiveToys ); } var qProductExpensive = from p in db . Product where p . Price > 8000 select p ; foreach ( var p in qProductExpensive ) p . Category = categoryExpensiveToys ; db . SaveChanges (); qProductExpensive = from p in db . Product where p . Category . Name == \"Expensive toys\" select p ; foreach ( var t in qProductExpensive ) Console . WriteLine ( \"\\t\\tName={0}\\tPrice={1}\" , t . Name , t . Price ); } Exercise 4: Using stored procedures \u00b6 Stored procedures can be mapped into the EDM model either as a method on the DbContext or an entity's operation. Procedure mapping in the EDM The mapping properties of the stored procedure (e.g., the return value) usually has to be manually altered using the Entity Data Model Browser : find the Function Import and open its properties. Create a new stored procedure that can be used to register a new payment method. Map this stored procedure as the insert operation of the PaymentMethod entity! Create the stored procedure using SQL Management Studio. CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Set this procedure as the PaymentMethod entity insert operation. Add the stored procedure to the EDM. In the EDM Browser right-click to open the context menu and use \"Update model from database\" to Add the previously created procedure. Save the model changes to generate the required C# code in the background. Set this procedure on the PaymentMethod entity as the insert operation: select the PaymentMethod entity in the EDM and in the Mapping Details window change to the Map Entity to Functions tab, then chose the procedure for Insert operation. The return value shall be mapped to the ID property. Save the model. Test the behavior: in C# code, create a new PaymentMethod entity and add it to the appropriate list of the DbContext using Add . Verify the creation of the new record in the database. Create a stored procedure that lists all products that have been sold more than a specified amount of times. Call this method from C# code! Create the stored procedure with the T-SQL command below. CREATE PROCEDURE dbo . PopularProducts ( @ MinAmount int = 10 ) AS SELECT Product . * FROM Product INNER JOIN ( SELECT OrderItem . ProductID FROM OrderItem GROUP BY OrderItem . ProductID HAVING SUM ( OrderItem . Amount ) > @ MinAmount ) a ON Product . ID = a . ProductID Import the procedure into the EDM. Open the function properties (in the EDM Model Browser double click function ) and set return value as type Product . Save the model changes. Use the new method available on the DbContext class to call this method and print the returned product names to console! Solution Console . WriteLine ( \"***** Exercise four *****\" ); using ( var db = new DataDrivenEntities ()) { // 4.3 Console . WriteLine ( \"\\t4.3:\" ); var pm = new PaymentMethod { Method = \"Apple pay\" , Deadline = 99999 }; db . PaymentMethod . Add ( pm ); db . SaveChanges (); // 4.6 Console . WriteLine ( \"\\t4.6:\" ); var qPopularProducts = db . PopularProducts ( 5 ); foreach ( var p in qPopularProducts ) Console . WriteLine ( \"\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); }","title":"Entity Framework"},{"location":"seminar/ef/#entity-framework","text":"The goal of the seminar is to practice writing Linq queries and working with Entity Framework. Entity Framework Core In this seminar, we are using Entity Framework available in the .NET Framework and not the cross-platform Core version. The usage of Linq is very similar, if not identical in both technologies. The visual code generation technology, however, is only available in the .NET Framework and Entity Framework.","title":"Entity Framework"},{"location":"seminar/ef/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2015/2017/2019 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: C# language Entity Framework and Linq","title":"Pre-requisites"},{"location":"seminar/ef/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/ef/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/ef/#exercise-1-create-a-project-and-map-the-database","text":"Let us create a new C# console application in Visual Studio. In VS 2019 search for \"console framework\"; this will yield the project template we are looking for. Do not create a .NET Core application because it does not support the visual Database First code generation we will use. Mind the project language: it shall be C#. Create a new project; you may work in directory c:\\work . Add a new ADO.NET Entity Data Model to the project. In Solution Explorer right-click the project / Add / New Item / Data / ADO.NET Entity Data Model . At the bottom of the dialog, use the name DataDrivenEntities . The model shall be created based on an existing database: in the wizard, choose the \"EF designer from database\" option. Specify the connection to the database. Create a new connection and save the connection string into the app config file. Data source : Microsoft SQL Server Server name : (localdb)\\mssqllocaldb Select or enter database name : let us chose our database Save connection settings in App.Config : yes (check) use the name DataDrivenEntities (this will be the name of the DbContext class) Use Entity Framework 6.0 mapping. Map all tables: make a check next to the Tables folder. Model namespace : e.g. DataDrivenEntitiesModel Wait for the code generation. If VS asks about running a \"template\" let us allow it. Locate the connection string in app.config and check its contents. app.config It is advisable to put configuration, such as the connection string here, because the database access information may be different for each installation of an application. If the connection string is hard-wired into the source code, the application needs to be recompiled to change it. However, the app.config file is part of the application next to the executable is an editable xml format. Open the EF data model (double-click in the Solution Explorer). Let us examine the entities and the connections. The model can be changed using the Entity Data Model Browser and Entity Data Model Mapping Details windows (if not visible open them through View / Other windows ). Let us make a few corrections in the generated names: Customer.CustomerSite1 -> .Sites CustomerSite.Customer1 -> .MainCustomer Order.OrderItem -> .OrderItem\u200b s Product.OrderItem -> .OrderItem\u200b s VAT.Product -> .Product\u200b s Category.Product -> .Product\u200b s Save the model using Ctrl-S. Let us examine the C# source code of the DbContext any one of the entity classes. In Solution Explorer expand the EDM model file, and the C# source files will be listed underneath. You should not make changes to these files, as these are generated by the EDM model. Any change we make is lost when the EDM is re-generated. We may note, however, that all classes are declared as partial ; thus we can extend them by creating new source files and using the same namespace and class name.","title":"Exercise 1: Create a project and map the database"},{"location":"seminar/ef/#exercise-2-queries","text":"In the following exercises, write C# code using Linq to Entities. The results should be printed to console. You can check the SQL query generated in runtime: hover over the IQueryable variable once the iteration of the list is underway; it will show you the generated command. List the names and the amount of stock of all products that we have more than 30 in stock! List the products that have been ordered at least twice! List the orders that have a total value of at least 30.000! For each order, print the customer name and list all items of the order (with the product name, amount, and price). Find the most expensive product! List all customer record pairs that have their main site of business in the same city. Each pair should only be listed once. Solution Console . WriteLine ( \"***** Exercise two *****\" ); using ( var db = new DataDrivenEntities ()) { // 2.1 Console . WriteLine ( \"\\t2.1:\" ); var qProductStock = from p in db . Product where p . Stock > 30 select p ; foreach ( var p in qProductStock ) Console . WriteLine ( \"\\t\\tName={0}\\tStock={1}\" , p . Name , p . Stock ); // 2.2 Console . WriteLine ( \"\\t2.2:\" ); var qProductOrder = from p in db . Product where p . OrderItems . Count >= 2 select p ; foreach ( var p in qProductOrder ) Console . WriteLine ( \"\\t\\tName={0}\" , p . Name ); // 2.3 Console . WriteLine ( \"\\t2.3:\" ); var qOrderTotal = from o in db . Order where o . OrderItems . Sum ( oi => oi . Amount * oi . Price ) > 30000 select o ; foreach ( var o in qOrderTotal ) { Console . WriteLine ( \"\\t\\tName={0}\" , o . CustomerSite . MainCustomer . Name ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( \"\\t\\t\\tProduct={0}\\tPrice={1}\\tAmount={2}\" , oi . Product . Name , oi . Price , oi . Amount ); } // 2.3 alternative solution // A new namespace import is needed; copy the following line to the top of this file! // using System.Data.Entity; // Generates a single query and populates the Navigation Properties too Console . WriteLine ( \"\\tc 2.3 alternative solution:\" ); var qOrderTotal2 = from o in db . Order . Include ( o => o . OrderItems ) // or .Include(\"OrderItem\") . Include ( o => o . OrderItems . Select ( oi => oi . Product )) // or .Include(\"OrderItem.Product\") . Include ( o => o . CustomerSite ) // or .Include(\"CustomerSite\") . Include ( o => o . CustomerSite . MainCustomer ) // or .Include(\"CustomerSite.Customer\") where o . OrderItems . Sum ( oi => oi . Amount * oi . Price ) > 30000 select o ; foreach ( var o in qOrderTotal2 ) { Console . WriteLine ( \"\\t\\tName={0}\" , o . CustomerSite . MainCustomer . Name ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( \"\\t\\t\\tProduct={0}\\tPrice={1}\\tAmount={2}\" , oi . Product . Name , oi . Price , oi . Amount ); } // 2.4 Console . WriteLine ( \"\\t2.4:\" ); var qPriceMax = from p in db . Product where p . Price == db . Product . Max ( a => a . Price ) select p ; foreach ( var t in qPriceMax ) Console . WriteLine ( \"\\t\\tName={0}\\tPrice={1}\" , t . Name , t . Price ); // 2.5 Console . WriteLine ( \"\\t2.5:\" ); var qJoin = from s1 in db . CustomerSite join s2 in db . CustomerSite on s1 . City equals s2 . City where s1 . CustomerID > s2 . CustomerID select new { c1 = s1 . MainCustomer , c2 = s2 . MainCustomer }; foreach ( var v in qJoin ) Console . WriteLine ( \"\\t\\tCustomer 1={0}\\tCustomer 2={1}\" , v . c1 . Name , v . c2 . Name ); }","title":"Exercise 2: Queries"},{"location":"seminar/ef/#exercise-3-data-modification","text":"The DbContext can also be used to modify the database. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Solution Console . WriteLine ( \"***** Exercise three *****\" ); using ( var db = new DataDrivenEntities ()) { // 3.1 Console . WriteLine ( \"\\t3.1:\" ); var qProductsLego = from p in db . Product where p . Category . Name == \"LEGO\" select p ; Console . WriteLine ( \"\\tBefore change:\" ); foreach ( var p in qProductsLego ) { Console . WriteLine ( \"\\t\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); p . Price = 1.1 * p . Price ; } db . SaveChanges (); qProductsLego = from p in db . Product where p . Category . Name == \"LEGO\" select p ; Console . WriteLine ( \"\\tAfter change:\" ); foreach ( var p in qProductsLego ) Console . WriteLine ( \"\\t\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); // 3.2 Console . WriteLine ( \"\\t3.2:\" ); Category categoryExpensiveToys = ( from c in db . Category where c . Name == \"Expensive toys\" select c ). SingleOrDefault (); if ( categoryExpensiveToys == null ) { categoryExpensiveToys = new Category { Name = \"Expensive toys\" }; // The following line is not always necessary. If there is any product assigned to this // new category, when saving the changes, a new category record will automatically be // created. If we add this explicitly though, it better reflects our way of thinking, // and if there are no assigned products, the new category record is created regardless. db . Category . Add ( categoryExpensiveToys ); } var qProductExpensive = from p in db . Product where p . Price > 8000 select p ; foreach ( var p in qProductExpensive ) p . Category = categoryExpensiveToys ; db . SaveChanges (); qProductExpensive = from p in db . Product where p . Category . Name == \"Expensive toys\" select p ; foreach ( var t in qProductExpensive ) Console . WriteLine ( \"\\t\\tName={0}\\tPrice={1}\" , t . Name , t . Price ); }","title":"Exercise 3: Data modification"},{"location":"seminar/ef/#exercise-4-using-stored-procedures","text":"Stored procedures can be mapped into the EDM model either as a method on the DbContext or an entity's operation. Procedure mapping in the EDM The mapping properties of the stored procedure (e.g., the return value) usually has to be manually altered using the Entity Data Model Browser : find the Function Import and open its properties. Create a new stored procedure that can be used to register a new payment method. Map this stored procedure as the insert operation of the PaymentMethod entity! Create the stored procedure using SQL Management Studio. CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Set this procedure as the PaymentMethod entity insert operation. Add the stored procedure to the EDM. In the EDM Browser right-click to open the context menu and use \"Update model from database\" to Add the previously created procedure. Save the model changes to generate the required C# code in the background. Set this procedure on the PaymentMethod entity as the insert operation: select the PaymentMethod entity in the EDM and in the Mapping Details window change to the Map Entity to Functions tab, then chose the procedure for Insert operation. The return value shall be mapped to the ID property. Save the model. Test the behavior: in C# code, create a new PaymentMethod entity and add it to the appropriate list of the DbContext using Add . Verify the creation of the new record in the database. Create a stored procedure that lists all products that have been sold more than a specified amount of times. Call this method from C# code! Create the stored procedure with the T-SQL command below. CREATE PROCEDURE dbo . PopularProducts ( @ MinAmount int = 10 ) AS SELECT Product . * FROM Product INNER JOIN ( SELECT OrderItem . ProductID FROM OrderItem GROUP BY OrderItem . ProductID HAVING SUM ( OrderItem . Amount ) > @ MinAmount ) a ON Product . ID = a . ProductID Import the procedure into the EDM. Open the function properties (in the EDM Model Browser double click function ) and set return value as type Product . Save the model changes. Use the new method available on the DbContext class to call this method and print the returned product names to console! Solution Console . WriteLine ( \"***** Exercise four *****\" ); using ( var db = new DataDrivenEntities ()) { // 4.3 Console . WriteLine ( \"\\t4.3:\" ); var pm = new PaymentMethod { Method = \"Apple pay\" , Deadline = 99999 }; db . PaymentMethod . Add ( pm ); db . SaveChanges (); // 4.6 Console . WriteLine ( \"\\t4.6:\" ); var qPopularProducts = db . PopularProducts ( 5 ); foreach ( var p in qPopularProducts ) Console . WriteLine ( \"\\t\\tName={0}\\tStock={1}\\tPrice={2}\" , p . Name , p . Stock , p . Price ); }","title":"Exercise 4: Using stored procedures"},{"location":"seminar/jpa/","text":"JPA & Spring Data \u00b6 To goal of this seminar is to practice working with JPA and Spring Data. Main topics of focus: working with entities, querying the database with various techniques, updating the database. The code is integrated into a skeleton web application with a UI for testing. Pre-requisites \u00b6 Required tools to complete the tasks: Eclipse for Java EE Microsoft SQL Server Express edition (localdb does not work here) SQL Server Management Studio Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo Recommended to review: JPA lecture EJB, Spring lecture How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Tips for using Eclipse \u00b6 Search Type(class, interface, enum): Ctrl+Shift+T (instead of opening folders in Project explorer) Search file: Ctrl+Shift+R Fix missing imports:Ctrl+Shift+O Format code: Ctrl+Shift+F In Java Resources right-click a package / New Class/Interfaces will create the source in this package Restore default layout of views: Window > Reset perspective Increase font size: Window menu / Preferences, start typing font to locate Fonts and Colors Select it and under Basic choose Text Font and increase the size Exercise 0: Create a database \u00b6 Use Microsoft SQL Server Management Studio to connect to the database. We are not using localdb here; the address is: localhost\\sqlexpress and use SQL Server Authentication . Create a new database with the name adatvez . You should this exact name or will have to update the Java project . To create a new database see the instructions in the first seminar material . If a database with this name already exists, no need to re-create it. Run the database initialization script on this database. If the database exists on this machine, run the script anyway to reset any changes made in the schema. Exercise 1: Start Eclipse \u00b6 Start Eclipse from here: C:\\work\\javaee\\eclipse\\eclipse.exe . (There might be a D:\\eclipse folder too, but do not use that one.) It will ask for a workspace, select: C:\\work\\javaee\\workspaces\\adatvez If there is a webshop project in the Project Explorer already, delete it: right-click the project / Delete , and check Delete project contents on disk Exercise 2: Import project \u00b6 Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo.git Import the downloaded project into the workspace: Open File / Import... Start typing Existing projects into workspace and choose it Locate the downloaded webshop project (the webshop folder in the checked-out repository), OK, check the webshop project in the dialog Finish Overview of the projects It is a maven based project. Maven is a command-line build tool that can be integrated with IDEs as well. It can download the libraries our projects depend on from public repositories. After opening the pom.xml file, the maven project's config file, you can see some dependency tags that will transitively download Hibernate, our JPA implementation, Spring Boot, Spring Data, Spring MVC and Thymeleaf. The application.properties file contains some basic settings. Let us verify the user name and password for the DB access here. The JNDI name of the database is set up via this line: spring.datasource.jndi-name=jdbc/termekDB . In classic Java EE web applications, this name should be defined in the persistence.xml , but Spring Boot supports XML-less configuration. The ConnectionProperties class is a Java representation of the user name/password pair in the previous config file. WebshopApplication is the entry point and configuration of the Spring Boot application. A traditional web application should be deployed to a web container (e.g., Tomcat, Jetty) running in a separate process. In the case of Spring Boot, however, Spring Boot itself will start an embedded web container (Tomcat, by default). The JDBC driver and JDBC URL is registered in the tomcatFactory method with the jdbc/ProductDB JNDI name so that JPA can find it. If the database name was changed, edit the JDBC URL: resource.setProperty(\"url\", \"jdbc:sqlserver://localhost;database= adatvez \"); The web interface is one page: src\\main\\resources\\templates\\testPage.html . We will not modify it. It contains standard HTML and some Thymeleaf attributes. WebshopController : the controller class implementing the web layer (its methods handle the HTTP requests). These methods typically call a query implemented in a repository or a service method and put the result into the model with a name that we can reference via Thymeleaf. You should call the methods implementing the tasks at the //TODO comments. Exercise 3: Overview of the entities \u00b6 The entities can be found in the hu.bme.aut.adatvez.webshop.model package. We could have written them by hand, but in this case, they were generated from the DB tables via the JPA plugin of Eclipse. Open an entity class, e.g., Vat, and check the JPA-related code. You can see the @Entity , @Id annotations, and @OneToMany or @ManyToOne for defining relationships. Exercise 4: Queries \u00b6 Implement the following queries on the data model. In JPA and Spring Data, you can write queries by different means. In the following tasks, we specify how to write the query so that multiple ways can be demonstrated. It is important to note that each task could be written using any technology and style. The requirements are provided for demonstrating all technologies. The methods implementing the queries should always be called in the WebshopController class, at the corresponding //TODO comment, run the application and test the query from a browser at address http://localhost:9080 . a) List the names and stock of those products of which we have more than 30 pieces in stock! Method: Spring Data repository interface with method name-derived query. b) Write a query that lists those products that were ordered at least twice! Method: JPQL query created with an injected EntityManager in a Spring Data custom repository implementation. c) List the data of the most expensive product! Method: Named query, called from Spring Data repository or with an injected EntityManager. When running the application, the SQL statements generated by Hibernate can be observed in the Console view of Eclipse, due to this config line in application.properties: spring.jpa.show-sql=true Running the application \u00b6 The project contains the configuration file webshop run.launch . Right-click on it > Debug As > webshop run. This starts the Spring Boot maven plugin in debug mode, which starts an embedded web container, and the application is available at http://localhost:9080 from a browser. Having done this once, we can do it more easily: Click on the Debug icon on the toolbar, and you will see the webshop run there. If under the Debug icon you find webshop run , the method above is unnecessary. The running application can be stopped with the red Terminate icon in the Console view. If we run the application twice, without terminating the first run, the second run will report a port collision on the port 9080 and stop. This second execution will be visible in the Console view, and the Terminate command will be inactive, as this copy has been terminated already. Click on the gray double C icon next to Terminate to close this view, and only the active running process will be visible. If we close the Console view by mistake, use shortcut Alt+Shift+Q, C or menu Window / Show View / Console to reopen. After shutdown, we can re-run using F11. When running the application in debug mode, the modifications in HTML files and some Java ode modifications are immediately actualized, so we only have to refresh the browser to see the effect of the code modification. But the application has to be restarted if we modify the Java code in either of the following ways adding a new type adding/removing/modifying an annotation adding a new class-or member variable, or method we changed the signature of a method. Simply put, when modifying code that is not inside of an existing method, a restart will be needed. Solution 4.a exercise In the dao package open ProductRepository interface that implements the Spring Data JpaRepository . There are a few methods for other exercises. Some define a @Query annotation and the query as text, some work without such annotation. We will not need the @Query annotation, but rather have Spring Data infer the SQL instruction from the method name as follows: package hu.bme.aut.adatvez.webshop.dao ; import java.math.BigDecimal ; import java.util.List ; import hu.bme.aut.adatvez.webshop.model.Product ; import org.springframework.data.jpa.repository.JpaRepository ; public interface ProductRepository extends JpaRepository < Product , Long > , ProductRepositoryCustom { ... List < Product > findByStockGreaterThan ( BigDecimal limit ); } WebshopController already contains an injected ProductRepository ; let us call this method at TODO 4.a: @Controller public class WebshopController { @Autowired ProductRepository productRepository ; //... // 4.a private List < Product > findProductsOver30 () { return productRepository . findByStockGreaterThan ( BigDecimal . valueOf ( 30 )); } } 4.b exercise In the dao package find ProductRepositoryCustom interface add a new method findProductsOrderedAtLeastTwice : package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; public interface ProductRepositoryCustom { List < Product > findProductsOrderedAtLeastTwice (); } The implementation class ProductRepositoryImpl will contain an error now as it does not implement ProductRepositoryCustom . Let us open this class, and line of the class declaration there will be a light bulb we can click to generate the method skeleton: Add the method's implementation as follows: use the injected EntityManager to create and run the query. package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; import javax.persistence.EntityManager ; import javax.persistence.PersistenceContext ; public class ProductRepositoryImpl implements ProductRepositoryCustom { @PersistenceContext EntityManager em ; @Override public List < Product > findProductsOrderedAtLeastTwice (){ return em . createQuery ( \"SELECT DISTINCT p FROM Product p LEFT JOIN FETCH p.orderitems WHERE size(p.orderitems) >= :itemsMin\" , Product . class ) . setParameter ( \"itemsMin\" , 2 ) . getResultList (); } } Note: we might try this command: SELECT p FROM Product p WHERE size(p.orderitems) /= :itemsMin , which will yield an org.hibernate.LazyInitializationException error, hence the LEFT JOIN FETCH above. Call this in WebshopController : // 4.b private List < Product > findProductsOrderedAtLeastTwice () { // TODO return productRepository . findProductsOrderedAtLeastTwice (); } 4.c exercise Open entity class Product where we can find a few named querys; we need the second one: @NamedQueries ({ @NamedQuery ( name = \"Product.findAll\" , query = \"SELECT p FROM Product p\" ), @NamedQuery ( name = \"Product.findMostExpensive\" , query = \"SELECT p FROM Product p WHERE p.price IN (SELECT MAX(p2.price) FROM Product p2)\" ) }) This named query can be called in two ways. The first is to create a method in ProductRepository with the same name (without the Product. prefix.), that is: public List < Product > findMostExpensive (); The second option is to execute it manually in ProductRepositoryImpl using EntityManager : @Override public List < Product > findMostExpensiveProducts (){ return em . createNamedQuery ( \"Product.findMostExpensive\" , Product . class ). getResultList (); } This method also needs to be added to the ProductRepositoryCustom interface. E.g. right-click / Refactor / Pull up Finally, call the method in WebshopController : // 4.c private List < Product > findMostExpensiveProducts () { // TODO // return productRepository.findMostExpensiveProducts(); return productRepository . findMostExpensive (); } Exercise 5: Data modification \u00b6 JPA can also be used to modify the database content. a) Write a JPQL query into the ProductRepository interface that raises the price of \"Building items\" by 10 percent! b) Write a method that creates a new category called \"Expensive toys\", if it does not exist yet, and move all the products with a price higher than 8000 into this category! c) Simple individual task: create a CategoryRepository interface, and implement a method name-derived query that you can use in task 5.b) instead of the query created with the injected EntityManager. Solution 5.a exercise Crate an UPDATE query in ProductRepository interface. We have to denote that this is a @Modifying query, and also add @Transactional (from package org.springframework...`): @Modifying @Transactional @Query ( \"UPDATE Product p SET p.price=p.price*1.1 WHERE p.id IN (SELECT p2.id FROM Product p2 WHERE p2.category.name=:categoryName)\" ) void categoryRaisePrice ( @Param ( \"categoryName\" ) String categoryName ); Call in WebshopController : // 5.a @RequestMapping ( value = \"/raisePriceOfBuildingItems\" , method = { RequestMethod . POST , RequestMethod . GET }) private String raisePriceOfBuildingItems () { // TODO productRepository . categoryRaisePrice ( \"Building items\" ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.b exercise In the dao package add a new class CategoryService with a @Service annotation with a @Transactional method: @Service public class CategoryService { @PersistenceContext private EntityManager em ; @Autowired ProductRepository productRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ String name = \"Expensive toys\" ; Category categoryExpensive = null ; List < Category > resultList = em . createQuery ( \"SELECT c from Category c WHERE c.name=:name\" , Category . class ) . setParameter ( \"name\" , name ) . getResultList (); if ( resultList . isEmpty ()){ // 0 or null id triggers @GeneratedValue; this is a scalar, hence use 0 categoryExpensive = new Category ( 0 , name ); em . persist ( categoryExpensive ); } else { categoryExpensive = resultList . get ( 0 ); } List < Product > expensiveProducts = productRepository . findByPriceGreaterThan ( priceLimit ); for ( Product product : expensiveProducts ) { categoryExpensive . addProduct ( product ); } } } Let us note that the managed entities (fetched through queries within the transaction, or added as new with persist) need no explicit save; the transaction saves them to DB automatically. Call in WebshopController : @Autowired CategoryService categoryService ; ... // 5.b @RequestMapping ( value = \"/moveToExpensiveToys\" , method = { RequestMethod . POST , RequestMethod . GET }) private String moveToExpensiveToys () { // TODO categoryService . moveToExpensiveToys ( 8000.0 ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.c exercise In dao package add a new interface CategoryRepository , similar to ProductRepository (without the Custom inheritance) with one method: public interface CategoryRepository extends JpaRepository < Category , Long > { List < Category > findByName ( String name ); } This simplifies the CategoryService as follows: @Service public class CategoryService { ... @Autowired CategoryRepository categoryRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ // ... List < Category > resultList = categoryRepository . findByName ( name ); // ... } } Exercise 6: Using stored procedures \u00b6 Use the CreatePaymentMethod stored procedure to create a new Paymentmethod ! Check in SQL Server Management Studio, whether the database contains the stored procedure with the name CreatePaymentMethod ! If not, create the procedure with the code below! CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Solution The PaymentMethod entity has the following annotation. Compare it to the stored procedure code! @NamedStoredProcedureQueries ({ @NamedStoredProcedureQuery ( name = \"createMethodSP\" , procedureName = \"CreateNewPaymentMethod\" , parameters = { @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Method\" , type = String . class ), @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Deadline\" , type = BigDecimal . class ) }) }) public class Paymentmethod implements Serializable { ... The named stored procedure query can be called from a Spring Data repository ( dao package New Interface ... / PaymentmethodRepository ): public interface PaymentmethodRepository extends JpaRepository < Paymentmethod , Long > { @Procedure ( name = \"createMethodSP\" ) void newMethod ( @Param ( \"Method\" ) String method , @Param ( \"Deadline\" ) BigDecimal deadline ); } Without Spring Data we could use EntityManager : @Service public class PaymentmethodService { @PersistenceContext private EntityManager em ; public void createNewMethod ( Paymentmethod paymentMethod ){ StoredProcedureQuery sp = em . createNamedStoredProcedureQuery ( \"createMethodSP\" ); sp . setParameter ( \"Method\" , paymentMethod . getMethod ()); sp . setParameter ( \"Deadline\" , paymentMethod . getDeadline ()); sp . execute (); } } Call from the web layer: Inject into WebshopController the PaymentmethodRepository interface: @Autowired PaymentmethodRepository paymentmethodRepository ; Call the method from WebshopController at the last TODO paymentmethodRepository . newMethod ( paymentMethod . getMethod (), paymentMethod . getDeadline ());","title":"JPA & Spring Data"},{"location":"seminar/jpa/#jpa-spring-data","text":"To goal of this seminar is to practice working with JPA and Spring Data. Main topics of focus: working with entities, querying the database with various techniques, updating the database. The code is integrated into a skeleton web application with a UI for testing.","title":"JPA &amp; Spring Data"},{"location":"seminar/jpa/#pre-requisites","text":"Required tools to complete the tasks: Eclipse for Java EE Microsoft SQL Server Express edition (localdb does not work here) SQL Server Management Studio Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo Recommended to review: JPA lecture EJB, Spring lecture","title":"Pre-requisites"},{"location":"seminar/jpa/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/jpa/#tips-for-using-eclipse","text":"Search Type(class, interface, enum): Ctrl+Shift+T (instead of opening folders in Project explorer) Search file: Ctrl+Shift+R Fix missing imports:Ctrl+Shift+O Format code: Ctrl+Shift+F In Java Resources right-click a package / New Class/Interfaces will create the source in this package Restore default layout of views: Window > Reset perspective Increase font size: Window menu / Preferences, start typing font to locate Fonts and Colors Select it and under Basic choose Text Font and increase the size","title":"Tips for using Eclipse"},{"location":"seminar/jpa/#exercise-0-create-a-database","text":"Use Microsoft SQL Server Management Studio to connect to the database. We are not using localdb here; the address is: localhost\\sqlexpress and use SQL Server Authentication . Create a new database with the name adatvez . You should this exact name or will have to update the Java project . To create a new database see the instructions in the first seminar material . If a database with this name already exists, no need to re-create it. Run the database initialization script on this database. If the database exists on this machine, run the script anyway to reset any changes made in the schema.","title":"Exercise 0: Create a database"},{"location":"seminar/jpa/#exercise-1-start-eclipse","text":"Start Eclipse from here: C:\\work\\javaee\\eclipse\\eclipse.exe . (There might be a D:\\eclipse folder too, but do not use that one.) It will ask for a workspace, select: C:\\work\\javaee\\workspaces\\adatvez If there is a webshop project in the Project Explorer already, delete it: right-click the project / Delete , and check Delete project contents on disk","title":"Exercise 1: Start Eclipse"},{"location":"seminar/jpa/#exercise-2-import-project","text":"Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo.git Import the downloaded project into the workspace: Open File / Import... Start typing Existing projects into workspace and choose it Locate the downloaded webshop project (the webshop folder in the checked-out repository), OK, check the webshop project in the dialog Finish Overview of the projects It is a maven based project. Maven is a command-line build tool that can be integrated with IDEs as well. It can download the libraries our projects depend on from public repositories. After opening the pom.xml file, the maven project's config file, you can see some dependency tags that will transitively download Hibernate, our JPA implementation, Spring Boot, Spring Data, Spring MVC and Thymeleaf. The application.properties file contains some basic settings. Let us verify the user name and password for the DB access here. The JNDI name of the database is set up via this line: spring.datasource.jndi-name=jdbc/termekDB . In classic Java EE web applications, this name should be defined in the persistence.xml , but Spring Boot supports XML-less configuration. The ConnectionProperties class is a Java representation of the user name/password pair in the previous config file. WebshopApplication is the entry point and configuration of the Spring Boot application. A traditional web application should be deployed to a web container (e.g., Tomcat, Jetty) running in a separate process. In the case of Spring Boot, however, Spring Boot itself will start an embedded web container (Tomcat, by default). The JDBC driver and JDBC URL is registered in the tomcatFactory method with the jdbc/ProductDB JNDI name so that JPA can find it. If the database name was changed, edit the JDBC URL: resource.setProperty(\"url\", \"jdbc:sqlserver://localhost;database= adatvez \"); The web interface is one page: src\\main\\resources\\templates\\testPage.html . We will not modify it. It contains standard HTML and some Thymeleaf attributes. WebshopController : the controller class implementing the web layer (its methods handle the HTTP requests). These methods typically call a query implemented in a repository or a service method and put the result into the model with a name that we can reference via Thymeleaf. You should call the methods implementing the tasks at the //TODO comments.","title":"Exercise 2: Import project"},{"location":"seminar/jpa/#exercise-3-overview-of-the-entities","text":"The entities can be found in the hu.bme.aut.adatvez.webshop.model package. We could have written them by hand, but in this case, they were generated from the DB tables via the JPA plugin of Eclipse. Open an entity class, e.g., Vat, and check the JPA-related code. You can see the @Entity , @Id annotations, and @OneToMany or @ManyToOne for defining relationships.","title":"Exercise 3: Overview of the entities"},{"location":"seminar/jpa/#exercise-4-queries","text":"Implement the following queries on the data model. In JPA and Spring Data, you can write queries by different means. In the following tasks, we specify how to write the query so that multiple ways can be demonstrated. It is important to note that each task could be written using any technology and style. The requirements are provided for demonstrating all technologies. The methods implementing the queries should always be called in the WebshopController class, at the corresponding //TODO comment, run the application and test the query from a browser at address http://localhost:9080 . a) List the names and stock of those products of which we have more than 30 pieces in stock! Method: Spring Data repository interface with method name-derived query. b) Write a query that lists those products that were ordered at least twice! Method: JPQL query created with an injected EntityManager in a Spring Data custom repository implementation. c) List the data of the most expensive product! Method: Named query, called from Spring Data repository or with an injected EntityManager. When running the application, the SQL statements generated by Hibernate can be observed in the Console view of Eclipse, due to this config line in application.properties: spring.jpa.show-sql=true","title":"Exercise 4: Queries"},{"location":"seminar/jpa/#running-the-application","text":"The project contains the configuration file webshop run.launch . Right-click on it > Debug As > webshop run. This starts the Spring Boot maven plugin in debug mode, which starts an embedded web container, and the application is available at http://localhost:9080 from a browser. Having done this once, we can do it more easily: Click on the Debug icon on the toolbar, and you will see the webshop run there. If under the Debug icon you find webshop run , the method above is unnecessary. The running application can be stopped with the red Terminate icon in the Console view. If we run the application twice, without terminating the first run, the second run will report a port collision on the port 9080 and stop. This second execution will be visible in the Console view, and the Terminate command will be inactive, as this copy has been terminated already. Click on the gray double C icon next to Terminate to close this view, and only the active running process will be visible. If we close the Console view by mistake, use shortcut Alt+Shift+Q, C or menu Window / Show View / Console to reopen. After shutdown, we can re-run using F11. When running the application in debug mode, the modifications in HTML files and some Java ode modifications are immediately actualized, so we only have to refresh the browser to see the effect of the code modification. But the application has to be restarted if we modify the Java code in either of the following ways adding a new type adding/removing/modifying an annotation adding a new class-or member variable, or method we changed the signature of a method. Simply put, when modifying code that is not inside of an existing method, a restart will be needed. Solution 4.a exercise In the dao package open ProductRepository interface that implements the Spring Data JpaRepository . There are a few methods for other exercises. Some define a @Query annotation and the query as text, some work without such annotation. We will not need the @Query annotation, but rather have Spring Data infer the SQL instruction from the method name as follows: package hu.bme.aut.adatvez.webshop.dao ; import java.math.BigDecimal ; import java.util.List ; import hu.bme.aut.adatvez.webshop.model.Product ; import org.springframework.data.jpa.repository.JpaRepository ; public interface ProductRepository extends JpaRepository < Product , Long > , ProductRepositoryCustom { ... List < Product > findByStockGreaterThan ( BigDecimal limit ); } WebshopController already contains an injected ProductRepository ; let us call this method at TODO 4.a: @Controller public class WebshopController { @Autowired ProductRepository productRepository ; //... // 4.a private List < Product > findProductsOver30 () { return productRepository . findByStockGreaterThan ( BigDecimal . valueOf ( 30 )); } } 4.b exercise In the dao package find ProductRepositoryCustom interface add a new method findProductsOrderedAtLeastTwice : package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; public interface ProductRepositoryCustom { List < Product > findProductsOrderedAtLeastTwice (); } The implementation class ProductRepositoryImpl will contain an error now as it does not implement ProductRepositoryCustom . Let us open this class, and line of the class declaration there will be a light bulb we can click to generate the method skeleton: Add the method's implementation as follows: use the injected EntityManager to create and run the query. package hu.bme.aut.adatvez.webshop.dao ; import hu.bme.aut.adatvez.webshop.model.Product ; import java.util.List ; import javax.persistence.EntityManager ; import javax.persistence.PersistenceContext ; public class ProductRepositoryImpl implements ProductRepositoryCustom { @PersistenceContext EntityManager em ; @Override public List < Product > findProductsOrderedAtLeastTwice (){ return em . createQuery ( \"SELECT DISTINCT p FROM Product p LEFT JOIN FETCH p.orderitems WHERE size(p.orderitems) >= :itemsMin\" , Product . class ) . setParameter ( \"itemsMin\" , 2 ) . getResultList (); } } Note: we might try this command: SELECT p FROM Product p WHERE size(p.orderitems) /= :itemsMin , which will yield an org.hibernate.LazyInitializationException error, hence the LEFT JOIN FETCH above. Call this in WebshopController : // 4.b private List < Product > findProductsOrderedAtLeastTwice () { // TODO return productRepository . findProductsOrderedAtLeastTwice (); } 4.c exercise Open entity class Product where we can find a few named querys; we need the second one: @NamedQueries ({ @NamedQuery ( name = \"Product.findAll\" , query = \"SELECT p FROM Product p\" ), @NamedQuery ( name = \"Product.findMostExpensive\" , query = \"SELECT p FROM Product p WHERE p.price IN (SELECT MAX(p2.price) FROM Product p2)\" ) }) This named query can be called in two ways. The first is to create a method in ProductRepository with the same name (without the Product. prefix.), that is: public List < Product > findMostExpensive (); The second option is to execute it manually in ProductRepositoryImpl using EntityManager : @Override public List < Product > findMostExpensiveProducts (){ return em . createNamedQuery ( \"Product.findMostExpensive\" , Product . class ). getResultList (); } This method also needs to be added to the ProductRepositoryCustom interface. E.g. right-click / Refactor / Pull up Finally, call the method in WebshopController : // 4.c private List < Product > findMostExpensiveProducts () { // TODO // return productRepository.findMostExpensiveProducts(); return productRepository . findMostExpensive (); }","title":"Running the application"},{"location":"seminar/jpa/#exercise-5-data-modification","text":"JPA can also be used to modify the database content. a) Write a JPQL query into the ProductRepository interface that raises the price of \"Building items\" by 10 percent! b) Write a method that creates a new category called \"Expensive toys\", if it does not exist yet, and move all the products with a price higher than 8000 into this category! c) Simple individual task: create a CategoryRepository interface, and implement a method name-derived query that you can use in task 5.b) instead of the query created with the injected EntityManager. Solution 5.a exercise Crate an UPDATE query in ProductRepository interface. We have to denote that this is a @Modifying query, and also add @Transactional (from package org.springframework...`): @Modifying @Transactional @Query ( \"UPDATE Product p SET p.price=p.price*1.1 WHERE p.id IN (SELECT p2.id FROM Product p2 WHERE p2.category.name=:categoryName)\" ) void categoryRaisePrice ( @Param ( \"categoryName\" ) String categoryName ); Call in WebshopController : // 5.a @RequestMapping ( value = \"/raisePriceOfBuildingItems\" , method = { RequestMethod . POST , RequestMethod . GET }) private String raisePriceOfBuildingItems () { // TODO productRepository . categoryRaisePrice ( \"Building items\" ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.b exercise In the dao package add a new class CategoryService with a @Service annotation with a @Transactional method: @Service public class CategoryService { @PersistenceContext private EntityManager em ; @Autowired ProductRepository productRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ String name = \"Expensive toys\" ; Category categoryExpensive = null ; List < Category > resultList = em . createQuery ( \"SELECT c from Category c WHERE c.name=:name\" , Category . class ) . setParameter ( \"name\" , name ) . getResultList (); if ( resultList . isEmpty ()){ // 0 or null id triggers @GeneratedValue; this is a scalar, hence use 0 categoryExpensive = new Category ( 0 , name ); em . persist ( categoryExpensive ); } else { categoryExpensive = resultList . get ( 0 ); } List < Product > expensiveProducts = productRepository . findByPriceGreaterThan ( priceLimit ); for ( Product product : expensiveProducts ) { categoryExpensive . addProduct ( product ); } } } Let us note that the managed entities (fetched through queries within the transaction, or added as new with persist) need no explicit save; the transaction saves them to DB automatically. Call in WebshopController : @Autowired CategoryService categoryService ; ... // 5.b @RequestMapping ( value = \"/moveToExpensiveToys\" , method = { RequestMethod . POST , RequestMethod . GET }) private String moveToExpensiveToys () { // TODO categoryService . moveToExpensiveToys ( 8000.0 ); return \"redirect:/\" ; } In the browser, the changes are visible after clicking the button. 5.c exercise In dao package add a new interface CategoryRepository , similar to ProductRepository (without the Custom inheritance) with one method: public interface CategoryRepository extends JpaRepository < Category , Long > { List < Category > findByName ( String name ); } This simplifies the CategoryService as follows: @Service public class CategoryService { ... @Autowired CategoryRepository categoryRepository ; @Transactional public void moveToExpensiveToys ( double priceLimit ){ // ... List < Category > resultList = categoryRepository . findByName ( name ); // ... } }","title":"Exercise 5: Data modification"},{"location":"seminar/jpa/#exercise-6-using-stored-procedures","text":"Use the CreatePaymentMethod stored procedure to create a new Paymentmethod ! Check in SQL Server Management Studio, whether the database contains the stored procedure with the name CreatePaymentMethod ! If not, create the procedure with the code below! CREATE PROCEDURE CreateNewPaymentMethod ( @ Method nvarchar ( 20 ), @ Deadline int ) AS insert into PaymentMethod values ( @ Method , @ Deadline ) select scope_identity () as NewId Solution The PaymentMethod entity has the following annotation. Compare it to the stored procedure code! @NamedStoredProcedureQueries ({ @NamedStoredProcedureQuery ( name = \"createMethodSP\" , procedureName = \"CreateNewPaymentMethod\" , parameters = { @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Method\" , type = String . class ), @StoredProcedureParameter ( mode = ParameterMode . IN , name = \"Deadline\" , type = BigDecimal . class ) }) }) public class Paymentmethod implements Serializable { ... The named stored procedure query can be called from a Spring Data repository ( dao package New Interface ... / PaymentmethodRepository ): public interface PaymentmethodRepository extends JpaRepository < Paymentmethod , Long > { @Procedure ( name = \"createMethodSP\" ) void newMethod ( @Param ( \"Method\" ) String method , @Param ( \"Deadline\" ) BigDecimal deadline ); } Without Spring Data we could use EntityManager : @Service public class PaymentmethodService { @PersistenceContext private EntityManager em ; public void createNewMethod ( Paymentmethod paymentMethod ){ StoredProcedureQuery sp = em . createNamedStoredProcedureQuery ( \"createMethodSP\" ); sp . setParameter ( \"Method\" , paymentMethod . getMethod ()); sp . setParameter ( \"Deadline\" , paymentMethod . getDeadline ()); sp . execute (); } } Call from the web layer: Inject into WebshopController the PaymentmethodRepository interface: @Autowired PaymentmethodRepository paymentmethodRepository ; Call the method from WebshopController at the last TODO paymentmethodRepository . newMethod ( paymentMethod . getMethod (), paymentMethod . getDeadline ());","title":"Exercise 6: Using stored procedures"},{"location":"seminar/mongodb/","text":"MongoDB \u00b6 The seminar's goal is to understand the concepts of the MongoDB document database and the usage of the MongoDB C#/.NET Driver . Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2019 ( not VS Code) MongoDB Community Edition Robo 3T Database initialization script: mongo.js Starter code: https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo Recommended to review: C# language and Linq queries MongoDB lecture Using MongoDB guide How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create database, open starter code \u00b6 Create a new working directory, e.g., c:\\work\\NEPTUN . Working directory This folder will be our working directory , that is, everything is created within this folder. Start the MongoDB server. Create a new folder for the database files, e.g., use name db . (Create this within the working directory.) Open a command prompt and start MongoDB server: mongod.exe --dbpath=\"c:\\work\\<NEPTUN>\\db\" Command prompt can be located in the start menu by typing \"cmd\". In the university computer laboratories, MongoDB is in folder c:\\tools\\mongodb\\bin . Navigate to this directory using the cd command. Keep this command prompt open because it hosts our server. To stop the server at the end of the seminar, press Ctrl+C. Create the database. Download the database initialization script and save it as mongo.js in the working directory. Open a new command prompt and initialize database: mongo.exe localhost:27017/datadriven c:\\work\\<NEPTUN>\\mongo.js The executable is not mongo\u200b d but simply mongo. This is a client software to connect to the server and execute queries. In the university computer laboratories, this executable is located at c:\\tools\\mongodb\\bin . Let us note the connection string that contains the name of the database too! Check the database using Robo3T. Start Robo3T (in the university computer laboratories, it is located at c:\\tools\\robo3t ) and connect to the server. Check if the Collections were created. Download the starter code! Open yet another new command prompt into our working directory. Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo.git Open the sln file from the newly created folder using Visual Studio. Let us examine this project. This is a .NET Core console application. The structure resembles the structure of the Entity Framework project seen before: directory Entities contains the database entities while our code will be written to Program.cs . Program.cs already contains the initialization of the connection to MongoDB. Interface IMongoClient is used for all communication with the database. We will not use this directly. Interface IMongoDatabase represents the database datadriven within the MongoDB server. And the IMongoCollection<TEntity> interfaces represent the specific collections we can use to execute queries and modification commands. The database documents are mapped to the C# entity classes in folder Entities . A major difference compared to the behavior previously seen in Entity Framework is that these classes were not generated by written manually. Most entities are already mapped. We will create one more class during an exercise. Exercise 1: Queries \u00b6 Write C# code using the MongoDB C#/.NET Driver in the following exercises. Print the results to the console. List the names and the amount of stock of all products that we have more than 30 in stock! List the orders that consist of at least two items! List the orders that have a total value of at least 30.000! For each order, print the customer name, and list all items of the order (with the product name, amount, and price). Find the most expensive product! List the products that have been ordered at least twice! Solution We need only the product collection and execute a simple query. The filter criteria can be written as a Lambda-expression and with the builder syntax too. Console . WriteLine ( \"***** Exercise one *****\" ); // 1.1 first solution Console . WriteLine ( \"\\t1.1 First solution:\" ); var qProductAndStock1 = productsCollection . Find ( p => p . Stock > 30 ) . ToList (); foreach ( var p in qProductAndStock1 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); // 1.1 second solution Console . WriteLine ( \"\\t1.1 Second solution:\" ); var qProductAndStock2 = productsCollection . Find ( Builders < Product >. Filter . Gt ( p => p . Stock , 30 )) . ToList (); foreach ( var p in qProductAndStock2 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); This is similar to the previous one. We may note that we would have needed a join in a relational database, but we have everything at hand here. // 1.2 first solution Console . WriteLine ( \"\\t1.2 First solution:\" ); var qOrderItems1 = ordersCollection . Find ( o => o . OrderItems . Length >= 2 ) . ToList (); foreach ( var o in qOrderItems1 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); // 1.2 second solution Console . WriteLine ( \"\\t1.2 Second solution:\" ); var qOrderItems2 = ordersCollection . Find ( Builders < Order >. Filter . SizeGte ( o => o . OrderItems , 2 )) . ToList (); foreach ( var o in qOrderItems2 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); A simple query is not sufficient for this exercise; thus, we need the aggregation pipeline. We may still note that every information we need is still available in one collection. // 1.3 Console . WriteLine ( \"\\t1.3:\" ); var qOrderTotal = ordersCollection . Aggregate () . Project ( order => new { CustomerID = order . CustomerID , OrderItems = order . OrderItems , Total = order . OrderItems . Sum ( oi => oi . Amount * oi . Price ) }) . Match ( order => order . Total > 30000 ) . ToList (); foreach ( var o in qOrderTotal ) { Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\" ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( $\"\\t\\t\\tProductID={oi.ProductID}\\tPrice={oi.Price}\\tAmount={oi.Amount}\" ); } To find the most expensive product, we need two queries: first, find the largest price value, then find the products with this price. // 1.4 Console . WriteLine ( \"\\t1.4:\" ); var maxPrice = productsCollection . Find ( _ => true ) . SortByDescending ( p => p . Price ) . Limit ( 1 ) . Project ( p => p . Price ) . Single (); var qProductMax = productsCollection . Find ( p => p . Price == maxPrice ) . ToList (); foreach ( var t in qProductMax ) Console . WriteLine ( $\"\\t\\tName={t.Name}\\tPrice={t.Price}\" ); This exercise is complicated with our current database scheme because we do not have everything at hand within one collection. We need the product information from one collection, and the order details from another one. We will be doing a \"join\" in the client-side, that is, in C# code. The solution's outline is to query the orders, then in C# gather the orders by product, and finally, query the product details. // 1.5 Console . WriteLine ( \"\\t1.5:\" ); var qOrders = ordersCollection . Find ( _ => true ) . ToList (); var productOrders = qOrders . SelectMany ( o => o . OrderItems ) // All order items into one list . GroupBy ( oi => oi . ProductID ) . Where ( p => p . Count () >= 2 ); var qProducts = productsCollection . Find ( _ => true ) . ToList (); var productLookup = qProducts . ToDictionary ( p => p . ID ); foreach ( var p in productOrders ) { var product = productLookup . GetValueOrDefault ( p . Key ); Console . WriteLine ( $\"\\t\\tName={product?.Name}\\tStock={product?.Stock}\\tOrders={p.Count()}\" ); } This solution is very elegant and works only for small databases. Suppose we face a similar task under real-life circumstances. We have two choices: denormalize the database scheme and copy product details into the orders, or create an aggregation pipeline executed by the server that does something similar to the code above (MongoDB can do that, but it will not be very fast). Exercise 2: Create a new entity class \u00b6 Examine the classes Product and VAT . Why is there a field with a [BsonId] attribute in class Product and not in class VAT ? Create a new entity class for mapping Category document, then add and initialize a IMongoCollection<Category> field next to the others. Solution Class Product represents the products collection; therefore each item has a unique ObjectID . On the other hand, class VAT is an embedded field used by Product and has no collection on its own; hence it needs no id. Create our new Category POCO class. Let us check a few sample documents using Robo3T in categories collection. Create a new class Category in folder Entities with matching fields as below. using MongoDB.Bson ; using MongoDB.Bson.Serialization.Attributes ; namespace BME.DataDriven.Mongo.Entitites { public class Category { [BsonId] public ObjectId ID { get ; set ; } public string Name { get ; set ; } public ObjectId ? ParentCategoryID { get ; set ; } } } Add a new collection interface field in Program.cs as follows. private static IMongoCollection < Category > categoriesCollection ; And assign the value in the initialize method to get the collection. categoriesCollection = database . GetCollection < Category >( \"categories\" ); Exercise 3: Data modification \u00b6 The collection classes IMongoColection<TEntity> can also be used to execute modification operations. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Delete all categories that contain no products. Solution Find the ID of the category then update all products that have this category id. Console . WriteLine ( \"***** Exercise three *****\" ); //3.1 Console . WriteLine ( \"\\t3.1:\" ); var categoryLegoId = categoriesCollection . Find ( c => c . Name == \"LEGO\" ) . Project ( c => c . ID ) . Single (); var qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tBefore modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); productsCollection . UpdateMany ( filter : p => p . CategoryID == categoryLegoId , update : Builders < Product >. Update . Mul ( p => p . Price , 1.1 )); qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tAfter modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); MongoDB can execute the following sequence of steps in a single atomic step: \"Get me category Expensive toys . If it does not exist, create it.\" We will use FindOneAndUpdate to achieve this. //3.2 Console . WriteLine ( \"\\t3.2:\" ); var catExpensiveToys = categoriesCollection . FindOneAndUpdate < Category >( filter : c => c . Name == \"Expensive toys\" , update : Builders < Category >. Update . SetOnInsert ( c => c . Name , \"Expensive toys\" ), options : new FindOneAndUpdateOptions < Category , Category > { IsUpsert = true , ReturnDocument = ReturnDocument . After }); productsCollection . UpdateMany ( filter : p => p . Price > 8000 , update : Builders < Product >. Update . Set ( p => p . CategoryID , catExpensiveToys . ID )); var qProdExpensive = productsCollection . Find ( p => p . CategoryID == catExpensiveToys . ID ) . ToList (); foreach ( var p in qProdExpensive ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tPrice={p.Price}\" ); Query categories that contain any product, then delete the ones that do not belong among this list. //3.3 Console . WriteLine ( \"\\t3.3:\" ); Console . WriteLine ( $\"\\t\\tBefore modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); var qProductCategory = new HashSet < ObjectId >( productsCollection . Find ( _ => true ) . Project ( p => p . CategoryID ) . ToList ()); categoriesCollection . DeleteMany ( c => ! qProductCategory . Contains ( c . ID )); Console . WriteLine ( $\"\\t\\tAfter modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); Let us note that this is not an atomic operation. If a product was added concurrently, we could have deleted its category.","title":"MongoDB"},{"location":"seminar/mongodb/#mongodb","text":"The seminar's goal is to understand the concepts of the MongoDB document database and the usage of the MongoDB C#/.NET Driver .","title":"MongoDB"},{"location":"seminar/mongodb/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2019 ( not VS Code) MongoDB Community Edition Robo 3T Database initialization script: mongo.js Starter code: https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo Recommended to review: C# language and Linq queries MongoDB lecture Using MongoDB guide","title":"Pre-requisites"},{"location":"seminar/mongodb/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/mongodb/#exercise-0-create-database-open-starter-code","text":"Create a new working directory, e.g., c:\\work\\NEPTUN . Working directory This folder will be our working directory , that is, everything is created within this folder. Start the MongoDB server. Create a new folder for the database files, e.g., use name db . (Create this within the working directory.) Open a command prompt and start MongoDB server: mongod.exe --dbpath=\"c:\\work\\<NEPTUN>\\db\" Command prompt can be located in the start menu by typing \"cmd\". In the university computer laboratories, MongoDB is in folder c:\\tools\\mongodb\\bin . Navigate to this directory using the cd command. Keep this command prompt open because it hosts our server. To stop the server at the end of the seminar, press Ctrl+C. Create the database. Download the database initialization script and save it as mongo.js in the working directory. Open a new command prompt and initialize database: mongo.exe localhost:27017/datadriven c:\\work\\<NEPTUN>\\mongo.js The executable is not mongo\u200b d but simply mongo. This is a client software to connect to the server and execute queries. In the university computer laboratories, this executable is located at c:\\tools\\mongodb\\bin . Let us note the connection string that contains the name of the database too! Check the database using Robo3T. Start Robo3T (in the university computer laboratories, it is located at c:\\tools\\robo3t ) and connect to the server. Check if the Collections were created. Download the starter code! Open yet another new command prompt into our working directory. Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo.git Open the sln file from the newly created folder using Visual Studio. Let us examine this project. This is a .NET Core console application. The structure resembles the structure of the Entity Framework project seen before: directory Entities contains the database entities while our code will be written to Program.cs . Program.cs already contains the initialization of the connection to MongoDB. Interface IMongoClient is used for all communication with the database. We will not use this directly. Interface IMongoDatabase represents the database datadriven within the MongoDB server. And the IMongoCollection<TEntity> interfaces represent the specific collections we can use to execute queries and modification commands. The database documents are mapped to the C# entity classes in folder Entities . A major difference compared to the behavior previously seen in Entity Framework is that these classes were not generated by written manually. Most entities are already mapped. We will create one more class during an exercise.","title":"Exercise 0: Create database, open starter code"},{"location":"seminar/mongodb/#exercise-1-queries","text":"Write C# code using the MongoDB C#/.NET Driver in the following exercises. Print the results to the console. List the names and the amount of stock of all products that we have more than 30 in stock! List the orders that consist of at least two items! List the orders that have a total value of at least 30.000! For each order, print the customer name, and list all items of the order (with the product name, amount, and price). Find the most expensive product! List the products that have been ordered at least twice! Solution We need only the product collection and execute a simple query. The filter criteria can be written as a Lambda-expression and with the builder syntax too. Console . WriteLine ( \"***** Exercise one *****\" ); // 1.1 first solution Console . WriteLine ( \"\\t1.1 First solution:\" ); var qProductAndStock1 = productsCollection . Find ( p => p . Stock > 30 ) . ToList (); foreach ( var p in qProductAndStock1 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); // 1.1 second solution Console . WriteLine ( \"\\t1.1 Second solution:\" ); var qProductAndStock2 = productsCollection . Find ( Builders < Product >. Filter . Gt ( p => p . Stock , 30 )) . ToList (); foreach ( var p in qProductAndStock2 ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tStock={p.Stock}\" ); This is similar to the previous one. We may note that we would have needed a join in a relational database, but we have everything at hand here. // 1.2 first solution Console . WriteLine ( \"\\t1.2 First solution:\" ); var qOrderItems1 = ordersCollection . Find ( o => o . OrderItems . Length >= 2 ) . ToList (); foreach ( var o in qOrderItems1 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); // 1.2 second solution Console . WriteLine ( \"\\t1.2 Second solution:\" ); var qOrderItems2 = ordersCollection . Find ( Builders < Order >. Filter . SizeGte ( o => o . OrderItems , 2 )) . ToList (); foreach ( var o in qOrderItems2 ) Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\" ); A simple query is not sufficient for this exercise; thus, we need the aggregation pipeline. We may still note that every information we need is still available in one collection. // 1.3 Console . WriteLine ( \"\\t1.3:\" ); var qOrderTotal = ordersCollection . Aggregate () . Project ( order => new { CustomerID = order . CustomerID , OrderItems = order . OrderItems , Total = order . OrderItems . Sum ( oi => oi . Amount * oi . Price ) }) . Match ( order => order . Total > 30000 ) . ToList (); foreach ( var o in qOrderTotal ) { Console . WriteLine ( $\"\\t\\tCustomerID={o.CustomerID}\" ); foreach ( var oi in o . OrderItems ) Console . WriteLine ( $\"\\t\\t\\tProductID={oi.ProductID}\\tPrice={oi.Price}\\tAmount={oi.Amount}\" ); } To find the most expensive product, we need two queries: first, find the largest price value, then find the products with this price. // 1.4 Console . WriteLine ( \"\\t1.4:\" ); var maxPrice = productsCollection . Find ( _ => true ) . SortByDescending ( p => p . Price ) . Limit ( 1 ) . Project ( p => p . Price ) . Single (); var qProductMax = productsCollection . Find ( p => p . Price == maxPrice ) . ToList (); foreach ( var t in qProductMax ) Console . WriteLine ( $\"\\t\\tName={t.Name}\\tPrice={t.Price}\" ); This exercise is complicated with our current database scheme because we do not have everything at hand within one collection. We need the product information from one collection, and the order details from another one. We will be doing a \"join\" in the client-side, that is, in C# code. The solution's outline is to query the orders, then in C# gather the orders by product, and finally, query the product details. // 1.5 Console . WriteLine ( \"\\t1.5:\" ); var qOrders = ordersCollection . Find ( _ => true ) . ToList (); var productOrders = qOrders . SelectMany ( o => o . OrderItems ) // All order items into one list . GroupBy ( oi => oi . ProductID ) . Where ( p => p . Count () >= 2 ); var qProducts = productsCollection . Find ( _ => true ) . ToList (); var productLookup = qProducts . ToDictionary ( p => p . ID ); foreach ( var p in productOrders ) { var product = productLookup . GetValueOrDefault ( p . Key ); Console . WriteLine ( $\"\\t\\tName={product?.Name}\\tStock={product?.Stock}\\tOrders={p.Count()}\" ); } This solution is very elegant and works only for small databases. Suppose we face a similar task under real-life circumstances. We have two choices: denormalize the database scheme and copy product details into the orders, or create an aggregation pipeline executed by the server that does something similar to the code above (MongoDB can do that, but it will not be very fast).","title":"Exercise 1: Queries"},{"location":"seminar/mongodb/#exercise-2-create-a-new-entity-class","text":"Examine the classes Product and VAT . Why is there a field with a [BsonId] attribute in class Product and not in class VAT ? Create a new entity class for mapping Category document, then add and initialize a IMongoCollection<Category> field next to the others. Solution Class Product represents the products collection; therefore each item has a unique ObjectID . On the other hand, class VAT is an embedded field used by Product and has no collection on its own; hence it needs no id. Create our new Category POCO class. Let us check a few sample documents using Robo3T in categories collection. Create a new class Category in folder Entities with matching fields as below. using MongoDB.Bson ; using MongoDB.Bson.Serialization.Attributes ; namespace BME.DataDriven.Mongo.Entitites { public class Category { [BsonId] public ObjectId ID { get ; set ; } public string Name { get ; set ; } public ObjectId ? ParentCategoryID { get ; set ; } } } Add a new collection interface field in Program.cs as follows. private static IMongoCollection < Category > categoriesCollection ; And assign the value in the initialize method to get the collection. categoriesCollection = database . GetCollection < Category >( \"categories\" );","title":"Exercise 2: Create a new entity class"},{"location":"seminar/mongodb/#exercise-3-data-modification","text":"The collection classes IMongoColection<TEntity> can also be used to execute modification operations. Write C# code that increases the price of all products in category \"LEGO\" by 10 percent! Create a new category named Expensive toys and move all products here that cost more than 8000! Delete all categories that contain no products. Solution Find the ID of the category then update all products that have this category id. Console . WriteLine ( \"***** Exercise three *****\" ); //3.1 Console . WriteLine ( \"\\t3.1:\" ); var categoryLegoId = categoriesCollection . Find ( c => c . Name == \"LEGO\" ) . Project ( c => c . ID ) . Single (); var qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tBefore modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); productsCollection . UpdateMany ( filter : p => p . CategoryID == categoryLegoId , update : Builders < Product >. Update . Mul ( p => p . Price , 1.1 )); qProductLego = productsCollection . Find ( p => p . CategoryID == categoryLegoId ) . ToList (); Console . WriteLine ( \"\\t\\tAfter modification:\" ); foreach ( var p in qProductLego ) Console . WriteLine ( $\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\" ); MongoDB can execute the following sequence of steps in a single atomic step: \"Get me category Expensive toys . If it does not exist, create it.\" We will use FindOneAndUpdate to achieve this. //3.2 Console . WriteLine ( \"\\t3.2:\" ); var catExpensiveToys = categoriesCollection . FindOneAndUpdate < Category >( filter : c => c . Name == \"Expensive toys\" , update : Builders < Category >. Update . SetOnInsert ( c => c . Name , \"Expensive toys\" ), options : new FindOneAndUpdateOptions < Category , Category > { IsUpsert = true , ReturnDocument = ReturnDocument . After }); productsCollection . UpdateMany ( filter : p => p . Price > 8000 , update : Builders < Product >. Update . Set ( p => p . CategoryID , catExpensiveToys . ID )); var qProdExpensive = productsCollection . Find ( p => p . CategoryID == catExpensiveToys . ID ) . ToList (); foreach ( var p in qProdExpensive ) Console . WriteLine ( $\"\\t\\tName={p.Name}\\tPrice={p.Price}\" ); Query categories that contain any product, then delete the ones that do not belong among this list. //3.3 Console . WriteLine ( \"\\t3.3:\" ); Console . WriteLine ( $\"\\t\\tBefore modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); var qProductCategory = new HashSet < ObjectId >( productsCollection . Find ( _ => true ) . Project ( p => p . CategoryID ) . ToList ()); categoriesCollection . DeleteMany ( c => ! qProductCategory . Contains ( c . ID )); Console . WriteLine ( $\"\\t\\tAfter modification: {categoriesCollection.CountDocuments(_ => true)} categories\" ); Let us note that this is not an atomic operation. If a product was added concurrently, we could have deleted its category.","title":"Exercise 3: Data modification"},{"location":"seminar/mssql/","text":"Microsoft SQL Server programming \u00b6 The seminar's goal is to get to know the server-side programming capabilities of the Microsoft SQL Server platform. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: SQL language Microsoft SQL Server programming (stored procedures, triggers) Microsoft SQL Server usage guide How to work during the seminar \u00b6 The first four exercises are solved together with the instructor. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: SQL commands (review) \u00b6 Write SQL commands/queries for the following exercises. How many uncompleted orders are there (look for a status other than \"Delivered\")? Solution select count ( * ) from [ Order ] o join Status s on o . StatusID = s . ID where s . Name != 'Delivered' We see a join and an aggregation here. (There are other syntaxes for joining tables; refer to the lecture notes.) Which payment methods have not been used at all? Solution select p . Method from [ Order ] o right outer join PaymentMethod p on o . PaymentMethodID = p . ID where o . ID is null The key in the solution is the outer join , through which we can see the payment method records that have no orders. Let us insert a new customer and query the auto-assigned primary key! Solution insert into Customer ( Name , Login , Password , Email ) values ( 'Test Test' , 'tt' , '********' , 'tt@email.com' ) select @@ IDENTITY It is recommended (though not required) to name the columns after insert to be unambiguous. No value was assigned to the ID column, as the definition of that column mandates that the database automatically assigns a new value upon insert. We can query this ID after the insert is completed. One of the categories has the wrong name. Let us change Tricycle to Tricycles ! Solution update Category set Name = 'Tricycles' where Name = 'Tricycle' Which category contains the largest number of products? Solution select top 1 Name , ( select count ( * ) from Product where Product . CategoryID = c . ID ) as cnt from Category c order by cnt desc There are many ways this query can be formulated. This is only one possible solution. It also serves as an example of the usage of subqueries. Exercise 2: Inserting a new product category \u00b6 Create a new stored procedure that helps inserting a new product category. The procedure's inputs are the name of the new category, and optionally the name of the parent category. Raise an error if the category already exists, or the parent category does not exist. Let the database generate the primary key for the insertion. Solution Stored procedure create or alter procedure AddNewCategory @ Name nvarchar ( 50 ), @ ParentName nvarchar ( 50 ) as begin tran -- Is there a category with identical name? declare @ ID int select @ ID = ID from Category with ( TABLOCKX ) where upper ( Name ) = upper ( @ Name ) if @ ID is not null begin rollback raiserror ( 'Category %s already exists' , 16 , 1 , @ Name ) return end declare @ ParentID int if @ ParentName is not null begin select @ ParentID = ID from Category where upper ( Name ) = upper ( @ ParentName ) if @ ParentID is null begin rollback raiserror ( 'Category %s does not exist' , 16 , 1 , @ ParentName ) return end end insert into Category values ( @ Name , @ ParentID ) commit Testing Let us open a new Query window and execute the following testing instructions. exec AddNewCategory 'Beach balls', NULL This shall succeed. Let us verify the table contents afterward. Let us repeat the same command; it shall fail now. We can also try with a parent category. exec AddNewCategory 'LEGO Star Wars', 'LEGO' Exercise 3: Maintenance of order status \u00b6 Create a trigger that updates the status of each item of an order when the status of the order changes. Do this only for those items of the order that have the same status the order had before the change. Other items in the order should not be affected. Solution Trigger create or alter trigger UpdateOrderStatus on [ Order ] for update as update OrderItem set StatusID = i . StatusID from OrderItem oi inner join inserted i on i . Id = oi . OrderID inner join deleted d on d . ID = oi . OrderID where i . StatusID != d . StatusID and oi . StatusID = d . StatusID Let us make sure we understand the update ... from syntax. The behavior is as follows. We use this command when some of the changes we want to make during the update require data from another table. The syntax is based on the usual update ... set... format extended with a from part, which follows the same syntax as a select from , including the join to gather information from other tables. This allows us to use the joined records and their content in the set statement (that is, a value from a joined record can be on the right side of an assignment). Testing Let us check the status of the order and each item in the order: select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Let us change the status of the order: update [ Order ] set StatusID = 4 where ID = 1 Check the status now (the update should have updated all stauses): select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Exercise 4: Summing the total purchases of a customer \u00b6 Let us calculate and store the value of all purchases made by a customer! Add a new column to the table: alter table Customer add Total float Calculate the current totals. Let us use a cursor for iterating through all customers. Solution declare cur_customer cursor for select ID from Customer declare @ CustomerId int declare @ Total float open cur_customer fetch next from cur_customer into @ CustomerId while @@ FETCH_STATUS = 0 begin select @ Total = sum ( oi . Amount * oi . Price ) from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join OrderItem oi on oi . OrderID = o . ID where s . CustomerID = @ CustomerId update Customer set Total = ISNULL ( @ Total , 0 ) where ID = @ CustomerId fetch next from cur_customer into @ CustomerId end close cur_customer deallocate cur_customer To verify check the contents of the Customer table. Exercise 5: Maintenance of the total value (individual exercise) \u00b6 The values calculated in the previous exercise contain the current state. Create a trigger that updates this value whenever a related order is changed. Instead of re-calculating the value, update it with the changes made! Solution The key in the solution is recognizing which table the trigger should be placed on. We are interested in changes in order, but the total value actually depends on the items registered for an order; thus the trigger should react to changes in the order items. The exercise is complicated because the inserted and deleted tables may contain multiple records, possibly even related to multiple customers. A solution for overcoming this obstacle is to use a cursor to process all changes; another option, as below, is aggregating the changes by customer. Trigger create or alter trigger CustomerTotalUpdate on OrderItem for insert , update , delete as update Customer set Total = isnull ( Total , 0 ) + TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join inserted i on i . OrderID = o . ID group by s . CustomerId ) CustomerChange on Customer . ID = CustomerChange . CustomerId update Customer set Total = isnull ( Total , 0 ) - TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join deleted d on d . OrderID = o . ID group by s . CustomerID ) CustomerChange on Customer . ID = CustomerChange . CustomerId Testing Let us remember the total for the customers. select ID , Total from Customer Change the ordered amount. update OrderItem set Amount = 3 where ID = 1 Check the totals again, should have changed. select ID , Total from Customer","title":"Microsoft SQL Server programming"},{"location":"seminar/mssql/#microsoft-sql-server-programming","text":"The seminar's goal is to get to know the server-side programming capabilities of the Microsoft SQL Server platform.","title":"Microsoft SQL Server programming"},{"location":"seminar/mssql/#pre-requisites","text":"Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: SQL language Microsoft SQL Server programming (stored procedures, triggers) Microsoft SQL Server usage guide","title":"Pre-requisites"},{"location":"seminar/mssql/#how-to-work-during-the-seminar","text":"The first four exercises are solved together with the instructor. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/mssql/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/mssql/#exercise-1-sql-commands-review","text":"Write SQL commands/queries for the following exercises. How many uncompleted orders are there (look for a status other than \"Delivered\")? Solution select count ( * ) from [ Order ] o join Status s on o . StatusID = s . ID where s . Name != 'Delivered' We see a join and an aggregation here. (There are other syntaxes for joining tables; refer to the lecture notes.) Which payment methods have not been used at all? Solution select p . Method from [ Order ] o right outer join PaymentMethod p on o . PaymentMethodID = p . ID where o . ID is null The key in the solution is the outer join , through which we can see the payment method records that have no orders. Let us insert a new customer and query the auto-assigned primary key! Solution insert into Customer ( Name , Login , Password , Email ) values ( 'Test Test' , 'tt' , '********' , 'tt@email.com' ) select @@ IDENTITY It is recommended (though not required) to name the columns after insert to be unambiguous. No value was assigned to the ID column, as the definition of that column mandates that the database automatically assigns a new value upon insert. We can query this ID after the insert is completed. One of the categories has the wrong name. Let us change Tricycle to Tricycles ! Solution update Category set Name = 'Tricycles' where Name = 'Tricycle' Which category contains the largest number of products? Solution select top 1 Name , ( select count ( * ) from Product where Product . CategoryID = c . ID ) as cnt from Category c order by cnt desc There are many ways this query can be formulated. This is only one possible solution. It also serves as an example of the usage of subqueries.","title":"Exercise 1: SQL commands (review)"},{"location":"seminar/mssql/#exercise-2-inserting-a-new-product-category","text":"Create a new stored procedure that helps inserting a new product category. The procedure's inputs are the name of the new category, and optionally the name of the parent category. Raise an error if the category already exists, or the parent category does not exist. Let the database generate the primary key for the insertion. Solution Stored procedure create or alter procedure AddNewCategory @ Name nvarchar ( 50 ), @ ParentName nvarchar ( 50 ) as begin tran -- Is there a category with identical name? declare @ ID int select @ ID = ID from Category with ( TABLOCKX ) where upper ( Name ) = upper ( @ Name ) if @ ID is not null begin rollback raiserror ( 'Category %s already exists' , 16 , 1 , @ Name ) return end declare @ ParentID int if @ ParentName is not null begin select @ ParentID = ID from Category where upper ( Name ) = upper ( @ ParentName ) if @ ParentID is null begin rollback raiserror ( 'Category %s does not exist' , 16 , 1 , @ ParentName ) return end end insert into Category values ( @ Name , @ ParentID ) commit Testing Let us open a new Query window and execute the following testing instructions. exec AddNewCategory 'Beach balls', NULL This shall succeed. Let us verify the table contents afterward. Let us repeat the same command; it shall fail now. We can also try with a parent category. exec AddNewCategory 'LEGO Star Wars', 'LEGO'","title":"Exercise 2: Inserting a new product category"},{"location":"seminar/mssql/#exercise-3-maintenance-of-order-status","text":"Create a trigger that updates the status of each item of an order when the status of the order changes. Do this only for those items of the order that have the same status the order had before the change. Other items in the order should not be affected. Solution Trigger create or alter trigger UpdateOrderStatus on [ Order ] for update as update OrderItem set StatusID = i . StatusID from OrderItem oi inner join inserted i on i . Id = oi . OrderID inner join deleted d on d . ID = oi . OrderID where i . StatusID != d . StatusID and oi . StatusID = d . StatusID Let us make sure we understand the update ... from syntax. The behavior is as follows. We use this command when some of the changes we want to make during the update require data from another table. The syntax is based on the usual update ... set... format extended with a from part, which follows the same syntax as a select from , including the join to gather information from other tables. This allows us to use the joined records and their content in the set statement (that is, a value from a joined record can be on the right side of an assignment). Testing Let us check the status of the order and each item in the order: select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1 Let us change the status of the order: update [ Order ] set StatusID = 4 where ID = 1 Check the status now (the update should have updated all stauses): select OrderItem . StatusID , [ Order ]. StatusID from OrderItem join [ Order ] on OrderItem . OrderID = [ Order ]. ID where OrderID = 1","title":"Exercise 3: Maintenance of order status"},{"location":"seminar/mssql/#exercise-4-summing-the-total-purchases-of-a-customer","text":"Let us calculate and store the value of all purchases made by a customer! Add a new column to the table: alter table Customer add Total float Calculate the current totals. Let us use a cursor for iterating through all customers. Solution declare cur_customer cursor for select ID from Customer declare @ CustomerId int declare @ Total float open cur_customer fetch next from cur_customer into @ CustomerId while @@ FETCH_STATUS = 0 begin select @ Total = sum ( oi . Amount * oi . Price ) from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join OrderItem oi on oi . OrderID = o . ID where s . CustomerID = @ CustomerId update Customer set Total = ISNULL ( @ Total , 0 ) where ID = @ CustomerId fetch next from cur_customer into @ CustomerId end close cur_customer deallocate cur_customer To verify check the contents of the Customer table.","title":"Exercise 4: Summing the total purchases of a customer"},{"location":"seminar/mssql/#exercise-5-maintenance-of-the-total-value-individual-exercise","text":"The values calculated in the previous exercise contain the current state. Create a trigger that updates this value whenever a related order is changed. Instead of re-calculating the value, update it with the changes made! Solution The key in the solution is recognizing which table the trigger should be placed on. We are interested in changes in order, but the total value actually depends on the items registered for an order; thus the trigger should react to changes in the order items. The exercise is complicated because the inserted and deleted tables may contain multiple records, possibly even related to multiple customers. A solution for overcoming this obstacle is to use a cursor to process all changes; another option, as below, is aggregating the changes by customer. Trigger create or alter trigger CustomerTotalUpdate on OrderItem for insert , update , delete as update Customer set Total = isnull ( Total , 0 ) + TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join inserted i on i . OrderID = o . ID group by s . CustomerId ) CustomerChange on Customer . ID = CustomerChange . CustomerId update Customer set Total = isnull ( Total , 0 ) - TotalChange from Customer inner join ( select s . CustomerId , sum ( Amount * Price ) as TotalChange from CustomerSite s inner join [ Order ] o on o . CustomerSiteID = s . ID inner join deleted d on d . OrderID = o . ID group by s . CustomerID ) CustomerChange on Customer . ID = CustomerChange . CustomerId Testing Let us remember the total for the customers. select ID , Total from Customer Change the ordered amount. update OrderItem set Amount = 3 where ID = 1 Check the totals again, should have changed. select ID , Total from Customer","title":"Exercise 5: Maintenance of the total value (individual exercise)"},{"location":"seminar/rest/","text":"REST API & ASP.NET Web API \u00b6 The seminar's goal is to practice working with REST APIs and the .NET Web API technology. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft Visual Studio 2019 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Postman: https://www.getpostman.com/downloads/ Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-rest-kiindulo Recommended to review: C# language Entity Framework and Linq REST API and Web API lecture How to work during the seminar \u00b6 The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 0: Create/check the database \u00b6 The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .) Exercise 1: Open starter project \u00b6 Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-rest-kiindulo.git Open the sln file in the rest folder using Visual Studio. Let us examine this project. This is an ASP.NET Core Web API project. This project is created for hosting REST API backends. It contains a web server internally; thus, when running it using F5 we get a fully functional API able to respond to http requests. Let us examine Program.cs . We do not need to understand everything here. This is like a console application; the Main method here, the entry point that starts a web server. The Entity Framework Code First mapping of our database is in the Dal folder. Class DataDrivenDbContext is the data access class. We need to fix the connection string in the OnConfiguring method in this class. The connection string usually should not be hard-wired in the source code. This is for the sake of simplicity here. There is a test controller in folder Controllers . Let us open and examine the code. Let us note the [ApiController] and [Route] attributes and the inheritance. These make a class a Web API controller . The behavior is automatic: the controller's methods are invoked by the framework when they match the expected signature. This means that no additional configuration is needed here. Start the application. After building the source code, a console application will start where we will see diagnostic messages. Let us open a browser and navigate to http://localhost:5000/api/values . We should receive a JSON response. Stop the application by pressing Ctrl-C in the console, or stop with Visual Studio. Exercise 2: First controller and testing with Postman \u00b6 Create a new Web API controller that responds with a greeting. Test the behavior using Postman. Delete the existing class ValuesController . Add a new empty Api Controller with the name HelloController : in Solution Explorer right-click the Controllers folder and choose Add / Controller... / API Controller - Empty . The HelloController should respond to URL /api/hello . The application shall respond with a text when GET request is received. Test this endpoint using Postman by sending a GET request to http://localhost:5000/api/hello . Change the REST endpoint by expecting an optional name as a query parameter ; if such value is provided, the response greeting should include this name. Test this with Postman: send a name by calling URL http://localhost:5000/api/hello?name=apple . Create a new REST API endpoint that responds to URL http://localhost:5000/api/hello/apple just like the previous one, but the name is in the path here. Solution [Route(\"api/hello\")] [ApiController] public class HelloController : ControllerBase { // 2. //[HttpGet] //public ActionResult<string> Hello() //{ // return \"Hello!\"; //} // 3. [HttpGet] public ActionResult < string > Hello ([ FromQuery ] string name ) { if ( string . IsNullOrEmpty ( name )) return \"Hello noname!\" ; else return \"Hello \" + name ; } // 4. [HttpGet] [Route(\"{personName}\")] // the liter inside {} in this route must match the parameter name public ActionResult < string > HelloRoute ( string personName ) { return \"Hello route \" + personName ; } } Let us summarize what we need to create a new WebAPI endpoint: Inherit from the ControllerBase class and add the [ApiController] attribute. Specify the URL route on the class or above the method (or on both) using the [Route] attribute. Define a method with the right signature (return value and parameters). Choose what type of http queries to respond to using one of the [Http*] attributes. Exercise 3: Product search API \u00b6 A real API does not return constant strings. Create an API for searching among the products of our webshop. Create a new controller. Enable listing products; 5 per page. Enable search based on the name. The data returned should not be the database entity; instead create a new DTO (data transfer object) class in a new folder called Models . Test the new endpoints. Solution // ********************************* // Models/Product.cs namespace BME.DataDriven.REST.Models { public class Product { public Product ( int id , string name , double? price , int? stock ) { Id = id ; Name = name ; Price = price ; Stock = stock ; } // Contains only the relevant data; e.g. the database foreign keys are of no use here. // Assignment only via the constructor; this makes it unambiguous // that this is a snapshot of information that cannot be modified. public int Id { get ; private set ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs using System.Linq ; using Microsoft.AspNetCore.Mvc ; namespace BME.DataDriven.REST.Controllers { [Route(\"api/products\")] // it is better to explicitly specify the url [ApiController] public class ProductsController : ControllerBase { private readonly Dal . DataDrivenDbContext dbContext ; // The database is obtained through the Dependency Injection service of the framework. // The DbContext is automatically disposed at the end of the request. public ProductsController ( Dal . DataDrivenDbContext dbContext ) { this . dbContext = dbContext ; } [HttpGet] public ActionResult < Models . Product []> List ([ FromQuery ] string search = null , [ FromQuery ] int from = 0 ) { IQueryable < Dal . Product > filteredList ; if ( string . IsNullOrEmpty ( search )) // no search yields all products filteredList = dbContext . Product ; else // search by name filteredList = dbContext . Product . Where ( p => p . Name . Contains ( search )); return filteredList . Skip ( from ) // paging: from which product . Take ( 5 ) // 5 items on one page . Select ( p => new Models . Product ( p . Id , p . Name , p . Price , p . Stock )) // db to dto conversion . ToArray (); // enforce evaluating the IQueryable - otherwise would yield an error } } } Let us note that we did not need to concern ourselves with JSON serialization. The API returns objects. The framework automatically handles the serialization. Paging is useful to limit the size of the response (and paging is also customary on UIs). Specifying a \u201cfrom\u201d is a simple and frequently used solution. The result of the method before the ToArray is an IQueryable . We may remember that the IQueryable does not contain the result; it is merely a descriptor of the query. If we had no ToArray , we would see an error. When the framework would begin the serialization to JSON, it would start iterating the query; but at this point, the database connection has already been closed. Therefore WebAPI endpoints should not return IEnumerable or IQueryable . Exercise 4: Editing products via the API \u00b6 Add the following functionality to our API: Fetch the data of a particular product specified by id at url /api/products/id . Update the name, price, and stock of a product. Add a new product (create a new DTO class for input that contains only the name, price, and stock). Delete a product by specifying the id. Test each endpoint! Inserting a new product you will need the following settings in Postman: POST request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"name\" : \"BME pen\" , \"price\" : 8900 , \"stock\" : 100 } Note: In our case the JSON data is deserialized into a newly introduced (see later) Models.NewProduct object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Updating a product you will need the following settings: PUT request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"ID\" : 10 , \"name\" : \"Silence for one hour\" , \"price\" : 440 , \"stock\" : 10 } Note: In our case the JSON data is deserialized into a Models.Product object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Make sure to check the headers of the response too! Update and insert should add the Location header. This header should contain the URL to fetch the record. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { public NewProduct ( string name , double? price , int? stock ) { Name = name ; Price = price ; Stock = stock ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { public class ProductsController : ControllerBase { // ... // GET api/products/id [HttpGet] [Route(\"{id}\")] public ActionResult < Models . Product > Get ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // expected response when an item is not found else return new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock ); // in case of success return the item itself } // PUT api/products/id [HttpPut] [Route(\"{id}\")] public ActionResult Modify ([ FromRoute ] int id , [ FromBody ] Models . Product updated ) { if ( id != updated . Id ) return BadRequest (); var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // modifications performed here dbProduct . Name = updated . Name ; dbProduct . Price = updated . Price ; dbProduct . Stock = updated . Stock ; // save to database dbContext . SaveChanges (); return NoContent (); // response 204 NoContent } // POST api/products [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , CategoryId = 1 , // not nice, temporary solution VatId = 1 // not nice, temporary solution }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } // DELETE api/products/id [HttpDelete] [Route(\"{id}\")] public ActionResult Delete ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); dbContext . Product . Remove ( dbProduct ); dbContext . SaveChanges (); return NoContent (); // successful delete is signaled with 204 NoContent (could be 200 OK as well if we included the entity) } } } Exercise 5: Add new product with category and VAT \u00b6 When creating the new product, we have to specify the category, as well as the value-added tax. Change the insert operation from before by allowing the category name and the tax percentage to be specified. Find the VAT and Category records based on the provided data, or create new records if needed. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { // ... // Also extend the constructor! // Important note: It's important how constructor parameters are named. // Our properties have private setters, and thanks to this json deserialization // maps JSON object field names to constructor parameter names (in a case // insensitive manner). public int VATPercentage { get ; private set ; } public string CategoryName { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { // ... [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = dbContext . Vat . FirstOrDefault ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = dbContext . Category . FirstOrDefault ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } } Exercise 6: Asynchronous controller method \u00b6 Let us refactor the previous exercise code for asynchronous execution, that is, let us use async-await . Asynchronous execution utilizes the execution threads of the server more efficiently while waiting for database operations. We can easily make our code asynchronous by relying on the asynchronous support of Entity Framework. Solution [HttpPost] public async Task < ActionResult > Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = await dbContext . Vat . FirstOrDefaultAsync ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = await dbContext . Category . FirstOrDefaultAsync ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); await dbContext . SaveChangesAsync (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } Let us see how simple this was. Entity Framework provides us the ...Async methods, and we only have to await them, and update the method signature a litte. Everything else is taken care of by the framework. Note. The async-await is a .NET frmaework feature supported by both ASP.NET Core and Entity Framework. It is also supported by a lot of other libraries as well.","title":"REST API & ASP.NET Web API"},{"location":"seminar/rest/#rest-api-aspnet-web-api","text":"The seminar's goal is to practice working with REST APIs and the .NET Web API technology.","title":"REST API &amp; ASP.NET Web API"},{"location":"seminar/rest/#pre-requisites","text":"Required tools to complete the tasks: Microsoft Visual Studio 2019 ( not VS Code) Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Postman: https://www.getpostman.com/downloads/ Database initialization script: mssql.sql Starter code: https://github.com/bmeviauac01/gyakorlat-rest-kiindulo Recommended to review: C# language Entity Framework and Linq REST API and Web API lecture","title":"Pre-requisites"},{"location":"seminar/rest/#how-to-work-during-the-seminar","text":"The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/rest/#exercise-0-createcheck-the-database","text":"The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material .)","title":"Exercise 0: Create/check the database"},{"location":"seminar/rest/#exercise-1-open-starter-project","text":"Download the project skeleton! Open a new command prompt Navigate to a directory, e.g. c:\\work\\NEPTUN Execute the following command: git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-rest-kiindulo.git Open the sln file in the rest folder using Visual Studio. Let us examine this project. This is an ASP.NET Core Web API project. This project is created for hosting REST API backends. It contains a web server internally; thus, when running it using F5 we get a fully functional API able to respond to http requests. Let us examine Program.cs . We do not need to understand everything here. This is like a console application; the Main method here, the entry point that starts a web server. The Entity Framework Code First mapping of our database is in the Dal folder. Class DataDrivenDbContext is the data access class. We need to fix the connection string in the OnConfiguring method in this class. The connection string usually should not be hard-wired in the source code. This is for the sake of simplicity here. There is a test controller in folder Controllers . Let us open and examine the code. Let us note the [ApiController] and [Route] attributes and the inheritance. These make a class a Web API controller . The behavior is automatic: the controller's methods are invoked by the framework when they match the expected signature. This means that no additional configuration is needed here. Start the application. After building the source code, a console application will start where we will see diagnostic messages. Let us open a browser and navigate to http://localhost:5000/api/values . We should receive a JSON response. Stop the application by pressing Ctrl-C in the console, or stop with Visual Studio.","title":"Exercise 1: Open starter project"},{"location":"seminar/rest/#exercise-2-first-controller-and-testing-with-postman","text":"Create a new Web API controller that responds with a greeting. Test the behavior using Postman. Delete the existing class ValuesController . Add a new empty Api Controller with the name HelloController : in Solution Explorer right-click the Controllers folder and choose Add / Controller... / API Controller - Empty . The HelloController should respond to URL /api/hello . The application shall respond with a text when GET request is received. Test this endpoint using Postman by sending a GET request to http://localhost:5000/api/hello . Change the REST endpoint by expecting an optional name as a query parameter ; if such value is provided, the response greeting should include this name. Test this with Postman: send a name by calling URL http://localhost:5000/api/hello?name=apple . Create a new REST API endpoint that responds to URL http://localhost:5000/api/hello/apple just like the previous one, but the name is in the path here. Solution [Route(\"api/hello\")] [ApiController] public class HelloController : ControllerBase { // 2. //[HttpGet] //public ActionResult<string> Hello() //{ // return \"Hello!\"; //} // 3. [HttpGet] public ActionResult < string > Hello ([ FromQuery ] string name ) { if ( string . IsNullOrEmpty ( name )) return \"Hello noname!\" ; else return \"Hello \" + name ; } // 4. [HttpGet] [Route(\"{personName}\")] // the liter inside {} in this route must match the parameter name public ActionResult < string > HelloRoute ( string personName ) { return \"Hello route \" + personName ; } } Let us summarize what we need to create a new WebAPI endpoint: Inherit from the ControllerBase class and add the [ApiController] attribute. Specify the URL route on the class or above the method (or on both) using the [Route] attribute. Define a method with the right signature (return value and parameters). Choose what type of http queries to respond to using one of the [Http*] attributes.","title":"Exercise 2: First controller and testing with Postman"},{"location":"seminar/rest/#exercise-3-product-search-api","text":"A real API does not return constant strings. Create an API for searching among the products of our webshop. Create a new controller. Enable listing products; 5 per page. Enable search based on the name. The data returned should not be the database entity; instead create a new DTO (data transfer object) class in a new folder called Models . Test the new endpoints. Solution // ********************************* // Models/Product.cs namespace BME.DataDriven.REST.Models { public class Product { public Product ( int id , string name , double? price , int? stock ) { Id = id ; Name = name ; Price = price ; Stock = stock ; } // Contains only the relevant data; e.g. the database foreign keys are of no use here. // Assignment only via the constructor; this makes it unambiguous // that this is a snapshot of information that cannot be modified. public int Id { get ; private set ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs using System.Linq ; using Microsoft.AspNetCore.Mvc ; namespace BME.DataDriven.REST.Controllers { [Route(\"api/products\")] // it is better to explicitly specify the url [ApiController] public class ProductsController : ControllerBase { private readonly Dal . DataDrivenDbContext dbContext ; // The database is obtained through the Dependency Injection service of the framework. // The DbContext is automatically disposed at the end of the request. public ProductsController ( Dal . DataDrivenDbContext dbContext ) { this . dbContext = dbContext ; } [HttpGet] public ActionResult < Models . Product []> List ([ FromQuery ] string search = null , [ FromQuery ] int from = 0 ) { IQueryable < Dal . Product > filteredList ; if ( string . IsNullOrEmpty ( search )) // no search yields all products filteredList = dbContext . Product ; else // search by name filteredList = dbContext . Product . Where ( p => p . Name . Contains ( search )); return filteredList . Skip ( from ) // paging: from which product . Take ( 5 ) // 5 items on one page . Select ( p => new Models . Product ( p . Id , p . Name , p . Price , p . Stock )) // db to dto conversion . ToArray (); // enforce evaluating the IQueryable - otherwise would yield an error } } } Let us note that we did not need to concern ourselves with JSON serialization. The API returns objects. The framework automatically handles the serialization. Paging is useful to limit the size of the response (and paging is also customary on UIs). Specifying a \u201cfrom\u201d is a simple and frequently used solution. The result of the method before the ToArray is an IQueryable . We may remember that the IQueryable does not contain the result; it is merely a descriptor of the query. If we had no ToArray , we would see an error. When the framework would begin the serialization to JSON, it would start iterating the query; but at this point, the database connection has already been closed. Therefore WebAPI endpoints should not return IEnumerable or IQueryable .","title":"Exercise 3: Product search API"},{"location":"seminar/rest/#exercise-4-editing-products-via-the-api","text":"Add the following functionality to our API: Fetch the data of a particular product specified by id at url /api/products/id . Update the name, price, and stock of a product. Add a new product (create a new DTO class for input that contains only the name, price, and stock). Delete a product by specifying the id. Test each endpoint! Inserting a new product you will need the following settings in Postman: POST request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"name\" : \"BME pen\" , \"price\" : 8900 , \"stock\" : 100 } Note: In our case the JSON data is deserialized into a newly introduced (see later) Models.NewProduct object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Updating a product you will need the following settings: PUT request to the correct URL Specify the Body : choose raw and then JSON And use the JSON as body below: { \"ID\" : 10 , \"name\" : \"Silence for one hour\" , \"price\" : 440 , \"stock\" : 10 } Note: In our case the JSON data is deserialized into a Models.Product object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class. Make sure to check the headers of the response too! Update and insert should add the Location header. This header should contain the URL to fetch the record. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { public NewProduct ( string name , double? price , int? stock ) { Name = name ; Price = price ; Stock = stock ; } public string Name { get ; private set ; } public double? Price { get ; private set ; } public int? Stock { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { public class ProductsController : ControllerBase { // ... // GET api/products/id [HttpGet] [Route(\"{id}\")] public ActionResult < Models . Product > Get ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // expected response when an item is not found else return new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock ); // in case of success return the item itself } // PUT api/products/id [HttpPut] [Route(\"{id}\")] public ActionResult Modify ([ FromRoute ] int id , [ FromBody ] Models . Product updated ) { if ( id != updated . Id ) return BadRequest (); var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); // modifications performed here dbProduct . Name = updated . Name ; dbProduct . Price = updated . Price ; dbProduct . Stock = updated . Stock ; // save to database dbContext . SaveChanges (); return NoContent (); // response 204 NoContent } // POST api/products [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , CategoryId = 1 , // not nice, temporary solution VatId = 1 // not nice, temporary solution }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } // DELETE api/products/id [HttpDelete] [Route(\"{id}\")] public ActionResult Delete ( int id ) { var dbProduct = dbContext . Product . SingleOrDefault ( p => p . Id == id ); if ( dbProduct == null ) return NotFound (); dbContext . Product . Remove ( dbProduct ); dbContext . SaveChanges (); return NoContent (); // successful delete is signaled with 204 NoContent (could be 200 OK as well if we included the entity) } } }","title":"Exercise 4: Editing products via the API"},{"location":"seminar/rest/#exercise-5-add-new-product-with-category-and-vat","text":"When creating the new product, we have to specify the category, as well as the value-added tax. Change the insert operation from before by allowing the category name and the tax percentage to be specified. Find the VAT and Category records based on the provided data, or create new records if needed. Solution // ********************************* // Models/NewProduct.cs namespace BME.DataDriven.REST.Models { public class NewProduct { // ... // Also extend the constructor! // Important note: It's important how constructor parameters are named. // Our properties have private setters, and thanks to this json deserialization // maps JSON object field names to constructor parameter names (in a case // insensitive manner). public int VATPercentage { get ; private set ; } public string CategoryName { get ; private set ; } } } // ********************************* // Controllers/ProductsController.cs namespace BME.DataDriven.REST.Controllers { // ... [HttpPost] public ActionResult Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = dbContext . Vat . FirstOrDefault ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = dbContext . Category . FirstOrDefault ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); dbContext . SaveChanges (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } }","title":"Exercise 5: Add new product with category and VAT"},{"location":"seminar/rest/#exercise-6-asynchronous-controller-method","text":"Let us refactor the previous exercise code for asynchronous execution, that is, let us use async-await . Asynchronous execution utilizes the execution threads of the server more efficiently while waiting for database operations. We can easily make our code asynchronous by relying on the asynchronous support of Entity Framework. Solution [HttpPost] public async Task < ActionResult > Create ([ FromBody ] Models . NewProduct newProduct ) { var dbVat = await dbContext . Vat . FirstOrDefaultAsync ( v => v . Percentage == newProduct . VATPercentage ); if ( dbVat == null ) dbVat = new Dal . VAT () { Percentage = newProduct . VATPercentage }; var dbCat = await dbContext . Category . FirstOrDefaultAsync ( c => c . Name == newProduct . CategoryName ); if ( dbCat == null ) dbCat = new Dal . Category () { Name = newProduct . CategoryName }; var dbProduct = new Dal . Product () { Name = newProduct . Name , Price = newProduct . Price , Stock = newProduct . Stock , Category = dbCat , VAT = dbVat }; // save to database dbContext . Product . Add ( dbProduct ); await dbContext . SaveChangesAsync (); return CreatedAtAction ( nameof ( Get ), new { id = dbProduct . Id }, new Models . Product ( dbProduct . Id , dbProduct . Name , dbProduct . Price , dbProduct . Stock )); // this will add the URL where the new item is available into the header } Let us see how simple this was. Entity Framework provides us the ...Async methods, and we only have to await them, and update the method signature a litte. Everything else is taken care of by the framework. Note. The async-await is a .NET frmaework feature supported by both ASP.NET Core and Entity Framework. It is also supported by a lot of other libraries as well.","title":"Exercise 6: Asynchronous controller method"},{"location":"seminar/transactions/","text":"Transactions \u00b6 The goal is to examine transaction handling of MS SQL Server, understand the practical limits to serializable isolation level, and controlling data dependency in read committed isolation level. Pre-requisites \u00b6 Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: Transaction properties, isolation levels Microsoft SQL Server usage guide How to work during the seminar \u00b6 The seminar is lead by the instructor. After getting to know the tools we use, the exercises are solved together. Experienced behavior is summarized and explained. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first! Exercise 1: Create a database in MS SQL Server \u00b6 We need a database first. Usually, the database is located on a central server, but we often run a server on our machine for development. Connect to Microsoft SQL Server using SQL Server Management Studio. Start Management Studio and use the following connection details: Server name: (localdb)\\mssqllocaldb Authentication: Windows authentication Create a new database (if it does not exist yet); the name should be your Neptun code: in Object Explorer right-click Databases and choose Create Database . Instantiate the sample database using the script. Open a new Query window, paste the script into the window, then execute it. Make sure to select the right database in the toolbar dropdown. Verify that the tables are created. If the Tables folder was open before, you need to refresh it. . Exercise 2: Concurrent transactions \u00b6 To simulate concurrent transactions, you need two Query windows by clicking the New Query button twice. You can align the windows next to each other by right-clicking the Query header and choosing New Vertical Tab group: Use the following scheduling. Transaction T1 checks the status of order 4, while transaction T2 changes the status. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID [Order] The brackets in [Order] are needed to distinguish it from the order by command. T2 transaction -- Change the status or the order update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Change the status of each item in the order update OrderItem set StatusID = 4 where OrderID = 4 T1 transaction : repeat the same command as in step 1 What did you experience? Why? In the beginning, everything was in status \"Packaged\", which is fine (the items in the order and the order itself had the same status). But after we changed the status of the order, the status seemed controversial: the order and the items had different statuses. We have to understand that the database itself was not inconsistent, as the database's integration requirements allow this. However, from a business perspective, there was an inconsistency. SQL Server, by default, runs in auto-commit mode. That is, every sql statement is a transaction by itself, which is committed when completed. Thus the problem was that our modifications were executed in separate transactions. In order to handle the two changes together, we would need to combine them into a single transaction. Exercise 3: using transactions, read committed isolation level \u00b6 Let us repeat the previous exercise so that the two modifications form a single transaction: T2 transaction should begin with a begin tran command, and finish with a commit statement. When changing the status, the new status should be 3 (to have an actual change in the data). What did you experience? Why? While data modification is underway in T2 , the query statement in T1 will wait. It will wait until the data modification transaction is completed. The select statement wants to place a reader lock on the records, but the other concurrent transaction is editing these records and has an exclusive writer lock on them. Let us remember that the default isolation level is read committed . This isolation level on this platform means that data under modification cannot be accessed, not even for reading. This is a matter of implementation; the SQL standard does not specify this (e.g., in Oracle Server the previously committed state of each record is available). In other isolation levels, MSSQL Server behaves differently; e.g., in the snapshot isolation level, the version of the data before the modification is accessible. Exercise 4: aborting transactions ( rollback ) in read committed isolation level \u00b6 Let us repeat the same command sequence, including the transaction, but let us abort the modification operation in the middle. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID T2 transaction -- Start new transaction begin tran -- Change the order status update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Abort the transaction rollback What did you experience? Why? Similarly to the previous exercise, the read operation was forced to wait while the modification transaction was underway. When this modification was aborted, the result of the read query arrived immediately. We are still using read committed isolation level; hence we must not see data being modified. But once the modification transaction finished, either successfully with commit or with a rollback , the data records are once again available. Let us understand that we have just avoided the problem of dirty read. If the read query showed us the uncommitted modification, we would have seen values that would have been invalid after the rollback . Exercise 5: Placing an order using serializable isolation level \u00b6 Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us have two concurrent transactions, both placing an order. We must allow an order for a product only if we have enough stock left. To properly isolate the effect of the transactions, let use serializable isolation level. T1 transaction set transaction isolation level serializable begin tran -- Check the product stock select * from Product where ID = 2 T2 transaction set transaction isolation level serializable begin tran select * from Product where ID = 2 T1 transaction -- Check the registered, but unprocessed orders for the same product select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T1 transaction -- Since the order can be completed, store the order insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 2 , 3 , 1 ) T1 transaction commit T2 transaction commit What did you experience? Why? A deadlock will occur due to the serializable isolation level, as both transactions require exclusive access to the table OrderItem . The query select sum - with the requirement to protect from unrepeatable reads - adds reader locks to the records. Thus the insert cannot complete, as it needs write access. This effectively means that both transactions are waiting for a lock that the other one holds. The result of the deadlock is the abortion of one of the transactions. This is the expected and the correct behavior under the circumstances since it prohibits the very problem we are trying to avoid (to sell more products than we have in stock). Let us repeat the same sequence of steps, but this time, the products should be different. This simulates two concurrent orders for different products. Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us replace the ID or ProductID values: one of the transactions should use ID 2 and the other one ID 3. What did you experience? Why? Even when the two orders reference different products a deadlock occurs. The locking mechanism behind the statement select sum locks the entire table, because it is not able to distinguish the records by ProductID . This is not unexpected, as the fact that two concurrent orders referencing different products are allowed, is a business requirement; the database has no knowledge of this. In other words, the serializable isolation level is too strict in this case. This is the reason that serializable is not frequently used in practice. Exercise 6: Order registration with read committed isolation level \u00b6 Let us consider what would happen in the previous exercise if the isolation level was left at default? Would there be any deadlock? Would the result be correct? What would we expect? Why? If we used the default isolation level, the behavior would be incorrect. The read committed isolation level would not protect us from a concurrent transaction inserting a new record that could potentially result in selling more products than available. This would be a manifestation of the unrepeatable read concurrency problem. We can conclude thus that the serializable isolation level was not unnecessary. It did in fact, protect us from a valid concurrency problem. Exercise 7: Manual locking \u00b6 Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Using read committed isolation level, let us find a solution that only prohibits concurrent orders of the same product . You can assume that all copies of the concurrent program run the same logic. The solution is based on manual locks we can place on records. These locks |(similarly to the automatic ones) have a lifespan identical to the transaction. select * from tablename with ( XLOCK ) ... Where do we place this lock? How does the order registration process look like? The key to this exercise is understanding where the lock should be placed. The question is what to lock. The answer is the product : we want to avoid placing orders of the same product. Thus we place a lock on the product record. The order process is as follows: T1 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 2 T2 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 3 T1 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 3 and StatusID = 1 T1 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 3 , 3 , 1 ) T1 transaction commit T2 transaction commit Exercise 8: Table locking \u00b6 There is another option for manual locking by locking entire tables: select * from tablename with ( TABLOCKX ) ... This might seem a simple solution, but why is this a not recommended option? In our scenario, the table lock should be placed on the order item table. But effectively, this would mean the same thing as using serializable isolation level: there would be no deadlocks; however, there would be no concurrency allowed either.","title":"Transactions"},{"location":"seminar/transactions/#transactions","text":"The goal is to examine transaction handling of MS SQL Server, understand the practical limits to serializable isolation level, and controlling data dependency in read committed isolation level.","title":"Transactions"},{"location":"seminar/transactions/#pre-requisites","text":"Required tools to complete the tasks: Microsoft SQL Server (LocalDB or Express edition) SQL Server Management Studio Database initialization script: mssql.sql Recommended to review: Transaction properties, isolation levels Microsoft SQL Server usage guide","title":"Pre-requisites"},{"location":"seminar/transactions/#how-to-work-during-the-seminar","text":"The seminar is lead by the instructor. After getting to know the tools we use, the exercises are solved together. Experienced behavior is summarized and explained. This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!","title":"How to work during the seminar"},{"location":"seminar/transactions/#exercise-1-create-a-database-in-ms-sql-server","text":"We need a database first. Usually, the database is located on a central server, but we often run a server on our machine for development. Connect to Microsoft SQL Server using SQL Server Management Studio. Start Management Studio and use the following connection details: Server name: (localdb)\\mssqllocaldb Authentication: Windows authentication Create a new database (if it does not exist yet); the name should be your Neptun code: in Object Explorer right-click Databases and choose Create Database . Instantiate the sample database using the script. Open a new Query window, paste the script into the window, then execute it. Make sure to select the right database in the toolbar dropdown. Verify that the tables are created. If the Tables folder was open before, you need to refresh it. .","title":"Exercise 1: Create a database in MS SQL Server"},{"location":"seminar/transactions/#exercise-2-concurrent-transactions","text":"To simulate concurrent transactions, you need two Query windows by clicking the New Query button twice. You can align the windows next to each other by right-clicking the Query header and choosing New Vertical Tab group: Use the following scheduling. Transaction T1 checks the status of order 4, while transaction T2 changes the status. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID [Order] The brackets in [Order] are needed to distinguish it from the order by command. T2 transaction -- Change the status or the order update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Change the status of each item in the order update OrderItem set StatusID = 4 where OrderID = 4 T1 transaction : repeat the same command as in step 1 What did you experience? Why? In the beginning, everything was in status \"Packaged\", which is fine (the items in the order and the order itself had the same status). But after we changed the status of the order, the status seemed controversial: the order and the items had different statuses. We have to understand that the database itself was not inconsistent, as the database's integration requirements allow this. However, from a business perspective, there was an inconsistency. SQL Server, by default, runs in auto-commit mode. That is, every sql statement is a transaction by itself, which is committed when completed. Thus the problem was that our modifications were executed in separate transactions. In order to handle the two changes together, we would need to combine them into a single transaction.","title":"Exercise 2: Concurrent transactions"},{"location":"seminar/transactions/#exercise-3-using-transactions-read-committed-isolation-level","text":"Let us repeat the previous exercise so that the two modifications form a single transaction: T2 transaction should begin with a begin tran command, and finish with a commit statement. When changing the status, the new status should be 3 (to have an actual change in the data). What did you experience? Why? While data modification is underway in T2 , the query statement in T1 will wait. It will wait until the data modification transaction is completed. The select statement wants to place a reader lock on the records, but the other concurrent transaction is editing these records and has an exclusive writer lock on them. Let us remember that the default isolation level is read committed . This isolation level on this platform means that data under modification cannot be accessed, not even for reading. This is a matter of implementation; the SQL standard does not specify this (e.g., in Oracle Server the previously committed state of each record is available). In other isolation levels, MSSQL Server behaves differently; e.g., in the snapshot isolation level, the version of the data before the modification is accessible.","title":"Exercise 3: using transactions, read committed isolation level"},{"location":"seminar/transactions/#exercise-4-aborting-transactions-rollback-in-read-committed-isolation-level","text":"Let us repeat the same command sequence, including the transaction, but let us abort the modification operation in the middle. T1 transaction -- List the order and the related items with their status select s1 . Name , p . Name , s2 . Name from [ Order ] o , OrderItem oi , Status s1 , status s2 , Product p where o . Id = oi . OrderID and o . ID = 4 and o . StatusID = s1 . ID and oi . StatusID = s2 . ID and p . ID = oi . ProductID T2 transaction -- Start new transaction begin tran -- Change the order status update [ Order ] set StatusID = 4 where ID = 4 T1 transaction : repeat the same command as in step 1 T2 transaction -- Abort the transaction rollback What did you experience? Why? Similarly to the previous exercise, the read operation was forced to wait while the modification transaction was underway. When this modification was aborted, the result of the read query arrived immediately. We are still using read committed isolation level; hence we must not see data being modified. But once the modification transaction finished, either successfully with commit or with a rollback , the data records are once again available. Let us understand that we have just avoided the problem of dirty read. If the read query showed us the uncommitted modification, we would have seen values that would have been invalid after the rollback .","title":"Exercise 4: aborting transactions (rollback) in read committed isolation level"},{"location":"seminar/transactions/#exercise-5-placing-an-order-using-serializable-isolation-level","text":"Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us have two concurrent transactions, both placing an order. We must allow an order for a product only if we have enough stock left. To properly isolate the effect of the transactions, let use serializable isolation level. T1 transaction set transaction isolation level serializable begin tran -- Check the product stock select * from Product where ID = 2 T2 transaction set transaction isolation level serializable begin tran select * from Product where ID = 2 T1 transaction -- Check the registered, but unprocessed orders for the same product select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T1 transaction -- Since the order can be completed, store the order insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 2 , 3 , 1 ) T1 transaction commit T2 transaction commit What did you experience? Why? A deadlock will occur due to the serializable isolation level, as both transactions require exclusive access to the table OrderItem . The query select sum - with the requirement to protect from unrepeatable reads - adds reader locks to the records. Thus the insert cannot complete, as it needs write access. This effectively means that both transactions are waiting for a lock that the other one holds. The result of the deadlock is the abortion of one of the transactions. This is the expected and the correct behavior under the circumstances since it prohibits the very problem we are trying to avoid (to sell more products than we have in stock). Let us repeat the same sequence of steps, but this time, the products should be different. This simulates two concurrent orders for different products. Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Let us replace the ID or ProductID values: one of the transactions should use ID 2 and the other one ID 3. What did you experience? Why? Even when the two orders reference different products a deadlock occurs. The locking mechanism behind the statement select sum locks the entire table, because it is not able to distinguish the records by ProductID . This is not unexpected, as the fact that two concurrent orders referencing different products are allowed, is a business requirement; the database has no knowledge of this. In other words, the serializable isolation level is too strict in this case. This is the reason that serializable is not frequently used in practice.","title":"Exercise 5: Placing an order using serializable isolation level"},{"location":"seminar/transactions/#exercise-6-order-registration-with-read-committed-isolation-level","text":"Let us consider what would happen in the previous exercise if the isolation level was left at default? Would there be any deadlock? Would the result be correct? What would we expect? Why? If we used the default isolation level, the behavior would be incorrect. The read committed isolation level would not protect us from a concurrent transaction inserting a new record that could potentially result in selling more products than available. This would be a manifestation of the unrepeatable read concurrency problem. We can conclude thus that the serializable isolation level was not unnecessary. It did in fact, protect us from a valid concurrency problem.","title":"Exercise 6: Order registration with read committed isolation level"},{"location":"seminar/transactions/#exercise-7-manual-locking","text":"Before we begin, let us get rid of any pending transactions we may have. Let us issue a few rollback statements in both windows. Using read committed isolation level, let us find a solution that only prohibits concurrent orders of the same product . You can assume that all copies of the concurrent program run the same logic. The solution is based on manual locks we can place on records. These locks |(similarly to the automatic ones) have a lifespan identical to the transaction. select * from tablename with ( XLOCK ) ... Where do we place this lock? How does the order registration process look like? The key to this exercise is understanding where the lock should be placed. The question is what to lock. The answer is the product : we want to avoid placing orders of the same product. Thus we place a lock on the product record. The order process is as follows: T1 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 2 T2 transaction set transaction isolation level read committed begin tran select * from Product with ( xlock ) where ID = 3 T1 transaction select sum ( Amount ) from OrderItem where ProductID = 2 and StatusID = 1 T2 transaction select sum ( Amount ) from OrderItem where ProductID = 3 and StatusID = 1 T1 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 2 , 2 , 3 , 1 ) T2 transaction insert into OrderItem ( OrderID , ProductID , Amount , StatusID ) values ( 3 , 3 , 3 , 1 ) T1 transaction commit T2 transaction commit","title":"Exercise 7: Manual locking"},{"location":"seminar/transactions/#exercise-8-table-locking","text":"There is another option for manual locking by locking entire tables: select * from tablename with ( TABLOCKX ) ... This might seem a simple solution, but why is this a not recommended option? In our scenario, the table lock should be placed on the order item table. But effectively, this would mean the same thing as using serializable isolation level: there would be no deadlocks; however, there would be no concurrency allowed either.","title":"Exercise 8: Table locking"}]}